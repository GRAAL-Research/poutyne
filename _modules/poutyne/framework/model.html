<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>poutyne.framework.model &mdash; Poutyne 1.10 documentation</title>
      <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../index.html">
            <img src="../../../_static/poutyne-light.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                1.10
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../model.html">Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../experiment.html">Experiment and ModelBundle</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../metrics.html">Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../callbacks.html">Callbacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../layers.html">Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../utils.html">Utils</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../examples/introduction.html">Introduction to PyTorch and Poutyne</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../examples/tips_and_tricks.html">Tips and Tricks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../examples/sequence_tagging.html">Sequence Tagging With an RNN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../examples/policy_interface.html">Interface of <code class="docutils literal notranslate"><span class="pre">policy</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../examples/transfer_learning.html">Transfer learning example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../examples/image_reconstruction.html">Image Reconstruction Using Poutyne</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../examples/classification_and_regression.html">Gender Classification and Eyes Location Detection: A Two Task Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../examples/semantic_segmentation.html">Semantic segmentation using Poutyne</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">Poutyne</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../index.html">Module code</a> &raquo;</li>
      <li>poutyne.framework.model</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for poutyne.framework.model</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Copyright (c) 2022 Poutyne and all respective contributors.</span>

<span class="sd">Each contributor holds copyright over their respective contributions. The project versioning (Git)</span>
<span class="sd">records all such contribution source information.</span>

<span class="sd">This file is part of Poutyne.</span>

<span class="sd">Poutyne is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General Public</span>
<span class="sd">License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later</span>
<span class="sd">version.</span>

<span class="sd">Poutyne is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty</span>
<span class="sd">of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more details.</span>

<span class="sd">You should have received a copy of the GNU Lesser General Public License along with Poutyne. If not, see</span>
<span class="sd">&lt;https://www.gnu.org/licenses/&gt;.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="c1"># pylint: disable=too-many-lines,too-many-public-methods</span>
<span class="kn">import</span> <span class="nn">contextlib</span>
<span class="kn">import</span> <span class="nn">timeit</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Iterable</span><span class="p">,</span> <span class="n">Mapping</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Union</span><span class="p">,</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Tuple</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">from</span> <span class="nn">torch.nn.utils.rnn</span> <span class="kn">import</span> <span class="n">PackedSequence</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="kn">from</span> <span class="nn">poutyne</span> <span class="kn">import</span> <span class="n">torch_to_numpy</span><span class="p">,</span> <span class="n">numpy_to_torch</span><span class="p">,</span> <span class="n">torch_to</span>
<span class="kn">from</span> <span class="nn">.callbacks</span> <span class="kn">import</span> <span class="n">CallbackList</span><span class="p">,</span> <span class="n">ProgressionCallback</span><span class="p">,</span> <span class="n">Callback</span>
<span class="kn">from</span> <span class="nn">.iterators</span> <span class="kn">import</span> <span class="n">EpochIterator</span><span class="p">,</span> <span class="n">_get_step_iterator</span><span class="p">,</span> <span class="n">StepIterator</span>
<span class="kn">from</span> <span class="nn">.metrics</span> <span class="kn">import</span> <span class="n">get_metric</span>
<span class="kn">from</span> <span class="nn">.metrics</span> <span class="kn">import</span> <span class="n">get_callables_and_names</span><span class="p">,</span> <span class="n">rename_doubles</span><span class="p">,</span> <span class="n">flatten_metric_names</span>
<span class="kn">from</span> <span class="nn">.metrics.decomposable</span> <span class="kn">import</span> <span class="n">convert_decomposable_metric_to_object</span>
<span class="kn">from</span> <span class="nn">.optimizers</span> <span class="kn">import</span> <span class="n">get_optimizer</span>
<span class="kn">from</span> <span class="nn">..utils</span> <span class="kn">import</span> <span class="n">TensorDataset</span><span class="p">,</span> <span class="n">_concat</span><span class="p">,</span> <span class="n">get_batch_size</span>


<div class="viewcode-block" id="Model"><a class="viewcode-back" href="../../../model.html#poutyne.Model">[docs]</a><span class="k">class</span> <span class="nc">Model</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The Model class encapsulates a PyTorch network, a PyTorch optimizer, a loss function and</span>
<span class="sd">    metric functions. It allows the user to train a neural network without hand-coding the</span>
<span class="sd">    epoch/step logic.</span>

<span class="sd">    Args:</span>
<span class="sd">        network (torch.nn.Module): A PyTorch network.</span>
<span class="sd">        optimizer (Union[torch.optim.Optimizer, str, dict]): If torch.optim.Optimier, an initialized PyTorch.</span>
<span class="sd">            If str, should be the name of the optimizer in Pytorch (i.e. &#39;Adam&#39; for torch.optim.Adam).</span>
<span class="sd">            If dict, should contain a key ``&#39;optim&#39;`` with the value be the name of the optimizer; other</span>
<span class="sd">            entries are passed to the optimizer as keyword arguments.</span>
<span class="sd">            (Default value = None)</span>
<span class="sd">        loss_function(Union[Callable, str]) It can be any PyTorch loss layer or custom loss function. It</span>
<span class="sd">            can also be a string with the same name as a PyTorch loss function (either the functional or</span>
<span class="sd">            object name). The loss function must have the signature ``loss_function(input, target)`` where</span>
<span class="sd">            ``input`` is the prediction of the network and ``target`` is the ground truth.</span>
<span class="sd">            (Default value = None)</span>
<span class="sd">        batch_metrics (list): List of functions with the same signature as a loss function or objects with the same</span>
<span class="sd">            signature as either :class:`~poutyne.Metric` or :class:`torchmetrics.Metric &lt;torchmetrics.Metric&gt;`. It can</span>
<span class="sd">            also be a string with the same name as a PyTorch loss function (either the functional or object name).</span>
<span class="sd">            Some metrics, such as  &#39;accuracy&#39; (or just &#39;acc&#39;), are also available as strings. See :ref:`metrics` and</span>
<span class="sd">            the `TorchMetrics documentation &lt;https://torchmetrics.readthedocs.io/en/latest/references/modules.html&gt;`__</span>
<span class="sd">            for available metrics.</span>

<span class="sd">            Batch metric are computed on computed for each batch.</span>
<span class="sd">            (Default value = None)</span>

<span class="sd">            .. warning:: When using this argument, the metrics are computed for each batch. This can significantly slow</span>
<span class="sd">                down the compuations depending on the metrics used. This mostly happens on non-decomposable metrics</span>
<span class="sd">                such as :class:`torchmetrics.AUROC &lt;torchmetrics.AUROC&gt;` where an ordering of the elements is necessary</span>
<span class="sd">                to compute the metric. In such case, we advise to use them as epoch metrics instead.</span>
<span class="sd">        epoch_metrics (list): List of functions with the same signature as a loss function or objects with the same</span>
<span class="sd">            signature as either :class:`~poutyne.Metric` or :class:`torchmetrics.Metric &lt;torchmetrics.Metric&gt;`. It can</span>
<span class="sd">            also be a string with the same name as a PyTorch loss function (either the functional or object name).</span>
<span class="sd">            Some metrics, such as  &#39;accuracy&#39; (or just &#39;acc&#39;), are also available as strings. See :ref:`metrics` and</span>
<span class="sd">            the `TorchMetrics documentation &lt;https://torchmetrics.readthedocs.io/en/latest/references/modules.html&gt;`__</span>
<span class="sd">            for available metrics.</span>

<span class="sd">            Epoch metrics are computed only at the end of the epoch.</span>
<span class="sd">            (Default value = None)</span>
<span class="sd">        device (Union[torch.torch.device, List[torch.torch.device]]): The device to which the network is</span>
<span class="sd">            sent or the list of device to which the network is sent. See :func:`~Model.to()` for details.</span>

<span class="sd">    Note:</span>
<span class="sd">        The name of each batch and epoch metric can be change by passing a tuple ``(name, metric)`` instead</span>
<span class="sd">        of simply the metric function or object, where ``name`` is the alternative name of the metric.</span>
<span class="sd">        Batch and epoch metrics can return multiple metrics (e.g. an epoch metric could return an F1-score</span>
<span class="sd">        with the associated precision and recall). See :ref:`multiple metrics at once` for more details.</span>


<span class="sd">    Attributes:</span>
<span class="sd">        network (torch.nn.Module): The associated PyTorch network.</span>
<span class="sd">        optimizer (torch.optim.Optimizer): The associated PyTorch optimizer.</span>
<span class="sd">        loss_function: The associated loss function.</span>
<span class="sd">        batch_metrics (list): The associated metric functions for every batch.</span>
<span class="sd">        epoch_metrics (list): The associated metric functions for every epoch.</span>

<span class="sd">    Examples:</span>
<span class="sd">        Using Numpy arrays (or tensors) dataset::</span>

<span class="sd">            from poutyne import Model</span>
<span class="sd">            import torch</span>
<span class="sd">            import numpy as np</span>
<span class="sd">            import torchmetrics</span>

<span class="sd">            num_features = 20</span>
<span class="sd">            num_classes = 5</span>

<span class="sd">            # Our training dataset with 800 samples.</span>
<span class="sd">            num_train_samples = 800</span>
<span class="sd">            train_x = np.random.randn(num_train_samples, num_features).astype(&#39;float32&#39;)</span>
<span class="sd">            train_y = np.random.randint(num_classes, size=num_train_samples).astype(&#39;int64&#39;)</span>

<span class="sd">            # Our validation dataset with 200 samples.</span>
<span class="sd">            num_valid_samples = 200</span>
<span class="sd">            valid_x = np.random.randn(num_valid_samples, num_features).astype(&#39;float32&#39;)</span>
<span class="sd">            valid_y = np.random.randint(num_classes, size=num_valid_samples).astype(&#39;int64&#39;)</span>

<span class="sd">            pytorch_network = torch.nn.Linear(num_features, num_classes) # Our network</span>

<span class="sd">            # We create and optimize our model</span>
<span class="sd">            model = Model(pytorch_network, &#39;sgd&#39;, &#39;cross_entropy&#39;,</span>
<span class="sd">                          batch_metrics=[&#39;accuracy&#39;],</span>
<span class="sd">                          epoch_metrics=[torchmetrics.AUROC(num_classes=num_classes)])</span>
<span class="sd">            model.fit(train_x, train_y,</span>
<span class="sd">                      validation_data=(valid_x, valid_y),</span>
<span class="sd">                      epochs=5,</span>
<span class="sd">                      batch_size=32)</span>

<span class="sd">        .. code-block:: none</span>

<span class="sd">            Epoch: 1/5 Train steps: 25 Val steps: 7 0.51s loss: 1.757784 acc: 20.750000 auroc: 0.494891</span>
<span class="sd">            val_loss: 1.756639 val_acc: 18.500000 val_auroc: 0.499404</span>
<span class="sd">            Epoch: 2/5 Train steps: 25 Val steps: 7 0.03s loss: 1.749623 acc: 20.375000 auroc: 0.496878</span>
<span class="sd">            val_loss: 1.748795 val_acc: 19.000000 val_auroc: 0.499723</span>
<span class="sd">            Epoch: 3/5 Train steps: 25 Val steps: 7 0.03s loss: 1.742070 acc: 20.250000 auroc: 0.499461</span>
<span class="sd">            val_loss: 1.741379 val_acc: 19.000000 val_auroc: 0.498577</span>
<span class="sd">            ...</span>

<span class="sd">        Using PyTorch DataLoader::</span>

<span class="sd">           import torch</span>
<span class="sd">           from torch.utils.data import DataLoader, TensorDataset</span>
<span class="sd">           from poutyne import Model</span>
<span class="sd">           import torchmetrics</span>

<span class="sd">           num_features = 20</span>
<span class="sd">           num_classes = 5</span>

<span class="sd">           # Our training dataset with 800 samples.</span>
<span class="sd">           num_train_samples = 800</span>
<span class="sd">           train_x = torch.rand(num_train_samples, num_features)</span>
<span class="sd">           train_y = torch.randint(num_classes, (num_train_samples,), dtype=torch.long)</span>
<span class="sd">           train_dataset = TensorDataset(train_x, train_y)</span>
<span class="sd">           train_generator = DataLoader(train_dataset, batch_size=32)</span>

<span class="sd">           # Our validation dataset with 200 samples.</span>
<span class="sd">           num_valid_samples = 200</span>
<span class="sd">           valid_x = torch.rand(num_valid_samples, num_features)</span>
<span class="sd">           valid_y = torch.randint(num_classes, (num_valid_samples,), dtype=torch.long)</span>
<span class="sd">           valid_dataset = TensorDataset(valid_x, valid_y)</span>
<span class="sd">           valid_generator = DataLoader(valid_dataset, batch_size=32)</span>

<span class="sd">           pytorch_network = torch.nn.Linear(num_features, num_classes)</span>

<span class="sd">           model = Model(pytorch_network, &#39;sgd&#39;, &#39;cross_entropy&#39;,</span>
<span class="sd">                         batch_metrics=[&#39;accuracy&#39;],</span>
<span class="sd">                         epoch_metrics=[torchmetrics.AUROC(num_classes=num_classes)])</span>
<span class="sd">           model.fit_generator(train_generator,</span>
<span class="sd">                               valid_generator,</span>
<span class="sd">                               epochs=5)</span>

<span class="sd">        .. code-block:: none</span>

<span class="sd">            Epoch: 1/5 Train steps: 25 Val steps: 7 0.07s loss: 1.614473 acc: 20.500000 auroc: 0.516850</span>
<span class="sd">            val_loss: 1.617141 val_acc: 21.500000 val_auroc: 0.522068</span>
<span class="sd">            Epoch: 2/5 Train steps: 25 Val steps: 7 0.03s loss: 1.614454 acc: 20.125000 auroc: 0.517618</span>
<span class="sd">            val_loss: 1.615585 val_acc: 22.000000 val_auroc: 0.521051</span>
<span class="sd">            Epoch: 3/5 Train steps: 25 Val steps: 7 0.03s loss: 1.613709 acc: 20.125000 auroc: 0.518307</span>
<span class="sd">            val_loss: 1.614440 val_acc: 22.000000 val_auroc: 0.520762</span>
<span class="sd">            ...</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">network</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="p">,</span>
        <span class="n">loss_function</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">batch_metrics</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">epoch_metrics</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">torch_metrics</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;network should be of type derived from nn.Module, received </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">network</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">optimizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="p">(</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;optimizer should be of type derived from optim.Optimizer, received </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

        <span class="n">batch_metrics</span> <span class="o">=</span> <span class="p">[]</span> <span class="k">if</span> <span class="n">batch_metrics</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">batch_metrics</span>
        <span class="n">epoch_metrics</span> <span class="o">=</span> <span class="p">[]</span> <span class="k">if</span> <span class="n">epoch_metrics</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">epoch_metrics</span>
        <span class="k">if</span> <span class="n">torch_metrics</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">batch_metrics</span> <span class="o">+=</span> <span class="n">torch_metrics</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;The torch_metrics keyword is deprecated as of v.1.11. Use batch_metrics instead. &quot;</span>
                <span class="s2">&quot;It will be remove in the next version.&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">network</span> <span class="o">=</span> <span class="n">network</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">get_optimizer</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">loss_function</span> <span class="o">=</span> <span class="n">get_metric</span><span class="p">(</span><span class="n">loss_function</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_function</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">loss_function</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_function</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_function</span> <span class="o">=</span> <span class="n">convert_decomposable_metric_to_object</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_function</span><span class="p">,</span> <span class="s1">&#39;loss&#39;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_check_network_optimizer_parameters_match</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_metrics_attributes</span><span class="p">(</span><span class="n">batch_metrics</span><span class="p">,</span> <span class="n">epoch_metrics</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">other_device</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_check_network_optimizer_parameters_match</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">param_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
            <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">param_group</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">]:</span>
                    <span class="k">if</span> <span class="n">param</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">param_set</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                            <span class="s2">&quot;All parameters in the optimizer should be part of the network. &quot;</span>
                            <span class="s2">&quot;This is so to insure that weights checkpointing and the likes &quot;</span>
                            <span class="s2">&quot;actually consider all parameters.&quot;</span>
                        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_set_metrics_attributes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_metrics</span><span class="p">,</span> <span class="n">epoch_metrics</span><span class="p">):</span>
        <span class="n">batch_metrics</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">get_metric</span><span class="p">,</span> <span class="n">batch_metrics</span><span class="p">))</span>
        <span class="n">batch_metrics</span><span class="p">,</span> <span class="n">batch_metrics_names</span> <span class="o">=</span> <span class="n">get_callables_and_names</span><span class="p">(</span><span class="n">batch_metrics</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_metrics</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">convert_decomposable_metric_to_object</span><span class="p">(</span><span class="n">metric</span><span class="p">,</span> <span class="n">names</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">metric</span><span class="p">,</span> <span class="n">names</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">batch_metrics</span><span class="p">,</span> <span class="n">batch_metrics_names</span><span class="p">)</span>
        <span class="p">]</span>

        <span class="n">epoch_metrics</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">get_metric</span><span class="p">,</span> <span class="n">epoch_metrics</span><span class="p">))</span>
        <span class="n">epoch_metrics</span><span class="p">,</span> <span class="n">epoch_metrics_names</span> <span class="o">=</span> <span class="n">get_callables_and_names</span><span class="p">(</span><span class="n">epoch_metrics</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epoch_metrics</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">convert_decomposable_metric_to_object</span><span class="p">(</span><span class="n">metric</span><span class="p">,</span> <span class="n">names</span><span class="p">,</span> <span class="n">is_epoch_metric</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">metric</span><span class="p">,</span> <span class="n">names</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">epoch_metrics</span><span class="p">,</span> <span class="n">epoch_metrics_names</span><span class="p">)</span>
        <span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">original_batch_metrics_names</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">original_epoch_metrics_names</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">batch_metrics_names</span><span class="p">,</span>
            <span class="n">epoch_metrics_names</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">batch_metrics_names</span><span class="p">,</span> <span class="n">epoch_metrics_names</span> <span class="o">=</span> <span class="n">rename_doubles</span><span class="p">(</span><span class="n">batch_metrics_names</span><span class="p">,</span> <span class="n">epoch_metrics_names</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">unflatten_batch_metrics_names</span> <span class="o">=</span> <span class="n">batch_metrics_names</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">unflatten_epoch_metrics_names</span> <span class="o">=</span> <span class="n">epoch_metrics_names</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">batch_metrics_names</span> <span class="o">=</span> <span class="n">flatten_metric_names</span><span class="p">(</span><span class="n">batch_metrics_names</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epoch_metrics_names</span> <span class="o">=</span> <span class="n">flatten_metric_names</span><span class="p">(</span><span class="n">epoch_metrics_names</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metrics_names</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_metrics_names</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">epoch_metrics_names</span>

    <span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
    <span class="k">def</span> <span class="nf">_set_training_mode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">training</span><span class="p">):</span>
        <span class="n">old_training</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">training</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">training</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">training</span><span class="p">):</span>
            <span class="k">yield</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">old_training</span><span class="p">)</span>

<div class="viewcode-block" id="Model.fit"><a class="viewcode-back" href="../../../model.html#poutyne.Model.fit">[docs]</a>    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">,</span>
        <span class="n">y</span><span class="p">,</span>
        <span class="n">validation_data</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
        <span class="n">epochs</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
        <span class="n">steps_per_epoch</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">validation_steps</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">batches_per_step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">initial_epoch</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">progress_options</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">dict</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">callbacks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">dataloader_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="c1"># pylint: disable=line-too-long,too-many-locals</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Trains the network on a dataset. This method creates generators and calls</span>
<span class="sd">        the :func:`~Model.fit_generator()` method.</span>

<span class="sd">        Args:</span>
<span class="sd">            x (Union[~torch.Tensor, ~numpy.ndarray] or Union[tuple, list] of Union[~torch.Tensor, ~numpy.ndarray]):</span>
<span class="sd">                Training dataset. Union[Tensor, ndarray] if the model has a single input.</span>
<span class="sd">                Union[tuple, list] of Union[Tensor, ndarray] if the model has multiple inputs.</span>
<span class="sd">            y (Union[~torch.Tensor, ~numpy.ndarray] or Union[tuple, list] of Union[~torch.Tensor, ~numpy.ndarray]):</span>
<span class="sd">                Target. Union[Tensor, ndarray] if the model has a single output.</span>
<span class="sd">                Union[tuple, list] of Union[Tensor, ndarray] if the model has multiple outputs.</span>
<span class="sd">            validation_data (Tuple[``x_val``, ``y_val``]):</span>
<span class="sd">                Same format as ``x`` and ``y`` previously described. Validation dataset on which to</span>
<span class="sd">                evaluate the loss and any model metrics at the end of each epoch. The model will not be</span>
<span class="sd">                trained on this data.</span>
<span class="sd">                (Default value = None)</span>
<span class="sd">            batch_size (int): Number of samples given to the network at one time.</span>
<span class="sd">                (Default value = 32)</span>
<span class="sd">            epochs (int): Number of times the entire training dataset is seen.</span>
<span class="sd">                (Default value = 1000)</span>
<span class="sd">            steps_per_epoch (int, optional): Number of batch used during one epoch. Obviously, using</span>
<span class="sd">                this argument may cause one epoch not to see the entire training dataset or see it</span>
<span class="sd">                multiple times.</span>
<span class="sd">                (Defaults the number of steps needed to see the entire training dataset)</span>
<span class="sd">            validation_steps (int, optional): Same as for ``steps_per_epoch`` but for the validation</span>
<span class="sd">                dataset.</span>
<span class="sd">                (Defaults to the number of steps needed to see the entire validation dataset)</span>
<span class="sd">            batches_per_step (int): Number of batches on which to compute the running loss before</span>
<span class="sd">                backpropagating it through the network. Note that the total loss used for backpropagation is</span>
<span class="sd">                the mean of the `batches_per_step` batch losses.</span>
<span class="sd">                (Default value = 1)</span>
<span class="sd">            initial_epoch (int, optional): Epoch at which to start training</span>
<span class="sd">                (useful for resuming a previous training run).</span>
<span class="sd">                (Default value = 1)</span>
<span class="sd">            verbose (bool): Whether to display the progress of the training.</span>
<span class="sd">                (Default value = True)</span>
<span class="sd">            progress_options (dict, optional): Keyword arguments to pass to the default progression callback used</span>
<span class="sd">                in Poutyne (See :class:`~poutyne.ProgressionCallback` for the available arguments).</span>
<span class="sd">                (Default value = None)</span>
<span class="sd">            callbacks (List[~poutyne.Callback]): List of callbacks that will be called</span>
<span class="sd">                during training.</span>
<span class="sd">                (Default value = None)</span>
<span class="sd">            dataloader_kwargs (dict, optional): Keyword arguments to pass to the PyTorch dataloaders created</span>
<span class="sd">                internally. By default, ``shuffle=True`` is passed for the training dataloader but this can be</span>
<span class="sd">                overridden by using this argument.</span>

<span class="sd">        Returns:</span>
<span class="sd">            List of dict containing the history of each epoch.</span>

<span class="sd">        Example:</span>
<span class="sd">            .. code-block:: python</span>

<span class="sd">                model = Model(pytorch_network, optimizer, loss_function)</span>
<span class="sd">                history = model.fit(train_x, train_y,</span>
<span class="sd">                                    validation_data=(valid_x, valid_y)</span>
<span class="sd">                                    epochs=num_epochs,</span>
<span class="sd">                                    batch_size=batch_size,</span>
<span class="sd">                                    verbose=False)</span>
<span class="sd">                print(*history, sep=&quot;\\n&quot;)</span>

<span class="sd">            .. code-block:: python</span>

<span class="sd">                {&#39;epoch&#39;: 1, &#39;loss&#39;: 1.7198852968215943, &#39;time&#39;: 0.019999928001197986, &#39;acc&#39;: 19.375, &#39;val_loss&#39;: 1.6674459838867188, &#39;val_acc&#39;: 22.0}</span>
<span class="sd">                {&#39;epoch&#39;: 2, &#39;loss&#39;: 1.7054892110824584, &#39;time&#39;: 0.015421080999658443, &#39;acc&#39;: 19.75, &#39;val_loss&#39;: 1.660806336402893, &#39;val_acc&#39;: 22.0}</span>
<span class="sd">                {&#39;epoch&#39;: 3, &#39;loss&#39;: 1.6923445892333984, &#39;time&#39;: 0.01363091799794347, &#39;acc&#39;: 19.625, &#39;val_loss&#39;: 1.6550078630447387, &#39;val_acc&#39;: 22.5}</span>
<span class="sd">                ...</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">train_dataset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dataset_from_data</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
        <span class="n">valid_dataset</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">validation_data</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">valid_dataset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dataset_from_data</span><span class="p">(</span><span class="n">validation_data</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_dataset</span><span class="p">(</span>
            <span class="n">train_dataset</span><span class="p">,</span>
            <span class="n">valid_dataset</span><span class="o">=</span><span class="n">valid_dataset</span><span class="p">,</span>
            <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="n">steps_per_epoch</span><span class="o">=</span><span class="n">steps_per_epoch</span><span class="p">,</span>
            <span class="n">validation_steps</span><span class="o">=</span><span class="n">validation_steps</span><span class="p">,</span>
            <span class="n">batches_per_step</span><span class="o">=</span><span class="n">batches_per_step</span><span class="p">,</span>
            <span class="n">initial_epoch</span><span class="o">=</span><span class="n">initial_epoch</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="n">progress_options</span><span class="o">=</span><span class="n">progress_options</span><span class="p">,</span>
            <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span>
            <span class="n">dataloader_kwargs</span><span class="o">=</span><span class="n">dataloader_kwargs</span><span class="p">,</span>
        <span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_dataset_from_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="n">numpy_to_torch</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<div class="viewcode-block" id="Model.fit_dataset"><a class="viewcode-back" href="../../../model.html#poutyne.Model.fit_dataset">[docs]</a>    <span class="k">def</span> <span class="nf">fit_dataset</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">train_dataset</span><span class="p">,</span>
        <span class="n">valid_dataset</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
        <span class="n">epochs</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
        <span class="n">steps_per_epoch</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">validation_steps</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">batches_per_step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">initial_epoch</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">progress_options</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">callbacks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">collate_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">dataloader_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="c1"># pylint: disable=line-too-long,too-many-locals</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Trains the network on a dataset. This method creates dataloaders and calls the</span>
<span class="sd">        :func:`~Model.fit_generator()` method.</span>

<span class="sd">        Args:</span>
<span class="sd">            train_dataset (~torch.utils.data.Dataset): Training dataset.</span>
<span class="sd">            valid_dataset (~torch.utils.data.Dataset): Validation dataset.</span>
<span class="sd">            batch_size (int): Number of samples given to the network at one time.</span>
<span class="sd">                (Default value = 32)</span>
<span class="sd">            epochs (int): Number of times the entire training dataset is seen.</span>
<span class="sd">                (Default value = 1000)</span>
<span class="sd">            steps_per_epoch (int, optional): Number of batch used during one epoch. Obviously, using</span>
<span class="sd">                this argument may cause one epoch not to see the entire training dataset or see it</span>
<span class="sd">                multiple times.</span>
<span class="sd">                (Defaults the number of steps needed to see the entire training dataset)</span>
<span class="sd">            validation_steps (int, optional): Same as for ``steps_per_epoch`` but for the validation</span>
<span class="sd">                dataset.</span>
<span class="sd">                (Defaults to the number of steps needed to see the entire validation dataset)</span>
<span class="sd">            batches_per_step (int): Number of batches on which to compute the running loss before</span>
<span class="sd">                backpropagating it through the network. Note that the total loss used for backpropagation is</span>
<span class="sd">                the mean of the `batches_per_step` batch losses.</span>
<span class="sd">                (Default value = 1)</span>
<span class="sd">            initial_epoch (int, optional): Epoch at which to start training</span>
<span class="sd">                (useful for resuming a previous training run).</span>
<span class="sd">                (Default value = 1)</span>
<span class="sd">            verbose (bool): Whether to display the progress of the training.</span>
<span class="sd">                (Default value = True)</span>
<span class="sd">            progress_options (dict, optional): Keyword arguments to pass to the default progression callback used</span>
<span class="sd">                in Poutyne (See :class:`~poutyne.ProgressionCallback` for the available arguments).</span>
<span class="sd">                (Default value = None)</span>
<span class="sd">            callbacks (List[~poutyne.Callback]): List of callbacks that will be called</span>
<span class="sd">                during training.</span>
<span class="sd">                (Default value = None)</span>
<span class="sd">            dataloader_kwargs (dict, optional): Keyword arguments to pass to the PyTorch dataloaders created</span>
<span class="sd">                internally. By default, ``shuffle=True`` is passed for the training dataloader but this can be</span>
<span class="sd">                overridden by using this argument.</span>
<span class="sd">            num_workers (int, optional): how many subprocesses to use for data loading.</span>
<span class="sd">                ``0`` means that the data will be loaded in the main process.</span>
<span class="sd">                (Default value = 0)</span>
<span class="sd">            collate_fn (Callable, optional): merges a list of samples to form a mini-batch of Tensor(s).</span>
<span class="sd">                Used when using batched loading from a map-style dataset.</span>

<span class="sd">        Returns:</span>
<span class="sd">            List of dict containing the history of each epoch.</span>

<span class="sd">        See :class:`~torch.utils.data.DataLoader` for details on ``batch_size``, ``num_workers`` and ``collate_fn``.</span>

<span class="sd">        Example:</span>
<span class="sd">            .. code-block:: python</span>

<span class="sd">                model = Model(pytorch_network, optimizer, loss_function)</span>
<span class="sd">                history = model.fit(train_dataset,</span>
<span class="sd">                                    valid_dataset,</span>
<span class="sd">                                    epochs=num_epochs,</span>
<span class="sd">                                    batch_size=batch_size,</span>
<span class="sd">                                    verbose=False)</span>
<span class="sd">                print(*history, sep=&quot;\\n&quot;)</span>

<span class="sd">            .. code-block:: python</span>

<span class="sd">                {&#39;epoch&#39;: 1, &#39;loss&#39;: 1.7198852968215943, &#39;time&#39;: 0.019999928001197986, &#39;acc&#39;: 19.375, &#39;val_loss&#39;: 1.6674459838867188, &#39;val_acc&#39;: 22.0}</span>
<span class="sd">                {&#39;epoch&#39;: 2, &#39;loss&#39;: 1.7054892110824584, &#39;time&#39;: 0.015421080999658443, &#39;acc&#39;: 19.75, &#39;val_loss&#39;: 1.660806336402893, &#39;val_acc&#39;: 22.0}</span>
<span class="sd">                {&#39;epoch&#39;: 3, &#39;loss&#39;: 1.6923445892333984, &#39;time&#39;: 0.01363091799794347, &#39;acc&#39;: 19.625, &#39;val_loss&#39;: 1.6550078630447387, &#39;val_acc&#39;: 22.5}</span>
<span class="sd">                ...</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">dataloader_kwargs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">dataloader_kwargs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">dataloader_kwargs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="n">batch_size</span><span class="p">,</span>
            <span class="s1">&#39;num_workers&#39;</span><span class="p">:</span> <span class="n">num_workers</span><span class="p">,</span>
            <span class="s1">&#39;collate_fn&#39;</span><span class="p">:</span> <span class="n">collate_fn</span><span class="p">,</span>
            <span class="o">**</span><span class="n">dataloader_kwargs</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="n">train_generator</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="o">**</span><span class="p">{</span><span class="s1">&#39;shuffle&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">dataloader_kwargs</span><span class="p">})</span>
        <span class="n">valid_generator</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">valid_dataset</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">valid_generator</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">valid_dataset</span><span class="p">,</span> <span class="o">**</span><span class="n">dataloader_kwargs</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_generator</span><span class="p">(</span>
            <span class="n">train_generator</span><span class="p">,</span>
            <span class="n">valid_generator</span><span class="o">=</span><span class="n">valid_generator</span><span class="p">,</span>
            <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span>
            <span class="n">steps_per_epoch</span><span class="o">=</span><span class="n">steps_per_epoch</span><span class="p">,</span>
            <span class="n">validation_steps</span><span class="o">=</span><span class="n">validation_steps</span><span class="p">,</span>
            <span class="n">batches_per_step</span><span class="o">=</span><span class="n">batches_per_step</span><span class="p">,</span>
            <span class="n">initial_epoch</span><span class="o">=</span><span class="n">initial_epoch</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="n">progress_options</span><span class="o">=</span><span class="n">progress_options</span><span class="p">,</span>
            <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="Model.fit_generator"><a class="viewcode-back" href="../../../model.html#poutyne.Model.fit_generator">[docs]</a>    <span class="k">def</span> <span class="nf">fit_generator</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">train_generator</span><span class="p">,</span>
        <span class="n">valid_generator</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">epochs</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
        <span class="n">steps_per_epoch</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">validation_steps</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">batches_per_step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">initial_epoch</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">progress_options</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">dict</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">callbacks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="c1"># pylint: disable=line-too-long</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Trains the network on a dataset using a generator.</span>

<span class="sd">        Args:</span>
<span class="sd">            train_generator: Generator-like object for the training dataset. The generator must</span>
<span class="sd">                yield a batch in the form of a tuple (x, y) where ``x`` is the input and ``y`` is the</span>
<span class="sd">                target. The batch size is inferred from ``x`` and ``y``. See :func:`get_batch_size()` for</span>
<span class="sd">                details on the inferring algorithm. The loss and the metrics are averaged using this</span>
<span class="sd">                batch size. If the batch size cannot be inferred then a warning is raised and the</span>
<span class="sd">                &quot;batch size&quot; defaults to 1.</span>

<span class="sd">                If the generator does not have a method ``__len__()``, either the ``steps_per_epoch``</span>
<span class="sd">                argument must be provided, or the iterator returned raises a StopIteration exception at</span>
<span class="sd">                the end of the training dataset. PyTorch DataLoaders object do provide a ``__len__()``</span>
<span class="sd">                method.</span>

<span class="sd">                Before each epoch, the method ``__iter__()`` on the generator is called and the method</span>
<span class="sd">                ``__next__()`` is called for each step on resulting object returned by ``__iter__()``.</span>
<span class="sd">                Notice that a call to ``__iter__()`` on a generator made using the python keyword</span>
<span class="sd">                ``yield`` returns the generator itself.</span>
<span class="sd">            valid_generator (optional): Generator-like object for the validation dataset. This generator</span>
<span class="sd">                is optional. The generator is used the same way as the  generator ``train_generator``. If</span>
<span class="sd">                the generator does not have a method ``__len__()``, either the ``validation_steps`` or the</span>
<span class="sd">                ``steps_per_epoch`` argument must be provided or the iterator returned raises a StopIteration</span>
<span class="sd">                exception at the end of the validation dataset.</span>
<span class="sd">                (Default value = None)</span>
<span class="sd">            epochs (int): Number of times the entire training dataset is seen.</span>
<span class="sd">                (Default value = 1000)</span>
<span class="sd">            steps_per_epoch (int, optional): Number of batch used during one epoch. Obviously, using this</span>
<span class="sd">                argument may cause one epoch not to see the entire training dataset or see it multiple times.</span>
<span class="sd">                See argument ``train_generator`` and ``valid_generator`` for more details of how</span>
<span class="sd">                ``steps_per_epoch`` is used.</span>
<span class="sd">            validation_steps (int, optional): Same as for ``steps_per_epoch`` but for the validation dataset.</span>
<span class="sd">                See argument ``valid_generator`` for more details of how ``validation_steps`` is used.</span>
<span class="sd">            batches_per_step (int): Number of batches on which to compute the running loss before</span>
<span class="sd">                backpropagating it through the network. Note that the total loss used for backpropagation is</span>
<span class="sd">                the mean of the `batches_per_step` batch losses.</span>
<span class="sd">                (Default value = 1)</span>
<span class="sd">            initial_epoch (int, optional): Epoch at which to start training (useful for resuming a previous</span>
<span class="sd">                training run).</span>
<span class="sd">                (Default value = 1)</span>
<span class="sd">            verbose (bool): Whether to display the progress of the training.</span>
<span class="sd">                (Default value = True)</span>
<span class="sd">            progress_options (dict, optional): Keyword arguments to pass to the default progression callback used</span>
<span class="sd">                in Poutyne (See :class:`~poutyne.ProgressionCallback` for the available arguments).</span>
<span class="sd">                (Default value = None, meaning default color setting and progress bar)</span>
<span class="sd">            callbacks (List[~poutyne.Callback]): List of callbacks that will be called during</span>
<span class="sd">                training. (Default value = None)</span>

<span class="sd">        Returns:</span>
<span class="sd">            List of dict containing the history of each epoch.</span>

<span class="sd">        Example:</span>
<span class="sd">            .. code-block:: python</span>

<span class="sd">                model = Model(pytorch_network, optimizer, loss_function)</span>
<span class="sd">                history = model.fit_generator(train_generator,</span>
<span class="sd">                                              valid_generator,</span>
<span class="sd">                                              epochs=num_epochs,</span>
<span class="sd">                                              verbose=False)</span>
<span class="sd">                print(*history, sep=&quot;\\n&quot;)</span>

<span class="sd">            .. code-block:: python</span>

<span class="sd">                {&#39;epoch&#39;: 1, &#39;loss&#39;: 1.7198852968215943, &#39;time&#39;: 0.019999928001197986, &#39;acc&#39;: 19.375, &#39;val_loss&#39;: 1.6674459838867188, &#39;val_acc&#39;: 22.0}</span>
<span class="sd">                {&#39;epoch&#39;: 2, &#39;loss&#39;: 1.7054892110824584, &#39;time&#39;: 0.015421080999658443, &#39;acc&#39;: 19.75, &#39;val_loss&#39;: 1.660806336402893, &#39;val_acc&#39;: 22.0}</span>
<span class="sd">                {&#39;epoch&#39;: 3, &#39;loss&#39;: 1.6923445892333984, &#39;time&#39;: 0.01363091799794347, &#39;acc&#39;: 19.625, &#39;val_loss&#39;: 1.6550078630447387, &#39;val_acc&#39;: 22.5}</span>
<span class="sd">                ...</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Impossible to fit when optimizer is None.&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_transfer_optimizer_state_to_right_device</span><span class="p">()</span>

        <span class="n">callbacks</span> <span class="o">=</span> <span class="p">[]</span> <span class="k">if</span> <span class="n">callbacks</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">callbacks</span>

        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="n">progress_options</span> <span class="o">=</span> <span class="p">{}</span> <span class="k">if</span> <span class="n">progress_options</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">progress_options</span>
            <span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">ProgressionCallback</span><span class="p">(</span><span class="o">**</span><span class="n">progress_options</span><span class="p">)]</span> <span class="o">+</span> <span class="n">callbacks</span>
        <span class="n">callback_list</span> <span class="o">=</span> <span class="n">CallbackList</span><span class="p">(</span><span class="n">callbacks</span><span class="p">)</span>
        <span class="n">callback_list</span><span class="o">.</span><span class="n">set_model</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">stop_training</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="n">epoch_iterator</span> <span class="o">=</span> <span class="n">EpochIterator</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">train_generator</span><span class="p">,</span>
            <span class="n">valid_generator</span><span class="p">,</span>
            <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span>
            <span class="n">steps_per_epoch</span><span class="o">=</span><span class="n">steps_per_epoch</span><span class="p">,</span>
            <span class="n">validation_steps</span><span class="o">=</span><span class="n">validation_steps</span><span class="p">,</span>
            <span class="n">initial_epoch</span><span class="o">=</span><span class="n">initial_epoch</span><span class="p">,</span>
            <span class="n">callback</span><span class="o">=</span><span class="n">callback_list</span><span class="p">,</span>
            <span class="n">batch_metrics_names</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_metrics_names</span><span class="p">,</span>
            <span class="n">epoch_metrics_names</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">epoch_metrics_names</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">batches_per_step</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_fit_generator_n_batches_per_step</span><span class="p">(</span><span class="n">epoch_iterator</span><span class="p">,</span> <span class="n">callback_list</span><span class="p">,</span> <span class="n">batches_per_step</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_fit_generator_one_batch_per_step</span><span class="p">(</span><span class="n">epoch_iterator</span><span class="p">,</span> <span class="n">callback_list</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">epoch_iterator</span><span class="o">.</span><span class="n">epoch_logs</span></div>

    <span class="k">def</span> <span class="nf">_fit_generator_n_batches_per_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch_iterator</span><span class="p">,</span> <span class="n">callback_list</span><span class="p">,</span> <span class="n">batches_per_step</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">train_step_iterator</span><span class="p">,</span> <span class="n">valid_step_iterator</span> <span class="ow">in</span> <span class="n">epoch_iterator</span><span class="p">:</span>
            <span class="n">examples_in_step</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_training_mode</span><span class="p">(</span><span class="kc">True</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="n">train_step_iterator</span><span class="p">:</span>
                    <span class="n">step</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="n">get_batch_size</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

                    <span class="n">examples_in_step</span> <span class="o">+=</span> <span class="n">step</span><span class="o">.</span><span class="n">size</span>

                    <span class="p">(</span><span class="n">step</span><span class="o">.</span><span class="n">loss</span><span class="p">,</span> <span class="n">step</span><span class="o">.</span><span class="n">batch_metrics</span><span class="p">,</span> <span class="n">did_backprop</span><span class="p">,</span> <span class="n">_</span><span class="p">,)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fit_batch_n_batches_per_step</span><span class="p">(</span>
                        <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">batches_per_step</span><span class="p">,</span> <span class="n">examples_in_step</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="n">callback_list</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="n">step</span>
                    <span class="p">)</span>

                    <span class="k">if</span> <span class="n">did_backprop</span><span class="p">:</span>
                        <span class="n">examples_in_step</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="n">did_backprop</span><span class="p">:</span>
                <span class="c1"># Did not step after last batch</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_adjust_step_size</span><span class="p">(</span><span class="n">examples_in_step</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

            <span class="n">train_step_iterator</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_loss</span><span class="p">()</span>
            <span class="n">train_step_iterator</span><span class="o">.</span><span class="n">batch_metrics</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_batch_metrics</span><span class="p">()</span>
            <span class="n">train_step_iterator</span><span class="o">.</span><span class="n">epoch_metrics</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_epoch_metrics</span><span class="p">()</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_run_validation</span><span class="p">(</span><span class="n">valid_step_iterator</span><span class="p">,</span> <span class="n">callback_list</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_fit_batch_n_batches_per_step</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">,</span>
        <span class="n">y</span><span class="p">,</span>
        <span class="n">batches_per_step</span><span class="p">,</span>
        <span class="n">examples_in_step</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">callback</span><span class="o">=</span><span class="n">Callback</span><span class="p">(),</span>
        <span class="n">step</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">return_pred</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">convert_to_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="c1"># pylint: disable=too-many-locals</span>
        <span class="n">zero_all_gradients</span> <span class="o">=</span> <span class="p">(</span><span class="n">step</span><span class="o">.</span><span class="n">number</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">batches_per_step</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="n">do_backprop</span> <span class="o">=</span> <span class="n">step</span><span class="o">.</span><span class="n">number</span> <span class="o">%</span> <span class="n">batches_per_step</span> <span class="o">==</span> <span class="mi">0</span>

        <span class="k">if</span> <span class="n">zero_all_gradients</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="n">loss_tensor</span><span class="p">,</span> <span class="n">batch_metrics</span><span class="p">,</span> <span class="n">pred_y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_loss_and_metrics</span><span class="p">(</span>
            <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">return_loss_tensor</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_pred</span><span class="o">=</span><span class="n">return_pred</span><span class="p">,</span> <span class="n">convert_to_numpy</span><span class="o">=</span><span class="n">convert_to_numpy</span>
        <span class="p">)</span>

        <span class="n">adjusted_loss_tensor</span> <span class="o">=</span> <span class="n">loss_tensor</span> <span class="o">*</span> <span class="n">step</span><span class="o">.</span><span class="n">size</span>
        <span class="n">adjusted_loss_tensor</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="n">callback</span><span class="o">.</span><span class="n">on_backward_end</span><span class="p">(</span><span class="n">step</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">do_backprop</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_adjust_step_size</span><span class="p">(</span><span class="n">examples_in_step</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">loss_tensor</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">batch_metrics</span><span class="p">,</span> <span class="n">do_backprop</span><span class="p">,</span> <span class="n">pred_y</span>

    <span class="k">def</span> <span class="nf">_fit_generator_one_batch_per_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch_iterator</span><span class="p">,</span> <span class="n">callback_list</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">train_step_iterator</span><span class="p">,</span> <span class="n">valid_step_iterator</span> <span class="ow">in</span> <span class="n">epoch_iterator</span><span class="p">:</span>
            <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_training_mode</span><span class="p">(</span><span class="kc">True</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="n">train_step_iterator</span><span class="p">:</span>
                    <span class="n">step</span><span class="o">.</span><span class="n">loss</span><span class="p">,</span> <span class="n">step</span><span class="o">.</span><span class="n">batch_metrics</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fit_batch</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="n">callback_list</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="n">step</span><span class="o">.</span><span class="n">number</span><span class="p">)</span>
                    <span class="n">step</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="n">get_batch_size</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

            <span class="n">train_step_iterator</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_loss</span><span class="p">()</span>
            <span class="n">train_step_iterator</span><span class="o">.</span><span class="n">batch_metrics</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_batch_metrics</span><span class="p">()</span>
            <span class="n">train_step_iterator</span><span class="o">.</span><span class="n">epoch_metrics</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_epoch_metrics</span><span class="p">()</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_run_validation</span><span class="p">(</span><span class="n">valid_step_iterator</span><span class="p">,</span> <span class="n">callback_list</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_fit_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="n">Callback</span><span class="p">(),</span> <span class="n">step</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_pred</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">convert_to_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="n">loss_tensor</span><span class="p">,</span> <span class="n">batch_metrics</span><span class="p">,</span> <span class="n">pred_y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_loss_and_metrics</span><span class="p">(</span>
            <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">return_loss_tensor</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_pred</span><span class="o">=</span><span class="n">return_pred</span><span class="p">,</span> <span class="n">convert_to_numpy</span><span class="o">=</span><span class="n">convert_to_numpy</span>
        <span class="p">)</span>

        <span class="n">loss_tensor</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">callback</span><span class="o">.</span><span class="n">on_backward_end</span><span class="p">(</span><span class="n">step</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">loss_tensor</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">batch_metrics</span><span class="p">,</span> <span class="n">pred_y</span>

    <span class="k">def</span> <span class="nf">_run_validation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">valid_step_iterator</span><span class="p">,</span> <span class="n">callback_list</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">valid_step_iterator</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">valid_begin_time</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">default_timer</span><span class="p">()</span>

            <span class="n">callback_list</span><span class="o">.</span><span class="n">on_valid_begin</span><span class="p">({})</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_validate</span><span class="p">(</span><span class="n">valid_step_iterator</span><span class="p">)</span>

            <span class="n">valid_step_iterator</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_loss</span><span class="p">()</span>
            <span class="n">valid_step_iterator</span><span class="o">.</span><span class="n">batch_metrics</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_batch_metrics</span><span class="p">()</span>
            <span class="n">valid_step_iterator</span><span class="o">.</span><span class="n">epoch_metrics</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_epoch_metrics</span><span class="p">()</span>

            <span class="n">valid_total_time</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">default_timer</span><span class="p">()</span> <span class="o">-</span> <span class="n">valid_begin_time</span>

            <span class="n">valid_metrics_log</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;time&#39;</span><span class="p">:</span> <span class="n">valid_total_time</span><span class="p">}</span>
            <span class="n">valid_metrics_log</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">valid_step_iterator</span><span class="o">.</span><span class="n">metrics_logs</span><span class="p">)</span>

            <span class="n">callback_list</span><span class="o">.</span><span class="n">on_valid_end</span><span class="p">(</span><span class="n">valid_metrics_log</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_adjust_step_size</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">examples_in_step</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="o">/=</span> <span class="n">examples_in_step</span>

    <span class="k">def</span> <span class="nf">_process_input</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="n">numpy_to_torch</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">args</span> <span class="o">=</span> <span class="n">torch_to</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">args</span>

    <span class="k">def</span> <span class="nf">preprocess_input</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_process_input</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_process_input</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">))</span> <span class="k">else</span> <span class="p">(</span><span class="n">x</span><span class="p">,)</span>

        <span class="c1"># We return PackedSequence in a tuple since it is a namedtuple, thus an iterator object and</span>
        <span class="c1"># would break later when we call self.network(*x) since it will iterate over the PackedSequence named attribute.</span>
        <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">,)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">PackedSequence</span><span class="p">)</span> <span class="k">else</span> <span class="n">x</span>

        <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="k">if</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">x</span>

<div class="viewcode-block" id="Model.train_on_batch"><a class="viewcode-back" href="../../../model.html#poutyne.Model.train_on_batch">[docs]</a>    <span class="k">def</span> <span class="nf">train_on_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">return_pred</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_dict_format</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">convert_to_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Trains the network for the batch ``(x, y)`` and computes the loss and the metrics, and</span>
<span class="sd">        optionally returns the predictions.</span>

<span class="sd">        Args:</span>
<span class="sd">            x: Input data as a batch.</span>
<span class="sd">            y: Target data as a batch.</span>
<span class="sd">            return_pred (bool, optional): Whether to return the predictions.</span>
<span class="sd">                (Default value = False)</span>
<span class="sd">            return_dict_format (bool, optional): Whether to return the loss and metrics in a dict format or not.</span>
<span class="sd">                (Default value = False)</span>
<span class="sd">            convert_to_numpy (bool, optional): Whether to convert the predictions into Numpy Arrays when ``return_pred``</span>
<span class="sd">                is true. (Default value = True)</span>

<span class="sd">        Returns:</span>
<span class="sd">            Float ``loss`` if no metrics were specified and ``return_pred`` is false.</span>

<span class="sd">            Otherwise, tuple ``(loss, metrics)`` if ``return_pred`` is false.</span>
<span class="sd">            ``metrics`` is a Numpy array of size ``n``, where ``n`` is the</span>
<span class="sd">            number of metrics if ``n &gt; 1``. If ``n == 1``, then ``metrics`` is a</span>
<span class="sd">            float. If ``n == 0``, the ``metrics`` is omitted.</span>

<span class="sd">            Tuple ``(loss, metrics, pred_y)`` if ``return_pred`` is true where</span>
<span class="sd">            ``pred_y`` is the predictions with tensors converted into Numpy</span>
<span class="sd">            arrays.</span>

<span class="sd">            If ``return_dict_format`` is True, then ``loss, metrics`` are replaced by a</span>
<span class="sd">            dictionary.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Impossible to fit when optimizer is None.&quot;</span><span class="p">)</span>

        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_training_mode</span><span class="p">(</span><span class="kc">True</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_transfer_optimizer_state_to_right_device</span><span class="p">()</span>
            <span class="n">loss</span><span class="p">,</span> <span class="n">batch_metrics</span><span class="p">,</span> <span class="n">pred_y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fit_batch</span><span class="p">(</span>
                <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">return_pred</span><span class="o">=</span><span class="n">return_pred</span><span class="p">,</span> <span class="n">convert_to_numpy</span><span class="o">=</span><span class="n">convert_to_numpy</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">return_dict_format</span><span class="p">:</span>
            <span class="n">logs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">)</span>
            <span class="n">logs</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_metrics_names</span><span class="p">,</span> <span class="n">batch_metrics</span><span class="p">))</span>

            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_format_truth_pred_return</span><span class="p">((</span><span class="n">logs</span><span class="p">,),</span> <span class="n">pred_y</span><span class="p">,</span> <span class="n">return_pred</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_format_loss_metrics_return</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">batch_metrics</span><span class="p">,</span> <span class="n">pred_y</span><span class="p">,</span> <span class="n">return_pred</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_format_loss_metrics_return</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span> <span class="n">pred_y</span><span class="p">,</span> <span class="n">return_pred</span><span class="p">,</span> <span class="n">true_y</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_ground_truth</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="c1"># pylint: disable=too-many-arguments</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="p">(</span><span class="n">loss</span><span class="p">,)</span>

        <span class="n">ret</span> <span class="o">+=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">1</span> <span class="k">else</span> <span class="p">(</span><span class="n">metrics</span><span class="p">,)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_format_truth_pred_return</span><span class="p">(</span><span class="n">ret</span><span class="p">,</span> <span class="n">pred_y</span><span class="p">,</span> <span class="n">return_pred</span><span class="p">,</span> <span class="n">true_y</span><span class="p">,</span> <span class="n">return_ground_truth</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_format_truth_pred_return</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">init</span><span class="p">,</span> <span class="n">pred_y</span><span class="p">,</span> <span class="n">return_pred</span><span class="p">,</span> <span class="n">true_y</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_ground_truth</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="c1"># pylint: disable=too-many-arguments</span>
        <span class="k">if</span> <span class="n">return_pred</span><span class="p">:</span>
            <span class="n">init</span> <span class="o">+=</span> <span class="p">(</span><span class="n">pred_y</span><span class="p">,)</span>

        <span class="k">if</span> <span class="n">return_ground_truth</span><span class="p">:</span>
            <span class="n">init</span> <span class="o">+=</span> <span class="p">(</span><span class="n">true_y</span><span class="p">,)</span>

        <span class="k">return</span> <span class="n">init</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">init</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">init</span>

<div class="viewcode-block" id="Model.predict"><a class="viewcode-back" href="../../../model.html#poutyne.Model.predict">[docs]</a>    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
        <span class="n">convert_to_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">progress_options</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">dict</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">callbacks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">dataloader_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the predictions of the network given a dataset ``x``, where the tensors are</span>
<span class="sd">        converted into Numpy arrays.</span>

<span class="sd">        Args:</span>
<span class="sd">            x (Union[~torch.Tensor, ~numpy.ndarray] or Union[tuple, list] of Union[~torch.Tensor, ~numpy.ndarray]):</span>
<span class="sd">                Input to the model. Union[Tensor, ndarray] if the model has a single input.</span>
<span class="sd">                Union[tuple, list] of Union[Tensor, ndarray] if the model has multiple inputs.</span>
<span class="sd">            batch_size (int): Number of samples given to the network at one time.</span>
<span class="sd">                (Default value = 32)</span>
<span class="sd">            concatenate_returns (bool, optional): Whether to concatenate the predictions when returning them.</span>
<span class="sd">                (Default value = True)</span>
<span class="sd">            verbose (bool): Whether to display the progress of the evaluation.</span>
<span class="sd">                (Default value = True)</span>
<span class="sd">            progress_options (dict, optional): Keyword arguments to pass to the default progression callback used</span>
<span class="sd">                in Poutyne (See :class:`~poutyne.ProgressionCallback` for the available arguments).</span>
<span class="sd">                (Default value = None, meaning default color setting and progress bar)</span>
<span class="sd">            callbacks (List[~poutyne.Callback]): List of callbacks that will be called during</span>
<span class="sd">                testing. (Default value = None)</span>
<span class="sd">            dataloader_kwargs (dict, optional): Keyword arguments to pass to the PyTorch dataloaders created</span>
<span class="sd">                internally.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Return the predictions in the format outputted by the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">))</span> <span class="k">else</span> <span class="p">(</span><span class="n">x</span><span class="p">,)</span>
        <span class="n">dataset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dataset_from_data</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict_dataset</span><span class="p">(</span>
            <span class="n">dataset</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="n">concatenate_returns</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">convert_to_numpy</span><span class="o">=</span><span class="n">convert_to_numpy</span><span class="p">,</span>
            <span class="n">dataloader_kwargs</span><span class="o">=</span><span class="n">dataloader_kwargs</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="n">progress_options</span><span class="o">=</span><span class="n">progress_options</span><span class="p">,</span>
            <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="Model.predict_dataset"><a class="viewcode-back" href="../../../model.html#poutyne.Model.predict_dataset">[docs]</a>    <span class="k">def</span> <span class="nf">predict_dataset</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">dataset</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
        <span class="n">steps</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">has_ground_truth</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">return_ground_truth</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">concatenate_returns</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">convert_to_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">collate_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">progress_options</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">dict</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">callbacks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">dataloader_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the predictions of the network given a dataset ``x``, where the tensors are</span>
<span class="sd">        converted into Numpy arrays.</span>

<span class="sd">        Args:</span>
<span class="sd">            dataset (~torch.utils.data.Dataset): Dataset. Must not return ``y``, just ``x``, unless</span>
<span class="sd">                `has_ground_truth` is true.</span>
<span class="sd">            batch_size (int): Number of samples given to the network at one time.</span>
<span class="sd">                (Default value = 32)</span>
<span class="sd">            steps (int, optional): Number of iterations done on ``generator``.</span>
<span class="sd">                (Defaults the number of steps needed to see the entire dataset)</span>
<span class="sd">            has_ground_truth (bool, optional): Whether the generator yields the target ``y``.  Automatically</span>
<span class="sd">                set to true if `return_ground_truth` is true. (Default value = False)</span>
<span class="sd">            return_ground_truth (bool, optional): Whether to return the ground truths. If true, automatically</span>
<span class="sd">                set `has_ground_truth` to true. (Default value = False)</span>
<span class="sd">            concatenate_returns (bool, optional): Whether to concatenate the predictions</span>
<span class="sd">                or the ground truths when returning them. See :func:`predict_generator()`</span>
<span class="sd">                for details. (Default value = True)</span>
<span class="sd">            concatenate_returns (bool, optional): Whether to concatenate the predictions</span>
<span class="sd">                or the ground truths when returning them. (Default value = True)</span>
<span class="sd">            num_workers (int, optional): how many subprocesses to use for data loading.</span>
<span class="sd">                ``0`` means that the data will be loaded in the main process.</span>
<span class="sd">                (Default value = 0)</span>
<span class="sd">            collate_fn (Callable, optional): merges a list of samples to form a mini-batch of Tensor(s).</span>
<span class="sd">                Used when using batched loading from a map-style dataset.</span>
<span class="sd">            verbose (bool): Whether to display the progress of the evaluation.</span>
<span class="sd">                (Default value = True)</span>
<span class="sd">            progress_options (dict, optional): Keyword arguments to pass to the default progression callback used</span>
<span class="sd">                in Poutyne (See :class:`~poutyne.ProgressionCallback` for the available arguments).</span>
<span class="sd">                (Default value = None, meaning default color setting and progress bar)</span>
<span class="sd">            callbacks (List[~poutyne.Callback]): List of callbacks that will be called during</span>
<span class="sd">                testing. (Default value = None)</span>
<span class="sd">            dataloader_kwargs (dict, optional): Keyword arguments to pass to the PyTorch dataloaders created</span>
<span class="sd">                internally.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Depends on the value of ``concatenate_returns``. By default, (``concatenate_returns`` is true),</span>
<span class="sd">            the data structures (tensor, tuple, list, dict) returned as predictions for the batches are</span>
<span class="sd">            merged together. In the merge, the tensors are converted into Numpy arrays and are then</span>
<span class="sd">            concatenated together. If ``concatenate_returns`` is false, then a list of the predictions</span>
<span class="sd">            for the batches is returned with tensors converted into Numpy arrays.</span>

<span class="sd">        See:</span>
<span class="sd">            :class:`~torch.utils.data.DataLoader` for details on ``batch_size``, ``num_workers`` and ``collate_fn``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">dataloader_kwargs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">dataloader_kwargs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">dataloader_kwargs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="n">batch_size</span><span class="p">,</span>
            <span class="s1">&#39;num_workers&#39;</span><span class="p">:</span> <span class="n">num_workers</span><span class="p">,</span>
            <span class="s1">&#39;collate_fn&#39;</span><span class="p">:</span> <span class="n">collate_fn</span><span class="p">,</span>
            <span class="o">**</span><span class="n">dataloader_kwargs</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="n">generator</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="o">**</span><span class="n">dataloader_kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict_generator</span><span class="p">(</span>
            <span class="n">generator</span><span class="p">,</span>
            <span class="n">steps</span><span class="o">=</span><span class="n">steps</span><span class="p">,</span>
            <span class="n">has_ground_truth</span><span class="o">=</span><span class="n">has_ground_truth</span><span class="p">,</span>
            <span class="n">return_ground_truth</span><span class="o">=</span><span class="n">return_ground_truth</span><span class="p">,</span>
            <span class="n">concatenate_returns</span><span class="o">=</span><span class="n">concatenate_returns</span><span class="p">,</span>
            <span class="n">convert_to_numpy</span><span class="o">=</span><span class="n">convert_to_numpy</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="n">progress_options</span><span class="o">=</span><span class="n">progress_options</span><span class="p">,</span>
            <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="Model.predict_generator"><a class="viewcode-back" href="../../../model.html#poutyne.Model.predict_generator">[docs]</a>    <span class="k">def</span> <span class="nf">predict_generator</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">generator</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">steps</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">has_ground_truth</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">return_ground_truth</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">concatenate_returns</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">convert_to_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">progress_options</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">dict</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">callbacks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the predictions of the network given batches of samples ``x``, where the tensors are</span>
<span class="sd">        converted into Numpy arrays.</span>

<span class="sd">        Args:</span>
<span class="sd">            generator: Generator-like object for the dataset. The generator must yield a batch of</span>
<span class="sd">                samples. See the :func:`fit_generator()` method for details on the types of generators</span>
<span class="sd">                supported. This should only yield input data ``x`` and NOT the target ``y``, unless</span>
<span class="sd">                `has_ground_truth` is true.</span>
<span class="sd">            steps (int, optional): Number of iterations done on ``generator``.</span>
<span class="sd">                (Defaults the number of steps needed to see the entire dataset)</span>
<span class="sd">            has_ground_truth (bool, optional): Whether the generator yields the target ``y``.  Automatically</span>
<span class="sd">                set to true if `return_ground_truth` is true. (Default value = False)</span>
<span class="sd">            return_ground_truth (bool, optional): Whether to return the ground truths. If true, automatically</span>
<span class="sd">                set `has_ground_truth` to true. (Default value = False)</span>
<span class="sd">            concatenate_returns (bool, optional): Whether to concatenate the predictions</span>
<span class="sd">                or the ground truths when returning them. (Default value = True)</span>
<span class="sd">            convert_to_numpy (bool, optional): Whether to convert the predictions or ground truths into Numpy Arrays.</span>
<span class="sd">                (Default value = True)</span>
<span class="sd">            verbose (bool): Whether to display the progress of the evaluation.</span>
<span class="sd">                (Default value = True)</span>
<span class="sd">            progress_options (dict, optional): Keyword arguments to pass to the default progression callback used</span>
<span class="sd">                in Poutyne (See :class:`~poutyne.ProgressionCallback` for the available arguments).</span>
<span class="sd">                (Default value = None, meaning default color setting and progress bar)</span>
<span class="sd">            callbacks (List[~poutyne.Callback]): List of callbacks that will be called during</span>
<span class="sd">                testing. (Default value = None)</span>

<span class="sd">        Returns:</span>
<span class="sd">            Depends on the value of ``concatenate_returns``. By default, (``concatenate_returns`` is true),</span>
<span class="sd">            the data structures (tensor, tuple, list, dict) returned as predictions for the batches are</span>
<span class="sd">            merged together. In the merge, the tensors are converted into Numpy arrays and are then</span>
<span class="sd">            concatenated together. If ``concatenate_returns`` is false, then a list of the predictions</span>
<span class="sd">            for the batches is returned with tensors converted into Numpy arrays.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># pylint: disable=too-many-locals</span>
        <span class="n">has_ground_truth</span> <span class="o">=</span> <span class="n">has_ground_truth</span> <span class="ow">or</span> <span class="n">return_ground_truth</span>

        <span class="k">if</span> <span class="n">steps</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">generator</span><span class="p">,</span> <span class="s1">&#39;__len__&#39;</span><span class="p">):</span>
            <span class="n">steps</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">generator</span><span class="p">)</span>
        <span class="n">pred_y</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="n">return_ground_truth</span><span class="p">:</span>
            <span class="n">true_y</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="n">callbacks</span> <span class="o">=</span> <span class="p">[]</span> <span class="k">if</span> <span class="n">callbacks</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">callbacks</span>

        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="n">progress_options</span> <span class="o">=</span> <span class="p">{}</span> <span class="k">if</span> <span class="n">progress_options</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">progress_options</span>
            <span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">ProgressionCallback</span><span class="p">(</span><span class="o">**</span><span class="n">progress_options</span><span class="p">)]</span> <span class="o">+</span> <span class="n">callbacks</span>
        <span class="n">callback_list</span> <span class="o">=</span> <span class="n">CallbackList</span><span class="p">(</span><span class="n">callbacks</span><span class="p">)</span>
        <span class="n">callback_list</span><span class="o">.</span><span class="n">set_model</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="n">callback_list</span><span class="o">.</span><span class="n">set_params</span><span class="p">({</span><span class="s1">&#39;steps&#39;</span><span class="p">:</span> <span class="n">steps</span><span class="p">})</span>

        <span class="n">predict_begin_time</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">default_timer</span><span class="p">()</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_training_mode</span><span class="p">(</span><span class="kc">False</span><span class="p">):</span>
            <span class="n">callback_list</span><span class="o">.</span><span class="n">on_predict_begin</span><span class="p">({})</span>
            <span class="n">time_since_last_batch</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">default_timer</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">_get_step_iterator</span><span class="p">(</span><span class="n">steps</span><span class="p">,</span> <span class="n">generator</span><span class="p">):</span>
                <span class="n">callback_list</span><span class="o">.</span><span class="n">on_predict_batch_begin</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="p">{})</span>

                <span class="k">if</span> <span class="n">has_ground_truth</span><span class="p">:</span>
                    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">preprocess_input</span><span class="p">(</span><span class="o">*</span><span class="n">batch</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">preprocess_input</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

                <span class="n">batch_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">(</span><span class="o">*</span><span class="n">x</span><span class="p">)</span>
                <span class="n">pred_y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch_to_numpy</span><span class="p">(</span><span class="n">batch_pred</span><span class="p">)</span> <span class="k">if</span> <span class="n">convert_to_numpy</span> <span class="k">else</span> <span class="n">batch_pred</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">return_ground_truth</span><span class="p">:</span>
                    <span class="n">true_y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch_to_numpy</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="k">if</span> <span class="n">convert_to_numpy</span> <span class="k">else</span> <span class="n">y</span><span class="p">)</span>

                <span class="n">batch_end_time</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">default_timer</span><span class="p">()</span>
                <span class="n">batch_total_time</span> <span class="o">=</span> <span class="n">batch_end_time</span> <span class="o">-</span> <span class="n">time_since_last_batch</span>
                <span class="n">time_since_last_batch</span> <span class="o">=</span> <span class="n">batch_end_time</span>

                <span class="n">callback_list</span><span class="o">.</span><span class="n">on_predict_batch_end</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;batch&#39;</span><span class="p">:</span> <span class="n">step</span><span class="p">,</span> <span class="s1">&#39;time&#39;</span><span class="p">:</span> <span class="n">batch_total_time</span><span class="p">})</span>

        <span class="k">if</span> <span class="n">concatenate_returns</span><span class="p">:</span>
            <span class="n">pred_y</span> <span class="o">=</span> <span class="n">_concat</span><span class="p">(</span><span class="n">pred_y</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">return_ground_truth</span><span class="p">:</span>
                <span class="n">true_y</span> <span class="o">=</span> <span class="n">_concat</span><span class="p">(</span><span class="n">true_y</span><span class="p">)</span>

        <span class="n">callback_list</span><span class="o">.</span><span class="n">on_predict_end</span><span class="p">({</span><span class="s1">&#39;time&#39;</span><span class="p">:</span> <span class="n">timeit</span><span class="o">.</span><span class="n">default_timer</span><span class="p">()</span> <span class="o">-</span> <span class="n">predict_begin_time</span><span class="p">})</span>

        <span class="k">if</span> <span class="n">return_ground_truth</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">pred_y</span><span class="p">,</span> <span class="n">true_y</span>
        <span class="k">return</span> <span class="n">pred_y</span></div>

<div class="viewcode-block" id="Model.predict_on_batch"><a class="viewcode-back" href="../../../model.html#poutyne.Model.predict_on_batch">[docs]</a>    <span class="k">def</span> <span class="nf">predict_on_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">convert_to_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the predictions of the network given a batch ``x``, where the tensors are converted</span>
<span class="sd">        into Numpy arrays.</span>

<span class="sd">        Args:</span>
<span class="sd">            x: Input data as a batch.</span>
<span class="sd">            convert_to_numpy (bool, optional): Whether to convert the predictions into Numpy Arrays.</span>
<span class="sd">                (Default value = True)</span>

<span class="sd">        Returns:</span>
<span class="sd">            Return the predictions in the format outputted by the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_training_mode</span><span class="p">(</span><span class="kc">False</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">preprocess_input</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">y_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">(</span><span class="o">*</span><span class="n">x</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">torch_to_numpy</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span> <span class="k">if</span> <span class="n">convert_to_numpy</span> <span class="k">else</span> <span class="n">y_pred</span></div>

<div class="viewcode-block" id="Model.evaluate"><a class="viewcode-back" href="../../../model.html#poutyne.Model.evaluate">[docs]</a>    <span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">,</span>
        <span class="n">y</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
        <span class="n">return_pred</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">return_dict_format</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">convert_to_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">callbacks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">progress_options</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">dict</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">dataloader_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes the loss and the metrics of the network on batches of samples and optionally</span>
<span class="sd">        returns the predictions.</span>

<span class="sd">        Args:</span>
<span class="sd">            x (Union[~torch.Tensor, ~numpy.ndarray] or Union[tuple, list] of Union[~torch.Tensor, ~numpy.ndarray]):</span>
<span class="sd">                Input to the model. Union[Tensor, ndarray] if the model has a single input.</span>
<span class="sd">                Union[tuple, list] of Union[Tensor, ndarray] if the model has multiple inputs.</span>
<span class="sd">            y (Union[~torch.Tensor, ~numpy.ndarray] or Union[tuple, list] of Union[~torch.Tensor, ~numpy.ndarray]):</span>
<span class="sd">                Target, corresponding ground truth.</span>
<span class="sd">                Union[Tensor, ndarray] if the model has a single output.</span>
<span class="sd">                Union[tuple, list] of Union[Tensor, ndarray] if the model has multiple outputs.</span>
<span class="sd">            batch_size (int): Number of samples given to the network at one time.</span>
<span class="sd">                (Default value = 32)</span>
<span class="sd">            return_pred (bool, optional): Whether to return the predictions.</span>
<span class="sd">                (Default value = False)</span>
<span class="sd">            return_dict_format (bool, optional): Whether to return the loss and metrics in a dict format or not.</span>
<span class="sd">                (Default value = False)</span>
<span class="sd">            convert_to_numpy (bool, optional): Whether to convert the predictions into Numpy Arrays when ``return_pred``</span>
<span class="sd">                is true. (Default value = True)</span>
<span class="sd">            callbacks (List[~poutyne.Callback]): List of callbacks that will be called during</span>
<span class="sd">                testing. (Default value = None)</span>
<span class="sd">            verbose (bool): Whether to display the progress of the evaluation.</span>
<span class="sd">                (Default value = True)</span>
<span class="sd">            progress_options (dict, optional): Keyword arguments to pass to the default progression callback used</span>
<span class="sd">                in Poutyne (See :class:`~poutyne.ProgressionCallback` for the available arguments).</span>
<span class="sd">                (Default value = None, meaning default color setting and progress bar)</span>
<span class="sd">            dataloader_kwargs (dict, optional): Keyword arguments to pass to the PyTorch dataloaders created</span>
<span class="sd">                internally.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tuple ``(loss, metrics, pred_y)`` where specific elements are omitted if not</span>
<span class="sd">            applicable. If only loss is applicable, then it is returned as a float.</span>

<span class="sd">            ``metrics`` is a Numpy array of size ``n``, where ``n`` is the</span>
<span class="sd">            number of batch metrics plus the number of epoch metrics if ``n &gt; 1``. If</span>
<span class="sd">            ``n == 1``, then ``metrics`` is a float. If ``n == 0``, the ``metrics`` is</span>
<span class="sd">            omitted. The first elements of ``metrics`` are the batch metrics and are</span>
<span class="sd">            followed by the epoch metrics. See the :func:`~Model.fit_generator()` method</span>
<span class="sd">            for examples with batch metrics and epoch metrics.</span>

<span class="sd">            If ``return_pred`` is True, ``pred_y`` is the list of the predictions</span>
<span class="sd">            of each batch with tensors converted into Numpy arrays. It is otherwise omitted.</span>

<span class="sd">            If ``return_dict_format`` is True, then ``loss, metrics`` are replaced by a</span>
<span class="sd">            dictionary as passed to :func:`~poutyne.Callback.on_test_end()`.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">dataset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dataset_from_data</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluate_dataset</span><span class="p">(</span>
            <span class="n">dataset</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="n">return_pred</span><span class="o">=</span><span class="n">return_pred</span><span class="p">,</span>
            <span class="n">return_dict_format</span><span class="o">=</span><span class="n">return_dict_format</span><span class="p">,</span>
            <span class="n">concatenate_returns</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">convert_to_numpy</span><span class="o">=</span><span class="n">convert_to_numpy</span><span class="p">,</span>
            <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="n">progress_options</span><span class="o">=</span><span class="n">progress_options</span><span class="p">,</span>
            <span class="n">dataloader_kwargs</span><span class="o">=</span><span class="n">dataloader_kwargs</span><span class="p">,</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="Model.evaluate_dataset"><a class="viewcode-back" href="../../../model.html#poutyne.Model.evaluate_dataset">[docs]</a>    <span class="k">def</span> <span class="nf">evaluate_dataset</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">dataset</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
        <span class="n">steps</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">return_pred</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">return_ground_truth</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">return_dict_format</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">concatenate_returns</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">convert_to_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">callbacks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">collate_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">dataloader_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">progress_options</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">dict</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">:</span>
        <span class="c1"># pylint: disable=too-many-locals</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes the loss and the metrics of the network on batches of samples and optionally</span>
<span class="sd">        returns the predictions.</span>

<span class="sd">        Args:</span>
<span class="sd">            dataset (~torch.utils.data.Dataset): Dataset.</span>
<span class="sd">            batch_size (int): Number of samples given to the network at one time.</span>
<span class="sd">                (Default value = 32)</span>
<span class="sd">            steps (int, optional): Number of batches used for evaluation.</span>
<span class="sd">                (Defaults the number of steps needed to see the entire dataset)</span>
<span class="sd">            return_pred (bool, optional): Whether to return the predictions.</span>
<span class="sd">                (Default value = False)</span>
<span class="sd">            return_ground_truth (bool, optional): Whether to return the ground truths.</span>
<span class="sd">                (Default value = False)</span>
<span class="sd">            return_dict_format (bool, optional): Whether to return the loss and metrics in a dict format or not.</span>
<span class="sd">                (Default value = False)</span>
<span class="sd">            concatenate_returns (bool, optional): Whether to concatenate the predictions</span>
<span class="sd">                or the ground truths when returning them. (Default value = True)</span>
<span class="sd">            convert_to_numpy (bool, optional): Whether to convert the predictions or ground truths into Numpy Arrays</span>
<span class="sd">                when ``return_pred`` or ``return_ground_truth`` are true. (Default value = True)</span>
<span class="sd">            callbacks (List[~poutyne.Callback]): List of callbacks that will be called during</span>
<span class="sd">                testing. (Default value = None)</span>
<span class="sd">            num_workers (int, optional): how many subprocesses to use for data loading.</span>
<span class="sd">                ``0`` means that the data will be loaded in the main process.</span>
<span class="sd">                (Default value = 0)</span>
<span class="sd">            collate_fn (Callable, optional): merges a list of samples to form a mini-batch of Tensor(s).</span>
<span class="sd">                Used when using batched loading from a map-style dataset.</span>
<span class="sd">            dataloader_kwargs (dict, optional): Keyword arguments to pass to the PyTorch dataloaders created</span>
<span class="sd">                internally.</span>
<span class="sd">            verbose (bool): Whether to display the progress of the evaluation.</span>
<span class="sd">                (Default value = True)</span>
<span class="sd">            progress_options (dict, optional): Keyword arguments to pass to the default progression callback used</span>
<span class="sd">                in Poutyne (See :class:`~poutyne.ProgressionCallback` for the available arguments).</span>
<span class="sd">                (Default value = None, meaning default color setting and progress bar)</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tuple ``(loss, metrics, pred_y)`` where specific elements are omitted if not</span>
<span class="sd">            applicable. If only loss is applicable, then it is returned as a float.</span>

<span class="sd">            ``metrics`` is a Numpy array of size ``n``, where ``n`` is the</span>
<span class="sd">            number of batch metrics plus the number of epoch metrics if ``n &gt; 1``. If</span>
<span class="sd">            ``n == 1``, then ``metrics`` is a float. If ``n == 0``, the ``metrics`` is</span>
<span class="sd">            omitted. The first elements of ``metrics`` are the batch metrics and are</span>
<span class="sd">            followed by the epoch metrics. See the :func:`~Model.fit_generator()` method</span>
<span class="sd">            for examples with batch metrics and epoch metrics.</span>

<span class="sd">            If ``return_pred`` is True, ``pred_y`` is the list of the predictions</span>
<span class="sd">            of each batch with tensors converted into Numpy arrays. It is otherwise omitted.</span>

<span class="sd">            If ``return_dict_format`` is True, then ``loss, metrics`` are replaced by a</span>
<span class="sd">            dictionary as passed to :func:`~poutyne.Callback.on_test_end()`.</span>

<span class="sd">        See:</span>
<span class="sd">            :class:`~torch.utils.data.DataLoader` for details on ``batch_size``, ``num_workers`` and ``collate_fn``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">dataloader_kwargs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">dataloader_kwargs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">dataloader_kwargs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="n">batch_size</span><span class="p">,</span>
            <span class="s1">&#39;num_workers&#39;</span><span class="p">:</span> <span class="n">num_workers</span><span class="p">,</span>
            <span class="s1">&#39;collate_fn&#39;</span><span class="p">:</span> <span class="n">collate_fn</span><span class="p">,</span>
            <span class="o">**</span><span class="n">dataloader_kwargs</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="n">generator</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="o">**</span><span class="n">dataloader_kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluate_generator</span><span class="p">(</span>
            <span class="n">generator</span><span class="p">,</span>
            <span class="n">steps</span><span class="o">=</span><span class="n">steps</span><span class="p">,</span>
            <span class="n">return_pred</span><span class="o">=</span><span class="n">return_pred</span><span class="p">,</span>
            <span class="n">return_ground_truth</span><span class="o">=</span><span class="n">return_ground_truth</span><span class="p">,</span>
            <span class="n">return_dict_format</span><span class="o">=</span><span class="n">return_dict_format</span><span class="p">,</span>
            <span class="n">concatenate_returns</span><span class="o">=</span><span class="n">concatenate_returns</span><span class="p">,</span>
            <span class="n">convert_to_numpy</span><span class="o">=</span><span class="n">convert_to_numpy</span><span class="p">,</span>
            <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="n">progress_options</span><span class="o">=</span><span class="n">progress_options</span><span class="p">,</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="Model.evaluate_generator"><a class="viewcode-back" href="../../../model.html#poutyne.Model.evaluate_generator">[docs]</a>    <span class="k">def</span> <span class="nf">evaluate_generator</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">generator</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">steps</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">return_pred</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">return_ground_truth</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">return_dict_format</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">concatenate_returns</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">convert_to_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">progress_options</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">dict</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">callbacks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">:</span>
        <span class="c1"># pylint: disable=too-many-locals</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes the loss and the metrics of the network on batches of samples and optionally returns</span>
<span class="sd">        the predictions.</span>

<span class="sd">        Args:</span>
<span class="sd">            generator: Generator-like object for the dataset. See the :func:`~Model.fit_generator()` method for</span>
<span class="sd">                details on the types of generators supported.</span>
<span class="sd">            steps (int, optional): Number of iterations done on ``generator``.</span>
<span class="sd">                (Defaults the number of steps needed to see the entire dataset)</span>
<span class="sd">            return_pred (bool, optional): Whether to return the predictions.</span>
<span class="sd">                (Default value = False)</span>
<span class="sd">            return_ground_truth (bool, optional): Whether to return the ground truths.</span>
<span class="sd">                (Default value = False)</span>
<span class="sd">            return_dict_format (bool, optional): Whether to return the loss and metrics in a dict format or not.</span>
<span class="sd">                (Default value = False)</span>
<span class="sd">            convert_to_numpy (bool, optional): Whether to convert the predictions or ground truths into Numpy Arrays</span>
<span class="sd">                when ``return_pred`` or ``return_ground_truth`` are true. (Default value = True)</span>
<span class="sd">            concatenate_returns (bool, optional): Whether to concatenate the predictions</span>
<span class="sd">                or the ground truths when returning them. (Default value = True)</span>
<span class="sd">            verbose (bool): Whether to display the progress of the evaluation.</span>
<span class="sd">                (Default value = True)</span>
<span class="sd">            progress_options (dict, optional): Keyword arguments to pass to the default progression callback used</span>
<span class="sd">                in Poutyne (See :class:`~poutyne.ProgressionCallback` for the available arguments).</span>
<span class="sd">                (Default value = None, meaning default color setting and progress bar)</span>
<span class="sd">            callbacks (List[~poutyne.Callback]): List of callbacks that will be called during</span>
<span class="sd">                testing. (Default value = None)</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tuple ``(loss, metrics, pred_y, true_y)`` where specific elements are</span>
<span class="sd">            omitted if not applicable. If only loss is applicable, then it is returned</span>
<span class="sd">            as a float.</span>

<span class="sd">            ``metrics`` is a Numpy array of size ``n``, where ``n`` is the</span>
<span class="sd">            number of batch metrics plus the number of epoch metrics if ``n &gt; 1``. If</span>
<span class="sd">            ``n == 1``, then ``metrics`` is a float. If ``n == 0``, the ``metrics`` is</span>
<span class="sd">            omitted. The first elements of ``metrics`` are the batch metrics and are</span>
<span class="sd">            followed by the epoch metrics.</span>

<span class="sd">            If ``return_pred`` is True, ``pred_y`` is the predictions returned as in</span>
<span class="sd">            the :func:`predict_generator()` method. It is otherwise ommited.</span>

<span class="sd">            If ``return_ground_truth`` is True, ``true_y`` is the ground truths returned</span>
<span class="sd">            as in the :func:`predict_generator()` method. It is otherwise omitted.</span>

<span class="sd">            If ``return_dict_format`` is True, then ``loss, metrics`` are replaced by a</span>
<span class="sd">            dictionary as passed to :func:`~poutyne.Callback.on_test_end()`.</span>

<span class="sd">        Example:</span>
<span class="sd">            With no metrics:</span>

<span class="sd">            .. code-block:: python</span>

<span class="sd">                model = Model(pytorch_network, optimizer, loss_function,</span>
<span class="sd">                              batch_metrics=None)</span>
<span class="sd">                loss = model.evaluate_generator(test_generator)</span>

<span class="sd">            With only one batch metric:</span>

<span class="sd">            .. code-block:: python</span>

<span class="sd">                model = Model(pytorch_network, optimizer, loss_function,</span>
<span class="sd">                              batch_metrics=[my_metric_fn])</span>
<span class="sd">                loss, my_metric = model.evaluate_generator(test_generator)</span>

<span class="sd">            With several batch metrics:</span>

<span class="sd">            .. code-block:: python</span>

<span class="sd">                model = Model(pytorch_network, optimizer, loss_function,</span>
<span class="sd">                              batch_metrics=[my_metric1_fn, my_metric2_fn])</span>
<span class="sd">                loss, (my_metric1, my_metric2) = model.evaluate_generator(test_generator)</span>

<span class="sd">            With one batch metric and one epoch metric:</span>

<span class="sd">            .. code-block:: python</span>

<span class="sd">                model = Model(pytorch_network, optimizer, loss_function,</span>
<span class="sd">                              batch_metrics=[my_metric_fn], epoch_metrics=[MyEpochMetricClass()])</span>
<span class="sd">                loss, (my_batch_metric, my__epoch_metric) = model.evaluate_generator(test_generator)</span>

<span class="sd">            With batch metrics and ``return_pred`` flag:</span>

<span class="sd">            .. code-block:: python</span>

<span class="sd">                model = Model(pytorch_network, optimizer, loss_function,</span>
<span class="sd">                              batch_metrics=[my_metric1_fn, my_metric2_fn])</span>
<span class="sd">                loss, (my_metric1, my_metric2), pred_y = model.evaluate_generator(</span>
<span class="sd">                    test_generator, return_pred=True</span>
<span class="sd">                )</span>

<span class="sd">            With batch metrics, ``return_pred`` and ``return_ground_truth`` flags:</span>

<span class="sd">            .. code-block:: python</span>

<span class="sd">                model = Model(pytorch_network, optimizer, loss_function,</span>
<span class="sd">                              batch_metrics=[my_metric1_fn, my_metric2_fn])</span>
<span class="sd">                loss, (my_metric1, my_metric2), pred_y, true_y = model.evaluate_generator(</span>
<span class="sd">                    test_generator, return_pred=True, return_ground_truth=True</span>
<span class="sd">                )</span>

<span class="sd">            With ``return_dict_format``:</span>

<span class="sd">            .. code-block:: python</span>

<span class="sd">                model = Model(pytorch_network, optimizer, loss_function,</span>
<span class="sd">                              batch_metrics=[my_metric_fn])</span>
<span class="sd">                logs = model.evaluate_generator(test_generator, return_dict_format=True)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">callbacks</span> <span class="o">=</span> <span class="p">[]</span> <span class="k">if</span> <span class="n">callbacks</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">callbacks</span>

        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="n">progress_options</span> <span class="o">=</span> <span class="p">{}</span> <span class="k">if</span> <span class="n">progress_options</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">progress_options</span>
            <span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">ProgressionCallback</span><span class="p">(</span><span class="o">**</span><span class="n">progress_options</span><span class="p">)]</span> <span class="o">+</span> <span class="n">callbacks</span>

        <span class="k">if</span> <span class="n">steps</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">generator</span><span class="p">,</span> <span class="s1">&#39;__len__&#39;</span><span class="p">):</span>
            <span class="n">steps</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">generator</span><span class="p">)</span>

        <span class="n">callback_list</span> <span class="o">=</span> <span class="n">CallbackList</span><span class="p">(</span><span class="n">callbacks</span><span class="p">)</span>
        <span class="n">callback_list</span><span class="o">.</span><span class="n">set_model</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="n">callback_list</span><span class="o">.</span><span class="n">set_params</span><span class="p">({</span><span class="s1">&#39;steps&#39;</span><span class="p">:</span> <span class="n">steps</span><span class="p">})</span>
        <span class="n">callback_list</span><span class="o">.</span><span class="n">on_test_begin</span><span class="p">({})</span>

        <span class="n">step_iterator</span> <span class="o">=</span> <span class="n">StepIterator</span><span class="p">(</span>
            <span class="n">generator</span><span class="p">,</span> <span class="n">steps</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_metrics_names</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">epoch_metrics_names</span><span class="p">,</span> <span class="n">callback_list</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;test&quot;</span>
        <span class="p">)</span>

        <span class="n">test_begin_time</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">default_timer</span><span class="p">()</span>
        <span class="n">pred_y</span><span class="p">,</span> <span class="n">true_y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate</span><span class="p">(</span>
            <span class="n">step_iterator</span><span class="p">,</span>
            <span class="n">return_pred</span><span class="o">=</span><span class="n">return_pred</span><span class="p">,</span>
            <span class="n">return_ground_truth</span><span class="o">=</span><span class="n">return_ground_truth</span><span class="p">,</span>
            <span class="n">convert_to_numpy</span><span class="o">=</span><span class="n">convert_to_numpy</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">step_iterator</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_loss</span><span class="p">()</span>
        <span class="n">step_iterator</span><span class="o">.</span><span class="n">batch_metrics</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_batch_metrics</span><span class="p">()</span>
        <span class="n">step_iterator</span><span class="o">.</span><span class="n">epoch_metrics</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_epoch_metrics</span><span class="p">()</span>
        <span class="n">test_total_time</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">default_timer</span><span class="p">()</span> <span class="o">-</span> <span class="n">test_begin_time</span>

        <span class="k">if</span> <span class="n">return_pred</span> <span class="ow">and</span> <span class="n">concatenate_returns</span><span class="p">:</span>
            <span class="n">pred_y</span> <span class="o">=</span> <span class="n">_concat</span><span class="p">(</span><span class="n">pred_y</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">return_ground_truth</span> <span class="ow">and</span> <span class="n">concatenate_returns</span><span class="p">:</span>
            <span class="n">true_y</span> <span class="o">=</span> <span class="n">_concat</span><span class="p">(</span><span class="n">true_y</span><span class="p">)</span>

        <span class="n">test_metrics_log</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;time&#39;</span><span class="p">:</span> <span class="n">test_total_time</span><span class="p">}</span>
        <span class="n">test_metrics_log</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">step_iterator</span><span class="o">.</span><span class="n">metrics_logs</span><span class="p">)</span>

        <span class="n">callback_list</span><span class="o">.</span><span class="n">on_test_end</span><span class="p">(</span><span class="n">test_metrics_log</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">return_dict_format</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_format_truth_pred_return</span><span class="p">((</span><span class="n">test_metrics_log</span><span class="p">,),</span> <span class="n">pred_y</span><span class="p">,</span> <span class="n">return_pred</span><span class="p">,</span> <span class="n">true_y</span><span class="p">,</span> <span class="n">return_ground_truth</span><span class="p">)</span>

        <span class="n">metrics</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">step_iterator</span><span class="o">.</span><span class="n">batch_metrics</span><span class="p">,</span> <span class="n">step_iterator</span><span class="o">.</span><span class="n">epoch_metrics</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_format_loss_metrics_return</span><span class="p">(</span>
            <span class="n">step_iterator</span><span class="o">.</span><span class="n">loss</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span> <span class="n">pred_y</span><span class="p">,</span> <span class="n">return_pred</span><span class="p">,</span> <span class="n">true_y</span><span class="p">,</span> <span class="n">return_ground_truth</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="Model.evaluate_on_batch"><a class="viewcode-back" href="../../../model.html#poutyne.Model.evaluate_on_batch">[docs]</a>    <span class="k">def</span> <span class="nf">evaluate_on_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">return_pred</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_dict_format</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">convert_to_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes the loss and the metrics of the network on a single batch of samples and optionally</span>
<span class="sd">        returns the predictions.</span>

<span class="sd">        Args:</span>
<span class="sd">            x: Input data as a batch.</span>
<span class="sd">            y: Target data as a batch.</span>
<span class="sd">            return_pred (bool, optional): Whether to return the predictions for ``batch``.</span>
<span class="sd">                (Default value = False)</span>
<span class="sd">            return_dict_format (bool, optional): Whether to return the loss and metrics in a dict format or not.</span>
<span class="sd">                (Default value = False)</span>
<span class="sd">            convert_to_numpy (bool, optional): Whether to convert the predictions into Numpy Arrays when ``return_pred``</span>
<span class="sd">                is true. (Default value = True)</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tuple ``(loss, metrics, pred_y)`` where specific elements are omitted if not</span>
<span class="sd">            applicable. If only loss is applicable, then it is returned as a float.</span>

<span class="sd">            `metrics`` is a Numpy array of size ``n``, where ``n`` is the</span>
<span class="sd">            number of metrics if ``n &gt; 1``. If ``n == 1``, then ``metrics`` is a</span>
<span class="sd">            float. If ``n == 0``, the ``metrics`` is omitted.</span>

<span class="sd">            If ``return_pred`` is True, ``pred_y`` is the list of the predictions</span>
<span class="sd">            of each batch with tensors converted into Numpy arrays. It is otherwise omitted.</span>

<span class="sd">            If ``return_dict_format`` is True, then ``loss, metrics`` are replaced by a</span>
<span class="sd">            dictionary.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_training_mode</span><span class="p">(</span><span class="kc">False</span><span class="p">):</span>
            <span class="n">loss</span><span class="p">,</span> <span class="n">batch_metrics</span><span class="p">,</span> <span class="n">pred_y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_loss_and_metrics</span><span class="p">(</span>
                <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">return_pred</span><span class="o">=</span><span class="n">return_pred</span><span class="p">,</span> <span class="n">convert_to_numpy</span><span class="o">=</span><span class="n">convert_to_numpy</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">return_dict_format</span><span class="p">:</span>
            <span class="n">logs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">)</span>
            <span class="n">logs</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_metrics_names</span><span class="p">,</span> <span class="n">batch_metrics</span><span class="p">))</span>

            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_format_truth_pred_return</span><span class="p">((</span><span class="n">logs</span><span class="p">,),</span> <span class="n">pred_y</span><span class="p">,</span> <span class="n">return_pred</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_format_loss_metrics_return</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">batch_metrics</span><span class="p">,</span> <span class="n">pred_y</span><span class="p">,</span> <span class="n">return_pred</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_validate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">step_iterator</span><span class="p">,</span> <span class="n">return_pred</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_ground_truth</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">convert_to_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="n">pred_list</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">true_list</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">return_pred</span><span class="p">:</span>
            <span class="n">pred_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="n">return_ground_truth</span><span class="p">:</span>
            <span class="n">true_list</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set_training_mode</span><span class="p">(</span><span class="kc">False</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="n">step_iterator</span><span class="p">:</span>
                <span class="n">step</span><span class="o">.</span><span class="n">loss</span><span class="p">,</span> <span class="n">step</span><span class="o">.</span><span class="n">batch_metrics</span><span class="p">,</span> <span class="n">pred_y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_loss_and_metrics</span><span class="p">(</span>
                    <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">return_pred</span><span class="o">=</span><span class="n">return_pred</span><span class="p">,</span> <span class="n">convert_to_numpy</span><span class="o">=</span><span class="n">convert_to_numpy</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="n">return_pred</span><span class="p">:</span>
                    <span class="n">pred_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pred_y</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">return_ground_truth</span><span class="p">:</span>
                    <span class="n">true_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch_to_numpy</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="k">if</span> <span class="n">convert_to_numpy</span> <span class="k">else</span> <span class="n">y</span><span class="p">)</span>

                <span class="n">step</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="n">get_batch_size</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">pred_list</span><span class="p">,</span> <span class="n">true_list</span>

    <span class="k">def</span> <span class="nf">_compute_loss_and_metrics</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">return_loss_tensor</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_pred</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">convert_to_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">preprocess_input</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">other_device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">pred_y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">other_device</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">pred_y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">(</span><span class="o">*</span><span class="n">x</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_function</span><span class="p">(</span><span class="n">pred_y</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_loss_tensor</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">batch_metrics</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_batch_metrics</span><span class="p">(</span><span class="n">pred_y</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">epoch_metric</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">epoch_metrics</span><span class="p">:</span>
                <span class="n">epoch_metric</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">pred_y</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">return_pred</span><span class="p">:</span>
            <span class="n">pred_y</span> <span class="o">=</span> <span class="n">torch_to_numpy</span><span class="p">(</span><span class="n">pred_y</span><span class="p">)</span> <span class="k">if</span> <span class="n">convert_to_numpy</span> <span class="k">else</span> <span class="n">pred_y</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">pred_y</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">batch_metrics</span><span class="p">,</span> <span class="n">pred_y</span>

    <span class="k">def</span> <span class="nf">_compute_batch_metrics</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pred_y</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">batch_metrics</span> <span class="o">=</span> <span class="p">[</span><span class="n">metric</span><span class="p">(</span><span class="n">pred_y</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="k">for</span> <span class="n">metric</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_metrics</span><span class="p">]</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_metric_array</span><span class="p">(</span><span class="n">batch_metrics</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">original_batch_metrics_names</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_function</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_get_batch_metrics</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">metrics</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch_metric</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span> <span class="k">for</span> <span class="n">batch_metric</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_metrics</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">batch_metric</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_metrics</span><span class="p">:</span>
            <span class="n">batch_metric</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_metric_array</span><span class="p">(</span><span class="n">metrics</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">original_batch_metrics_names</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_epoch_metrics</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">metrics</span> <span class="o">=</span> <span class="p">[</span><span class="n">epoch_metric</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span> <span class="k">for</span> <span class="n">epoch_metric</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">epoch_metrics</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">epoch_metric</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">epoch_metrics</span><span class="p">:</span>
            <span class="n">epoch_metric</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_metric_array</span><span class="p">(</span><span class="n">metrics</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">original_epoch_metrics_names</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_compute_metric_array</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">metrics_list</span><span class="p">,</span> <span class="n">names_list</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">_get_metric</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">metrics</span><span class="p">):</span>
            <span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="n">names</span><span class="p">]</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">else</span> <span class="n">names</span>
            <span class="n">values</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">metrics</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">))</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">values</span> <span class="o">=</span> <span class="p">[</span><span class="nb">float</span><span class="p">(</span><span class="n">metrics</span><span class="p">)]</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">metrics</span><span class="p">,</span> <span class="n">Mapping</span><span class="p">):</span>
                <span class="n">values</span> <span class="o">=</span> <span class="p">[</span><span class="nb">float</span><span class="p">(</span><span class="n">metrics</span><span class="p">[</span><span class="n">name</span><span class="p">])</span> <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">names</span><span class="p">]</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">metrics</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">):</span>
                <span class="n">values</span> <span class="o">=</span> <span class="p">[</span><span class="nb">float</span><span class="p">(</span><span class="n">metric</span><span class="p">)</span> <span class="k">for</span> <span class="n">metric</span> <span class="ow">in</span> <span class="n">metrics</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">values</span> <span class="o">=</span> <span class="p">[</span><span class="nb">float</span><span class="p">(</span><span class="n">metrics</span><span class="p">)]</span>
            <span class="k">return</span> <span class="n">values</span>

        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
            <span class="p">[</span><span class="n">metric</span> <span class="k">for</span> <span class="n">names</span><span class="p">,</span> <span class="n">metrics</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">names_list</span><span class="p">,</span> <span class="n">metrics_list</span><span class="p">)</span> <span class="k">for</span> <span class="n">metric</span> <span class="ow">in</span> <span class="n">_get_metric</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">metrics</span><span class="p">)]</span>
        <span class="p">)</span>

<div class="viewcode-block" id="Model.load_weights"><a class="viewcode-back" href="../../../model.html#poutyne.Model.load_weights">[docs]</a>    <span class="k">def</span> <span class="nf">load_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Loads the weights saved using the :func:`torch.save()` method or the :func:`save_weights()` method</span>
<span class="sd">        of this class. Contrary to :func:`torch.load()`, the weights are not transferred to the device</span>
<span class="sd">        from which they were saved from. In other words, the PyTorch module will stay on the same</span>
<span class="sd">        device it already is on.</span>

<span class="sd">        Args:</span>
<span class="sd">            f: File-like object (has to implement fileno that returns a file descriptor) or string</span>
<span class="sd">                containing a file name.</span>


<span class="sd">        Returns:</span>
<span class="sd">            ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:</span>
<span class="sd">                * **missing_keys** is a list of str containing the missing keys</span>
<span class="sd">                * **unexpected_keys** is a list of str containing the unexpected keys</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">set_weights</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">),</span> <span class="n">strict</span><span class="o">=</span><span class="n">strict</span><span class="p">)</span></div>

<div class="viewcode-block" id="Model.save_weights"><a class="viewcode-back" href="../../../model.html#poutyne.Model.save_weights">[docs]</a>    <span class="k">def</span> <span class="nf">save_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">f</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Saves the weights of the current network.</span>

<span class="sd">        Args:</span>
<span class="sd">            f: File-like object (has to implement fileno that returns a file descriptor) or string</span>
<span class="sd">                containing a file name.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">f</span><span class="p">)</span></div>

<div class="viewcode-block" id="Model.load_optimizer_state"><a class="viewcode-back" href="../../../model.html#poutyne.Model.load_optimizer_state">[docs]</a>    <span class="k">def</span> <span class="nf">load_optimizer_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">f</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Loads the optimizer state saved using the :func:`torch.save()` method or the</span>
<span class="sd">        :func:`save_optimizer_state()` method of this class.</span>

<span class="sd">        Args:</span>
<span class="sd">            f: File-like object (has to implement fileno that returns a file descriptor) or string</span>
<span class="sd">                containing a file name.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">))</span></div>

<div class="viewcode-block" id="Model.save_optimizer_state"><a class="viewcode-back" href="../../../model.html#poutyne.Model.save_optimizer_state">[docs]</a>    <span class="k">def</span> <span class="nf">save_optimizer_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">f</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Saves the state of the current optimizer.</span>

<span class="sd">        Args:</span>
<span class="sd">            f: File-like object (has to implement fileno that returns a file descriptor) or string</span>
<span class="sd">                containing a file name.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">f</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_transfer_optimizer_state_to_right_device</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="c1"># Since the optimizer state is loaded on CPU, it will crash when the optimizer will receive</span>
        <span class="c1"># gradient for parameters not on CPU. Thus, for each parameter, we transfer its state in the</span>
        <span class="c1"># optimizer on the same device as the parameter itself just before starting the</span>
        <span class="c1"># optimization.</span>
        <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">]:</span>
                <span class="k">if</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">state</span><span class="p">:</span>
                    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">p</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">device</span> <span class="o">!=</span> <span class="n">v</span><span class="o">.</span><span class="n">device</span><span class="p">:</span>
                            <span class="n">v</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_named_optimizer_attrs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">param_to_name</span> <span class="o">=</span> <span class="p">{</span><span class="n">param</span><span class="p">:</span> <span class="n">name</span> <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()}</span>

        <span class="n">param_name_groups</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="n">param_name_groups</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">param_to_name</span><span class="p">[</span><span class="n">param</span><span class="p">]</span> <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">]])</span>

        <span class="n">named_state</span> <span class="o">=</span> <span class="p">{</span><span class="n">param_to_name</span><span class="p">[</span><span class="n">param</span><span class="p">]:</span> <span class="n">state</span> <span class="k">for</span> <span class="n">param</span><span class="p">,</span> <span class="n">state</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

        <span class="k">return</span> <span class="n">param_name_groups</span><span class="p">,</span> <span class="n">named_state</span>

    <span class="k">def</span> <span class="nf">_set_named_optimizer_attrs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">param_name_groups</span><span class="p">,</span> <span class="n">named_state</span><span class="p">):</span>
        <span class="n">name_to_param</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">())</span>

        <span class="k">for</span> <span class="n">param_name_group</span><span class="p">,</span> <span class="n">optim_group</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">param_name_groups</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">):</span>
            <span class="n">optim_group</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">name_to_param</span><span class="p">[</span><span class="n">param_name</span><span class="p">]</span> <span class="k">if</span> <span class="n">optim_param</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">name_to_param</span><span class="p">[</span><span class="n">param_name</span><span class="p">]</span> <span class="k">else</span> <span class="n">optim_param</span>
                <span class="k">for</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">optim_param</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">param_name_group</span><span class="p">,</span> <span class="n">optim_group</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">])</span>
            <span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">state</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">dict</span><span class="p">,</span> <span class="p">{</span><span class="n">name_to_param</span><span class="p">[</span><span class="n">name</span><span class="p">]:</span> <span class="n">state</span> <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">named_state</span><span class="o">.</span><span class="n">items</span><span class="p">()})</span>

    <span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
    <span class="k">def</span> <span class="nf">_update_optim_device</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">yield</span>
            <span class="k">return</span>

        <span class="n">param_name_groups</span><span class="p">,</span> <span class="n">named_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_named_optimizer_attrs</span><span class="p">()</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">yield</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_set_named_optimizer_attrs</span><span class="p">(</span><span class="n">param_name_groups</span><span class="p">,</span> <span class="n">named_state</span><span class="p">)</span>

<div class="viewcode-block" id="Model.get_weights"><a class="viewcode-back" href="../../../model.html#poutyne.Model.get_weights">[docs]</a>    <span class="k">def</span> <span class="nf">get_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a dictionary containing the parameters of the network. The tensors are just</span>
<span class="sd">        references to the parameters. To get copies of the weights, see the :func:`get_weight_copies()`</span>
<span class="sd">        method.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span></div>

<div class="viewcode-block" id="Model.get_weight_copies"><a class="viewcode-back" href="../../../model.html#poutyne.Model.get_weight_copies">[docs]</a>    <span class="k">def</span> <span class="nf">get_weight_copies</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a dictionary containing copies of the parameters of the network.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">weights</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="n">weights</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">weights</span></div>

<div class="viewcode-block" id="Model.set_weights"><a class="viewcode-back" href="../../../model.html#poutyne.Model.set_weights">[docs]</a>    <span class="k">def</span> <span class="nf">set_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Modifies the weights of the network with the given weights.</span>

<span class="sd">        Args:</span>
<span class="sd">            weights (dict): Weights returned by either :func:`get_weights()` or :func:`get_weight_copies()`.</span>

<span class="sd">        Returns:</span>
<span class="sd">            ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:</span>
<span class="sd">                * **missing_keys** is a list of str containing the missing keys</span>
<span class="sd">                * **unexpected_keys** is a list of str containing the unexpected keys</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="n">strict</span><span class="p">)</span></div>

<div class="viewcode-block" id="Model.cuda"><a class="viewcode-back" href="../../../model.html#poutyne.Model.cuda">[docs]</a>    <span class="k">def</span> <span class="nf">cuda</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Transfers the network on the GPU. The arguments are passed to the :meth:`torch.nn.Module.cuda()` method.</span>
<span class="sd">        Notice that the device is saved so that the batches can send to the right device before passing it to</span>
<span class="sd">        the network.</span>

<span class="sd">        Note:</span>
<span class="sd">            PyTorch optimizers assume that the parameters have been transferred to the right device</span>
<span class="sd">            before their creations. Furthermore, future versions of PyTorch will no longer modify</span>
<span class="sd">            the parameters of a PyTorch module in-place when transferring them to another device.</span>
<span class="sd">            See this `issue &lt;https://github.com/pytorch/pytorch/issues/7844&gt;`_ and this</span>
<span class="sd">            `pull request &lt;https://github.com/pytorch/pytorch/pull/21613&gt;`_ for details.</span>

<span class="sd">            Since Poutyne supposes that the optimizer has been initialized before the Poutyne Model,</span>
<span class="sd">            necessarily the parameters are not guaranteed to be in sync with those contained in the</span>
<span class="sd">            optimizer once the PyTorch module is transferred to another device. Thus, this method</span>
<span class="sd">            takes care of this inconsistency by updating the parameters inside the optimizer.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `self`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_optim_device</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># Assuming the PyTorch module has at least one parameter.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">other_device</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_transfer_loss_and_metrics_modules_to_right_device</span><span class="p">()</span>

        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="Model.cpu"><a class="viewcode-back" href="../../../model.html#poutyne.Model.cpu">[docs]</a>    <span class="k">def</span> <span class="nf">cpu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Transfers the network on the CPU. The arguments are passed to the :meth:`torch.nn.Module.cpu()`</span>
<span class="sd">        method. Notice that the device is saved so that the batches can send to the right device</span>
<span class="sd">        before passing it to the network.</span>

<span class="sd">        Note:</span>
<span class="sd">            PyTorch optimizers assume that the parameters have been transferred to the right device</span>
<span class="sd">            before their creations. Furthermore, future versions of PyTorch will no longer modify</span>
<span class="sd">            the parameters of a PyTorch module in-place when transferring them to another device.</span>
<span class="sd">            See this `issue &lt;https://github.com/pytorch/pytorch/issues/7844&gt;`_ and this</span>
<span class="sd">            `pull request &lt;https://github.com/pytorch/pytorch/pull/21613&gt;`_ for details.</span>

<span class="sd">            Since Poutyne supposes that the optimizer has been initialized before the Poutyne Model,</span>
<span class="sd">            necessarily the parameters are not guaranteed to be in sync with those contained in the</span>
<span class="sd">            optimizer once the PyTorch module is transferred to another device. Thus, this method</span>
<span class="sd">            takes care of this inconsistency by updating the parameters inside the optimizer.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `self`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_optim_device</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">cpu</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># Assuming the PyTorch module has at least one parameter.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">other_device</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_transfer_loss_and_metrics_modules_to_right_device</span><span class="p">()</span>

        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="Model.to"><a class="viewcode-back" href="../../../model.html#poutyne.Model.to">[docs]</a>    <span class="k">def</span> <span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Transfer the network on the specified device. The device is saved so that the batches can</span>
<span class="sd">        send to the right device before passing it to the network. One could also use multi GPUs by</span>
<span class="sd">        using either a list of devices or &quot;all&quot; to take all the available devices. In both cases,</span>
<span class="sd">        the training loop will use the `~torch.nn.parallel.data_parallel()` function for single</span>
<span class="sd">        node multi GPUs parallel process and the main device is the first device.</span>

<span class="sd">        Note:</span>
<span class="sd">            PyTorch optimizers assume that the parameters have been transferred to the right device</span>
<span class="sd">            before their creations. Furthermore, future versions of PyTorch will no longer modify</span>
<span class="sd">            the parameters of a PyTorch module in-place when transferring them to another device.</span>
<span class="sd">            See this `issue &lt;https://github.com/pytorch/pytorch/issues/7844&gt;`_ and this</span>
<span class="sd">            `pull request &lt;https://github.com/pytorch/pytorch/pull/21613&gt;`_ for details.</span>

<span class="sd">            Since Poutyne supposes that the optimizer has been initialized before the Poutyne Model,</span>
<span class="sd">            necessarily the parameters are not guaranteed to be in sync with those contained in the</span>
<span class="sd">            optimizer once the PyTorch module is transferred to another device. Thus, this method</span>
<span class="sd">            takes care of this inconsistency by updating the parameters inside the optimizer.</span>

<span class="sd">        Args:</span>
<span class="sd">            device (Union[torch.torch.device, List[torch.torch.device]]): The device to which the network is sent or</span>
<span class="sd">            the list of device to which the network is sent.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `self`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">other_device</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">List</span><span class="p">)</span> <span class="ow">or</span> <span class="n">device</span> <span class="o">==</span> <span class="s2">&quot;all&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="s2">&quot;all&quot;</span><span class="p">:</span>
                <span class="n">device</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;cuda:</span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">device</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">())]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># case where we use all when having only one GPU or using a list of one device</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">other_device</span> <span class="o">=</span> <span class="n">device</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>

        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_optim_device</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_transfer_loss_and_metrics_modules_to_right_device</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span></div>

    <span class="k">def</span> <span class="nf">_transfer_loss_and_metrics_modules_to_right_device</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_function</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">loss_function</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">metric</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_metrics</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">epoch_metrics</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">metric</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
                <span class="n">metric</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span></div>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2018-2022, Frdrik Paradis.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-177874682-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-177874682-1');
</script>


</body>
</html>