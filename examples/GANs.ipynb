{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec4b3426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imageio in /home/fredy/.venv/base/lib/python3.9/site-packages (2.22.3)\n",
      "Requirement already satisfied: numpy in /home/fredy/.venv/base/lib/python3.9/site-packages (from imageio) (1.23.4)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /home/fredy/.venv/base/lib/python3.9/site-packages (from imageio) (9.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1aaee8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "import imageio.v3 as iio\n",
    "\n",
    "from poutyne import BaseStrategy, StepOutput, CumulativeAverage, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62f8aae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_channels = 1\n",
    "num_classes = 10\n",
    "image_size = 28\n",
    "latent_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4b7360e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split_percent = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46b7c4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_dataset = MNIST('./datasets', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = MNIST('./datasets', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "num_data = len(full_train_dataset)\n",
    "train_length = int(math.floor(train_split_percent * num_data))\n",
    "valid_length = num_data - train_length\n",
    "\n",
    "train_dataset, valid_dataset = random_split(\n",
    "    full_train_dataset, [train_length, valid_length], generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=2, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3426a4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_images, labels = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "969493bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_in_channels = latent_dim + num_classes\n",
    "discriminator_in_channels = num_channels + num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af8745bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = nn.Sequential(\n",
    "    nn.Conv2d(discriminator_in_channels, 64, 3, stride=2, padding=1),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.AdaptiveMaxPool2d(1),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(128, 1),\n",
    ")\n",
    "\n",
    "generator = nn.Sequential(\n",
    "    nn.Linear(generator_in_channels, generator_in_channels * 7 * 7),\n",
    "    nn.Unflatten(1, (generator_in_channels, 7, 7)),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.ConvTranspose2d(generator_in_channels, 128, 4, stride=2, padding=1),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.ConvTranspose2d(128, 128, 4, stride=2, padding=1),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Conv2d(128, 1, 7, padding=3),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "network = nn.ModuleDict(dict(discriminator=discriminator, generator=generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e51fe47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleDict(\n",
       "  (discriminator): Sequential(\n",
       "    (0): Conv2d(11, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2)\n",
       "    (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (3): LeakyReLU(negative_slope=0.2)\n",
       "    (4): AdaptiveMaxPool2d(output_size=1)\n",
       "    (5): Flatten(start_dim=1, end_dim=-1)\n",
       "    (6): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       "  (generator): Sequential(\n",
       "    (0): Linear(in_features=138, out_features=6762, bias=True)\n",
       "    (1): Unflatten(dim=1, unflattened_size=(138, 7, 7))\n",
       "    (2): LeakyReLU(negative_slope=0.2)\n",
       "    (3): ConvTranspose2d(138, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (4): LeakyReLU(negative_slope=0.2)\n",
       "    (5): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (6): LeakyReLU(negative_slope=0.2)\n",
       "    (7): Conv2d(128, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "    (8): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8aa401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANStrategy(BaseStrategy):\n",
    "    def __init__(self, latent_dim, num_classes):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.batch_metrics_average = CumulativeAverage()\n",
    "\n",
    "    def get_batch_metric_names(self):\n",
    "        return ['g_loss', 'd_loss']\n",
    "\n",
    "    def compute_batch_metrics(self):\n",
    "        return self.batch_metrics_average.compute()\n",
    "\n",
    "    def reset_batch_metrics(self):\n",
    "        self.batch_metrics_average.reset()\n",
    "\n",
    "    def train_step(self, data, **kwargs):\n",
    "        return self._step(data, training=True, **kwargs)\n",
    "\n",
    "    def test_step(self, data, **kwargs):\n",
    "        # Implementing this method allows us to have a validation steps when\n",
    "        # training and enable us to call the evaluate* methods.\n",
    "        return self._step(data, training=False, **kwargs)\n",
    "\n",
    "    def _step(self, data, *, training=True, **kwargs):\n",
    "        # Unpack the data.\n",
    "        real_images, image_labels = data\n",
    "        discriminator = self.model.network.discriminator\n",
    "        generator = self.model.network.generator\n",
    "        discriminator_optimizer, generator_optimizer = self.model.optimizers\n",
    "        device = real_images.device\n",
    "\n",
    "        # Add dummy dimensions to the labels so that they can be concatenated with\n",
    "        # the images. This is for the discriminator.\n",
    "        one_hot_labels = F.one_hot(image_labels, num_classes=self.num_classes)\n",
    "        image_one_hot_labels = one_hot_labels[:, :, None, None]\n",
    "        image_one_hot_labels = image_one_hot_labels.expand(-1, -1, *real_images.shape[-2:])\n",
    "\n",
    "        # Sample random points in the latent space and concatenate the labels.\n",
    "        # This is for the generator.\n",
    "        batch_size = real_images.shape[0]\n",
    "        random_latent_vectors = torch.randn((batch_size, self.latent_dim), device=device)\n",
    "        random_vector_labels = torch.cat((random_latent_vectors, one_hot_labels), axis=1)\n",
    "\n",
    "        # Decode the noise (guided by labels) to fake images.\n",
    "        generated_images = generator(random_vector_labels)\n",
    "\n",
    "        # Combine them with real images. Note that we are concatenating the labels\n",
    "        # with these images here.\n",
    "        fake_image_and_labels = torch.cat((generated_images, image_one_hot_labels), axis=1)\n",
    "        real_image_and_labels = torch.cat((real_images, image_one_hot_labels), axis=1)\n",
    "        combined_images = torch.cat((fake_image_and_labels, real_image_and_labels), axis=0)\n",
    "\n",
    "        # Assemble labels discriminating real from fake images.\n",
    "        labels = torch.cat(\n",
    "            (torch.ones((batch_size, 1), device=device), torch.zeros((batch_size, 1), device=device)), axis=0\n",
    "        )\n",
    "        predictions = discriminator(combined_images)\n",
    "\n",
    "        d_loss = self.model.loss_function(predictions, labels)\n",
    "        if training:\n",
    "            discriminator_optimizer.zero_grad()\n",
    "            generator.zero_grad()\n",
    "            d_loss.backward()\n",
    "            discriminator_optimizer.step()\n",
    "\n",
    "        # Sample random points in the latent space.\n",
    "        random_latent_vectors = torch.randn((batch_size, self.latent_dim), device=device)\n",
    "        random_vector_labels = torch.cat((random_latent_vectors, one_hot_labels), axis=1)\n",
    "\n",
    "        # Assemble labels that say \"all real images\".\n",
    "        misleading_labels = torch.zeros((batch_size, 1), device=device)\n",
    "\n",
    "        # Train the generator (note that we should *not* update the weights\n",
    "        # of the discriminator)!\n",
    "        fake_images = generator(random_vector_labels)\n",
    "        fake_image_and_labels = torch.cat((fake_images, image_one_hot_labels), axis=1)\n",
    "        predictions = discriminator(fake_image_and_labels)\n",
    "\n",
    "        g_loss = self.model.loss_function(predictions, misleading_labels)\n",
    "        if training:\n",
    "            discriminator.zero_grad()\n",
    "            generator_optimizer.zero_grad()\n",
    "            g_loss.backward()\n",
    "            generator_optimizer.step()\n",
    "\n",
    "        batch_metrics = [g_loss, d_loss]\n",
    "        self.batch_metrics_average.update(batch_metrics, batch_size)\n",
    "\n",
    "        return StepOutput(batch_metrics=batch_metrics, x=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0efb898b",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = GANStrategy(latent_dim, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24fbcaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=0.0003)\n",
    "generator_optimizer = optim.Adam(generator.parameters(), lr=0.0003)\n",
    "model = Model(\n",
    "    network, [discriminator_optimizer, generator_optimizer], 'bce_with_logits', strategy=strategy, device='cuda'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a099e16e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35mEpoch: \u001b[36m1/2 \u001b[35mTrain steps: \u001b[36m750 \u001b[35mVal steps: \u001b[36m188 \u001b[32m24.44s \u001b[0m                         \n",
      "\u001b[35mEpoch: \u001b[36m2/2 \u001b[35mTrain steps: \u001b[36m750 \u001b[35mVal steps: \u001b[36m188 \u001b[32m24.30s \u001b[0m                        \n"
     ]
    }
   ],
   "source": [
    "_ = model.fit_generator(train_loader, valid_loader, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51e0fe95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35mTest steps: \u001b[36m157 \u001b[32m2.23s \u001b[0m                                                  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'time': 2.229028018191457}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(test_loader, return_dict_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52022e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35mPrediction steps: \u001b[36m1 \u001b[32m0.00s \u001b[0m                                         \n"
     ]
    }
   ],
   "source": [
    "generator_model = Model(generator, None, None, device='cuda')\n",
    "\n",
    "# Choose the number of intermediate images that would be generated in\n",
    "# between the interpolation + 2 (start and last images).\n",
    "num_interpolation = 9  # @param {type:\"integer\"}\n",
    "\n",
    "# Sample noise for the interpolation.\n",
    "interpolation_noise = torch.randn((1, latent_dim))\n",
    "interpolation_noise = interpolation_noise.expand(num_interpolation, latent_dim)\n",
    "\n",
    "\n",
    "def interpolate_class(first_number, second_number):\n",
    "    first_label = F.one_hot(torch.tensor([first_number]), num_classes=num_classes).float()\n",
    "    second_label = F.one_hot(torch.tensor([second_number]), num_classes=num_classes).float()\n",
    "\n",
    "    # Calculate the interpolation vector between the two labels.\n",
    "    percent_second_label = torch.linspace(0, 1, num_interpolation)[:, None]\n",
    "    interpolation_labels = first_label * (1 - percent_second_label) + second_label * percent_second_label\n",
    "\n",
    "    # Combine the noise and the labels and run inference with the generator.\n",
    "    noise_and_labels = torch.cat((interpolation_noise, interpolation_labels), axis=1)\n",
    "    fake = generator_model.predict(noise_and_labels, convert_to_numpy=False).cpu()\n",
    "    return fake\n",
    "\n",
    "\n",
    "start_class = 1  # @param {type:\"slider\", min:0, max:9, step:1}\n",
    "end_class = 5  # @param {type:\"slider\", min:0, max:9, step:1}\n",
    "\n",
    "fake_images = interpolate_class(start_class, end_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0d7e715",
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_images = 255.0 * fake_images\n",
    "converted_images = F.interpolate(converted_images, (96, 96)).byte()\n",
    "converted_images = converted_images.expand(-1, 3, -1, -1)\n",
    "converted_images = torch.movedim(converted_images, 1, 3)\n",
    "converted_images = converted_images.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a591c4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fredy/.venv/base/lib/python3.9/site-packages/imageio/core/request.py:267: UserWarning: The usage of `format_hint` is deprecated and will be removed in ImageIO v3. Use `extension` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/gif": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gif_data = iio.imwrite(\"<bytes>\", converted_images, format_hint=\".gif\", duration=1000)\n",
    "from IPython.display import Image\n",
    "\n",
    "display(Image(data=gif_data, format='gif'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf58bbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
