{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch and Poutyne\n",
    "\n",
    "In this notebook, we train a simple fully-connected network and a simple convolutional network on MNIST. First, we train it by coding our own training loop as the PyTorch library expects of us to. Then, we use Poutyne to simplify our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the package needed.\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.dataset import Subset\n",
    "\n",
    "from torchvision import transforms, utils\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "\n",
    "from poutyne.framework import Model, ModelCheckpoint, CSVLogger, Callback, Experiment\n",
    "from poutyne import torch_to_numpy, set_seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Pythons's, NumPy's and PyTorch's seeds so that our training are (almost) reproducible.\n",
    "set_seeds(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basis of Training a Neural Network\n",
    "\n",
    "In **stochastic gradient descent**, a **batch** of `m` examples are drawn from the train dataset. In the so-called forward pass, these examples are passed through the neural network and an average of their loss values is done. In the backward pass, the average loss is backpropagated through the network to compute the gradient of each parameter. In practice, the `m` examples of a batch are drawn without replacement. Thus, we define one **epoch** of training being the number of batches needed to loop through the entire training dataset.\n",
    "\n",
    "In addition to the training dataset, a **validation dataset** is used to evaluate the neural network at the end of each epoch. This validation dataset can be used to select the best model during training and thus avoiding overfitting the training set. It also can have other uses such as selecting hyperparameters\n",
    "\n",
    "Finally, a **test dataset** is used at the end to evaluate the final model.\n",
    "\n",
    "## Training constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on GPU if one is present\n",
    "cuda_device = 0\n",
    "device = torch.device(\"cuda:%d\" % cuda_device if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# The dataset is split 80/20 for the train and validation datasets respectively.\n",
    "train_split_percent = 0.8\n",
    "\n",
    "# The MNIST dataset has 10 classes\n",
    "num_classes = 10\n",
    "\n",
    "# Training hyperparameters\n",
    "batch_size = 32\n",
    "learning_rate = 0.1\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the MNIST dataset\n",
    "\n",
    "The following loads the MNIST dataset and creates the PyTorch DataLoaders that split our datasets into batches. The train DataLoader shuffles the examples of the train dataset to draw the examples without replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_dataset = MNIST('./mnist/', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = MNIST('./mnist/', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "num_data = len(full_train_dataset)\n",
    "indices = list(range(num_data))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "split = math.floor(train_split_percent * num_data)\n",
    "\n",
    "train_indices = indices[:split]\n",
    "train_dataset = Subset(full_train_dataset, train_indices)\n",
    "\n",
    "valid_indices = indices[split:]\n",
    "valid_dataset = Subset(full_train_dataset, valid_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "loaders = train_loader, valid_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at some examples of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe3ac234860>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlkAAAE8CAYAAADzKaQzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3XncFeP/x/HXKMq+R0SFLKlkaSGUNSVZsiQSsmSrrNn6Zd/J9rUkuyRLhCwlUbaQJbK0kJSUJVskMb8/6jPXNec+937mbPf7+Xj0uKeZOedc97nnzJn5XJ/rcwVhGCIiIiIimbVCrhsgIiIiUox0kSUiIiKSAF1kiYiIiCRAF1kiIiIiCdBFloiIiEgCdJElIiIikgBdZImIiIgkILGLrCAI9guC4MsgCGYEQXB+Uq8jIiIiko+CJIqRBkFQC5gG7APMAd4DjgzD8LOMv5iIiIhIHqqd0PO2BmaEYfgVQBAEjwEHAmkvsoIgUNl5ERERKRQ/hmG4fnk7JdVduDHwrff/OcvXiYiIiBS6byqyU1KRrCDNuli0KgiCk4CTEnp9ERERkZxK6iJrDrCJ9/8GwHf+DmEYDgGGgLoLRUREpPgk1V34HtAkCILGQRCsBHQHnk3otURERETyTiKRrDAMlwZBcDrwMlALuC8Mw6lJvJaIiIhIPkqkhEOlG6HuQhERESkck8Mw3Km8nVTxXURERCQBusgSERERSUBSowtFaoTatd1H6OKLLwZg4MCBAAwdOhSAVVZZJdqnZ8+eWWydiIjkkiJZIiIiIgnQRZaIiIhIAtRdKBW2xhprAPDLL79E6/bcc08AXnvttVw0Keesi9BfthG7nTt3BmCvvfbKfsNERCTnFMkSERERSYAiWVKu1VZbDYAnn3wScJEagJdffhmALl26ADB27Ngsty43nnnmGQAOOOCAaJ29L6+++ioAZ555JgDTpk3Lcuuy6/333wdg++23L3WfiRMnAjBy5Mho3a233ppswyQvrLTSSgDUqVMHgEGDBkXbzj77bABOOeUUAIYNG1bq8/z555/R8r///pvxdookQZEsERERkQSo4rukdfjhh0fLN998MwAbbLABAB988EG0berUZbMlNWjQAIjfbVq5gl9//TXZxmaRRbA6deoEwOLFi6NtBx10EABvvvkmAEuWLMly63LDIlI77LBDiW2TJ08GoFu3boCLagDsuuuuQPFH+moqK11y1llnAXDJJZdU6/n23XffaPnzzz8HYMGCBUDNi2z16dMHgDvvvDNa99VXXwHufZo5c2b2G1azqOK7iIiISK4oJ0tiWrRoAcA999wTrbOCmyNGjADgjDPOiLb99NNPAKy99tqAu8ME2HDDDYHCjWSttdZagCsuCtCqVSvARbBsdCW4qE1N07dv33L3Wbp0KQD9+vWL1lkejuWu5aMgCADYdNNNo3WWf2i+/PLLaNkiu7///jsAf/zxR9JNzFvdu3cHqh/BMmPGjCmxbqONNgJcRCsf7bLLLtHyVlttBcBDDz0EVD8C999//0XLjRo1AuDRRx8FoE2bNtV67nxSv359ACZMmBCt69+/PwCjR4/OSZsqSpEsERERkQToIktEREQkAeouzKJatWoBrjhl+/bto22WJH3ppZdmv2GeZs2aAa5sA7gim1dffXWpj1u4cCEA++23X7Ru9uzZSTQxa7bccksg3sVlBg8eDNTcLsLKOuGEE0qssy6AfGZlSw4++OBKPW7WrFkATJkypcRzWZmCfBh0VOiaN28OwPjx46N1fhdaLnXs2BFw3XfgUhDs2L/qqqsq9Zx2ThowYECp+xT6eTedhx9+GIDNN988WmcDKtRdKCIiIlIDKZKVwoaYW9Tpr7/+KnVf22f11VeP1lkpg9122w2AQw89NNpmCZCWNH3XXXdF22688cZqt706bPh9ugKRc+bMqfDzfPTRRxlrU675U+YYey8eeOCBLLemcFipD3DlPmxKJj/K4Cex5qumTZtW6XGWhGw/Abp27Qq4hPDLLrsMgHfffbfqDazhrBiyHV8QLyOTC3bMWPTFole+9ddfv0rPfcQRRwDxgRjGBltYyZ1C1bJly2jZBgjYwCp/oIB9/+Y7RbJEREREEqBIFrDNNttEy7fccgsAjRs3BqBHjx6x/wM0bNgQcH3u/jB+G/L9888/A67sAcCzzz4be418YmUZ7I7Bj17V1Mmf999/fyCeO2O5RTZUXxzLF3njjTeideussw7gIlj+seR/NvKV5RgeffTRVXq8TRIO7v2xdTbEfqedXD3Db775pkqvU4xsuqYffvih3H3zqRipRS/XXXfdUvexKFcmffjhh4ArhlxorDyOH+G2qZjse7UQ8xgVyRIRERFJgC6yRERERBJQI7sLLTHRyhX48z9Zd5mZNGlSicfPnz8fcJWe77jjjmibdZWMGzcOqFioOx+kJvh+99130fK3336b7ebklA0NXmGFZfcgfrJ2Ta7gXZrbbrsNgN133x1wXYQ+68Lw58S02QLymXXfXXnllVV6vP+49dZbD3AV8m1ghZUDATjkkEOq9DrFaOzYsYBLr/jxxx9z2ZwKO++880rdZikjH3/8caWe00rqpM42UMisK9DOCTYQzBL4waXk2KCxK664Itq26qqrArDiiisC8M8//yTc4qpRJEtEREQkATUmkmVXxACjRo0CXLkG399//w3Aq6++CsBjjz0GxO88pk+fDpRd3qHQXXvttbluQs5YcqVFsAox2TKb2rZtC8C2225b6j42iMC/S62pTj755Nj/q1omIt9NmzYNgPvvvx+A7bbbDnDlYspzwQUXAC6i7Je5sYFFhcC+U8D9DpVN1LcIjj9IotB169YNgAcffBBwvT7+YBEr9mzJ8P/3f/8Xbdtxxx0BeO655wAYOnRotM0K/+YDRbJEREREElD0kaw111wTiPeTWwTLhrzalTC4O4alS5dmq4mSZ6zgn1SMTQV1zz33AFCvXr0S+8ydOze2D7gosX0Oi/Ezd+SRR0bL//vf/wBXnNIipemK3hYDy0+1n/vssw8Qz6uxaERZbAqZxx9/PFqXb5Esi9QC7LrrrrFtfiTLL28CsNVWW0XL/pRk4L67AE477bRy23DhhRdWrLE55E+LY+cCy3O197CyU5Xtu+++QDyfWpEsERERkSKniywRERGRBBR9d+FBBx0ExEO4NuT8mmuuAWDevHnZb1iesYq69rMmq+rxYMOrrfunfv36QPw9tSR66yrzu04KNSn8+eefB1xJlD322CPaZhXybX7PvfbaK9rWr18/AHbeeWcA3nvvveQbmzDrCrR5Cf1untTPls0xl09dG0mykgz++3D55ZcDFes2PP7446PlSy65BICFCxdmsIVV53f5pv6d/bltU8vhrLzyytFyavmginjhhRei5cp2s+WC//vasp0LrcJ/WSw1AeDqq6+ObbPzSb5RJEtEREQkAUUfybJEOytYBtChQwfAJThbuQZwRd+s4KjdKQ0cODDax09kLBYWYalIuQKLPIAr3GnJhxYdBFdcLl/uNivKEjHTFSO1OR4PO+wwIH6X2rt377TP99tvv0XLs2bNAuCcc84BXOQDSg7tLzRWXNSPzKRGaXr16hUt33vvvYCLAhx44IFJNzERtWu70+hTTz0FxKN5xo4ji2DZMVDTjBkzJlq2842VdfAHClhk1PhRQXvPLSk+10WC7fsiHT+ytdFGG2X0df3z7ZIlSzL63NliRYxr1aoFpC9vYVGvTp06lfo8+Vr4W5EsERERkQQUfSTrkUceAaBJkybRutS7zD333LPUx9tM6rNnz47W3X777ZlsYsGwOw6/5IVFcuyO1M8x2nLLLQE47rjjstXEjPjggw8A6NGjBxCP7qWWd/DvUi2n6tNPP43t8/LLL0fLNk3LfffdB8SjX5aT4Jc5yDX7+9rwasufqCy7g0/32bHctUJjd96jR4+O1qWeW/78889o+cQTTwRg+PDhWWhdYbA8Lfvp52alRrJ8FvW1HJ1cR7LsbwuuuKad/2z6F4hPV5bKPmt+mYPSWB6knasKhf89OnHiRAB22203AMaPHw+kn27Lotz+NHcWuUtXVDyfKJIlIiIikgBdZImIiIgkoOi7C23+LD+hsiIsIfnzzz8HoGHDhpltWJ7r379/tGyJ20888QTgBgf4rAvRT1q0Cs+2zR+Cna9JihVlx5X/O1l34dSpU0t9nHUhWMXqddZZJ9pmCeD51F1oYfpTTjkFqHp3oQ25X2WVVUps87uYC4kNhrHj3Gfnja5du0brZs6cWenXWG+99aLl9ddfH3DpDfZ/gF9++QVwx6ANKsh3Rx99NOC6WStSyiEf+eezY445BnDJ2pZyAvDRRx+V+hwbbrgh4NILyuouveGGG4DCmz/XHwRkJW/uvvtuALp37w7EBxrZd8edd94JQN++faNt9p5PmTIFqNrnKxsUyRIRERFJQJUjWUEQbAI8BGwI/AcMCcPwliAI1gFGAI2AWcDhYRgW1hh+3JxSG2ywARAvAVGMZsyYAbhZ3hs1ahRtO/TQQ4F4uQFjSZ5WdNKPZA0ePBhwdx/NmzePtvllMwrR999/D8A///wTrSsrgmUsudUiYW3bto22+bPI54vtt98eqFhpj3TsDtQS/P3nGTlyJOCSngtNugiWDeW3ZN6y5tjbbLPNANhll12idVYepWXLlgC0aNEi2uYnUEM8imERkkGDBlX8F8iyQw45BHARC4DtttsOqFiyd6H48ccfY/9PLUBaGjun2DGULpJl59eyImKFwqKuRx11FBDvFTCLFy+O/d//zNncjvleTLw6kaylwNlhGG4DtAVOC4KgKXA+MC4MwybAuOX/FxEREalRqhzJCsNwHjBv+fLvQRB8DmwMHAh0WL7bg8BrwIBqtbIKLG9h+vTpQMXvJmxYtkVmjF9ArxhZkU2L4FkEA9IXhzNW2NWG0fp325bPY/3wfn98PrPh4HZnabkSvvbt2wNu2DG4KZzefPNNIH3RWht23LhxYyBeAsKfIqOQ9enTJ1r2i49CvLyFRbcKLa/EjBo1CogX57VIZUWic1aAs6wooX8Mvf322wCMGzcOgMcffzzallo2JElbbLFFtHzrrbdW+HEWufMfL5VjU8kU6hRc6djxnxq1SscKRIM7d77++uvJNCxDMpKTFQRBI2B7YBKwwfILMLsQq5eJ1xAREREpJNUeXRgEwWrAU0D/MAx/q+gEw0EQnAScVN3XFxEREclH1brICoJgRZZdYA0Lw3Dk8tXzgyCoH4bhvCAI6gML0j02DMMhwJDlz1O1rNoy3HbbbYCbN2/XXXet0OM23nhjALp16wbAe++9B8Arr7yS6SbmFUvQtYrc/hBwG2ZtXajp5iK0BPBHH300WmdlLyozy3o+sBIK1h1kCdo+6+rxu0etS9m6EK+77jog3iVoz20DKoYMGRJtmzx5cmZ+gQyy37MirOr1TTfdFK2zbmTrgvXnnyv0Lg+bg7BNmzbROptbze9uL411kyxdujRaZxWx7bxz/fXXR9tyVd3bShA8/fTTQPyYt8T1bLOSPFa6ohhssskmgPsOkoopa3BJPqhyd2Gw7JvjXuDzMAxv8jY9C1giRi9gVNWbJyIiIlKYqhPJagf0BD4JgsDGk14IXAM8HgRBb2A2cFj1mlg1VjjTCsP5yZZWrsC0atUqWn7ggQdi2/r16wcU7gznlfXxxx8D8STENdZYA3BJzDbk1nfHHXcA8eHoFpmx97DQLFiwLAibLgpqUT0rpOezwor2049kWfTirrvuAuCss87KYIszz6InNm9lnTp1gHiBxWeffRZIH72xCJYNRMnHaF1V2TnBot4AZ555JgDHHnss4N4vcMVDrVCpJY3787HZ8P9FixYl1OqK8YvGWmS2adOmuWoOEC/9YEUqi+m8vPXWW8d++qxApx07NZXNoerL90Fp1Rld+AZQWgLWXlV9XhEREZFiEFS1yGBGG5FATpYV03zppZcA+PXXX6NtX375JQDt2rUD4n3gNmz/iCOOAOJ3mTWJP/TeojXpCrLakFq70/KLc1o0sWfPnom1M9f8SJaVJEjlRyUuu+wywOUM5vud+DnnnAPANddcA7iohl9E1aIedi756aefom1WgNLKWtQUlotmJWEA6tatC6TPacw3VooCctde+zxNnDgRgLlz50bb0pVHKUT16rnB98OHDwdcWRyf/b7ppqWqSd55551ouXXr1gA0aNAAgO+++y7bzZkchuFO5e2kaXVEREREElC0kSxjd2Q2+S64iTuNX1DSrpTzvVR/NlkO1kMPPVRiW2oky0Yigrszk8JlOUU2YvDkk08usY/lnFk+0WGHuTTMCRMmJN1ESUC2Ill2Xr7vvvtKbLPeh2KJWqXTo0ePaPnhhx8udT/L4fRH59ZEf/75Z7RskWFFskRERERqIF1kiYiIiCSg2hXf850VPBwwIOvTJxaNYcOGxX5KzWFdNVYU00Lz/lDqgw8+GHBd7Vb6QoqTlXkB2G233WLbbKADwF57LRtkXlYXlw2U8QfM1CR+V6gVpa1du+TX8hdffJG1NuUzKwkDrrvQBpnkK0WyRERERBJQ9InvIiIi+e7yyy8H4MILLwTiZYf23ntvIHdTK+WLZs2aRcs29ZS9JzalFcBvv/2WjeYo8V1EREQkVxTJEhERkYJiBbNtSrcWLVpE22bOnJmNJiiSJSIiIpIrusgSERERSYC6C0VEREQqR92FIiIiIrmiiywRERGRBOgiS0RERCQBusgSERERSYAuskREREQSoIssERERkQToIktEREQkAbrIEhEREUmALrJEREREEqCLLBEREZEE6CJLREREJAG6yBIRERFJgC6yRERERBJQO9cNEBEREUnVuHFjAF5//XUAXnnllWjboEGDAPj222+z37BKUCRLREREJAFBGIa5bgNBEOS+ESIiIpI32rdvD8D48eNLbJs+fToA++yzDwCzZ8/OXsOWmRyG4U7l7aRIloiIiEgCFMkSKcVHH30EQIsWLaJ18+bNA+K5AamCIADgySefBODXX3+NtllugYiIlK1BgwYAnHPOOQCceuqp0bbatZellE+bNg2A3XbbLdr2ww8/ZKN5imSJiIiI5IouskREREQSoO7CKmjevDkAJ554Yrn7Tp06NVp+/PHHAVi4cGEyDauGrbbaCoDTTz8dgCZNmkTbbIjsc889B8CYMWOibYsXL85WExN12WWXRcsnnHACAOuuuy4QT6hcZ511AFhrrbVKfS7rLrTP1n///RdtszD2DTfcAMBtt90Wbfvnn3+q/gvkqTp16gDQoUMHADp27Bhta9asGQAvv/wyALfeemu0rRjfC1nGjgWAvffeG4Bzzz0XiB8fG264YWwf+wku2dmSn6VmGDt2bLS81157xbb1798/WvbPJQlSd6GIiIhIriiSVY7tt98eiF8ld+/eHXCJdxU1YcIEAPbYY48Mta76jj76aADuuecewEUeyvLQQw9Fy1dddRXgkg8L1eqrrx4tW5Jl3bp1Abj33nujbX/99Rfg7qK22WabaNtmm20GQLt27QB3fFhEzGfRrvr160fr5s+fX83fIj+suuqq0bIdKwcffHC5j+vUqVO0bNEtKR4DBw4EXNQKYJVVVgFKRn/L89JLLwHQpUuXTDZR8tymm24aLU+cOBGATTbZBICHH3442tarV69sNEeRLBEREZFcqfa0OkEQ1ALeB+aGYdglCILGwGPAOsAHQM8wDJdU93WyZYUVll139uvXD4BLL70UiN+d//bbbwBcccUVADz11FOlPt8111wTLbdu3RqAlVZaCYCePXtG2/xoSTbZ77viiitW+DHHHHNMtGy5EZYv8fnnn2ewddnz+++/R8s2XUNZHnjggXL3seiYf3yk5hEUIz/PrCIRLKmYAw44AIBRo0YBMHr06Ghb7969AViwYEH2G1YKO2fuueeeAJx11lmAi14JrLHGGtGy9Zo0bNgQSN/jYfmx9erVA9L3POy4445AvKeladOmGWpxbvn5sffddx/gztf+e2nfZ/mQ25mJSFY/wP9mvRYYHIZhE2Ah0DsDryEiIiJSUKoVyQqCoAGwP3AlcFawrGN9T6DH8l0eBC4B7qzO6yRt4403jpYtx8hylSwH55JLLon2ue666wD4+++/K/U6dodiz+lHOHIVybKcmU8++QRwIyb79OlTocdbTtG4ceMA6Ny5c7TNinnWVJab5Y+KMp9++ingoqLFoGvXrgAccsghlXrczJkzAXjvvfcy3qZC16pVq2g5dTTz/vvvHy3vvvvugCuAmw8OO+wwAIYOHZrx5/7+++8z/pzZUKtWLcCdG/w8IsstSsJBBx0EwDPPPJPYa2SbfWeZAw88MFq276UcTLVTQnUjWTcD5wE2Rn1d4JcwDJcu//8cYON0DxQREREpZlW+yAqCoAuwIAzDyf7qNLumHS4SBMFJQRC8HwTB+1Vtg4iIiEi+qk53YTugaxAEnYG6wBosi2ytFQRB7eXRrAbAd+keHIbhEGAI5L6Eg1+I0roJP/74Y8CF6CdPnlzygWU49NBDgXhI34wYMQJwRS/zwRdffAG4sHJlWeFAK7IJsN9++wGwdOnStI8pNmuvvTYAZ555ZuxnumHpxx57LOC6jgvZ5ptvDriuj0mTJkXbrBivDSRJ548//gDg559/TqqJWbf11lsDMHLkyGiddQ1bN9qRRx4JwCmnnFLq81iCM8DKK69c6n5TpkypemMzyC/Dcf3111frueyzYedeK+YMMHz48Go9dza1bds2Wj7iiCOAsj8PSfAHbkl2VTmSFYbhBWEYNgjDsBHQHXg1DMOjgPHAoct36wWMqnYrRURERApMtUs4pDEAeCwIgiuAD4HcZHRXgM3abXcX4CI6AwYMACoWwfLvMM8777zYT38Y7RNPPAHA8ccfD+THlDTWvrvuugtwEal///032seiLq+99lqJx9sw4/vvvx9ww7UBhgwZArjfN5+tueaa0bIlpVpxUT+Z2LY1bty4xHPYEGyLUKSbVufKK68E4IMPPshY23PNkkttmhy/jIC9F+lYBMuG9heDli1bAi6KYxEt3zfffFOt1/jpp58AuPjii6N1+VLI1qblAhfZrYxbbrklWn700UeByvci5Jvzzz8/WrbBIWWxiK5f9Dm1NM5NN90ElB2hsu8bKKzIX7HJyEVWGIavAa8tX/4KaJ2J5xUREREpVElEsgqGTfPgswKj/kSUqazQ2b777gvEC1PaBMJff/01EJ+O5/nnn69egxNgRQz9wqgAp556arQ8bNiwUh//yCOPAC5ac/bZZ0fb/Clj8tVJJ50ExAuQWjTP+HfXlZ3+A+J3kX4pkGJhBf+sUKJ9hsBNUZTORRddBMD48eMTbF1yrGyCP02MFYJMPYYyyY7Vu+++O7HXyDaL7PrFm20y9UJlPRzp8nLTsamk7Jz83XcundmKRlsPSVn5eZbz6/cg+NH0YuVH0CtbXilJmlZHREREJAG6yBIRERFJQFCZbo/EGpGjEg7WvbFkiZta0YbbpoaqbY4+cAmnu+66a4nntO4RS1q0JNV84id5v/nmm0DJua38aro//vgjAPfccw8As2bNira98847STUzUdad44fkq8sSdK3LyPhlGqxERlnd0YWqefPmALz00kvRutQuY7/r9KijjspOwzLABsmA+x3WW289wM1F6rOBI35FfxsIYp599lnAfQbTsWr44Crp22wB+XDuTuXPp9ixY8dy97fSEzan58KFC5NpWA48+OCDQMlUDJ91EYIr7WEDQnzWNZxa9d9nie42t2w+dZklwarX22ACv3p+r169stGEyWEY7lTeTopkiYiIiCSgRie+21xp/pxHW265JeCiW5bEawnS4MoezJgxA4CDDz442mZDbfPxLtNstNFG0bL9vqksKuGzcg3+HZINHT/88MOBeGmCfC5C+vvvvwMu6mKFU6Hk386/u7bop0Wt/Pknbf5GiwranZY/J5mtK6bigDb3p0UxyhrwYOVACoVFfR977LFoXVm/n5WzsGjXBRdcUGIfGxzjJ3mneuutt4B4tK+6pR+y4f333QQeFYlk2TFTTBEs48+Jm8pK5vTt2zdal3q+9EsLlVa4+tVXX42We/RYNmWwX36nkNhnrU6dOtE6P5kdXAkdKDknrJURyjeKZImIiIgkoEZHsp577jkgHsmyfCvL2WnRogUAn332WbTP4MGDARe5KIQ7TJ9f2M7upCxy9fbbb5f6OMtLs9wQgE033RRwuVl+4b3rrrsuQy3OvEWLFgHQuXNnwP2d00kXySrLRx99BLi8AP9us27duoDLv/ALBhYqG3JuETs/EmglL26//Xag8O6ya9WqBcCLL74YrbMps2ydX8LF/vZlFQe97777gPSFKceMGQO4c1Kh5dX4U5RZmYGyis1aDqu9lxbBKwYW7W7d2pWNvPDCCwF3zKSL9tu52D+u7HNkXn/9dcBN3QWF8dnyi1X7U7CBO16sRBLA5ZdfDrj8Nr9cyiqrrBJ7/Ny5czPb2AxRJEtEREQkAbrIEhEREUlAjS7hYEPt33333RLbrIq1DZl9+umno23phtjWJH53Yffu3QHXhWLlHsBVxLculJpmu+22A9LPU2glHPyE+0LSpk2baNlKEay//vol9rNuIBsc4pdLKSR+d43NyWdzzJXFr8xtXR12TrHEaBtAA9C+fXsA5s2bV80W516nTp2Ais10YRXN/e4gm2nBzsWFyh/4Ula6wRprrAHAtGnTAKhXr16JfSZNmgS4Lno/jSWf2bnhyy+/jNattdZa5T7uzz//BFzZIH/OWPts3XjjjUB8Ls8sdbOrhIOIiIhIrtSYxHcruwDQqlUrAEaNGlXq/pYY7Rc4k2VGjhwZLVtJgieffBJwxTbBlTfYfPPNs9i6/JOatAplDzDIZ5YIfumll0br7C7Vfk//bt0GQhRqBMv4Ef+KRLCMn9yeOm+llRHxCx0XQwTL2BB7e+9soJBfAPjoo4+OPcYva9GyZcu0+xSasqJXfikXK0GQLoJlcw/afJ+FEsEy9v3rR6/s+Lfiqeuuuy4ADRo0iPbp06cPULJYts96CvJ1kIgiWSIiIiIJKPqs2D+mAAAgAElEQVRIluU99O/fP1qXOqTYv3ssq9CglGR3WMOGDQPid+VWBsOGMKfLfcsVGya8//77A/Eid926dQPc3ZcfnUuXX1WaHXbYAUhfmDafy1uU5ZFHHgFcvp3Pfk/LF4H49Ew1yRlnnAHAeeedV2Kb5anZ8PRCKwFTFr+48ZFHHhnbdtpppwHxHDSLbtkQfd/OO+8MwGqrrQYUZy6slXKBeFHrVJZv5JeDKXTff/89UHaPkk1nZd8l6dh0b1dffXW0buLEiYD7rk+dJi+bFMkSERERSYAuskREREQSULTdhVaJ3Oam22qrrUrsM2DAACA+L5kNR2/YsGHSTSwqluTuD6O1EgZnn302EJ+LK9esG9na7Sen2zBjS7itTBchuKrGfvg61V9//VWp58y1tm3bAvHyHca6Ca+66irAdQHVRPb+WCkCf55Q6zq1eVDztUJ1dZx88snRsiVw2yAP6x7yWfezdQ36j7dzsHUlHnfccdG23377LZPNzhn7DkpnwoQJ0bLNdViobLBZWWwAjZ/aY92FZbHK7/adDy5VIx+S4RXJEhEREUlAUUWy/DmP7r33XsBFsPy5xDp06AC4BEz/7snmjSqmodTZ5N9x3XnnnQCst956uWpOqexOJx07Psqafy6dzTbbDHBlP9IV5yw0NsTcIpT+Z8x88cUXAAwcODB7Dcsjp556arRsc/HZMHR/nlCL7BazrbfeusS6OXPmAGUnrvfr1w+IR/4OOOAAwM3juMsuu0TbrIeiUFmSe7oeFuMnxf/yyy+JtylJn376aYl1VtTX/q4WBS5rrsuOHTtGyzY35LbbbgvEBw7YvIj5EPFUJEtEREQkAUUVyfL7by0vxgrBvfLKK9E2m7bAIhb+MGub3uGKK65ItK3FqqyhtvnkrbfeAlwuVrqCoRVxwgknRMu9evUCSpYB8fMCevbsWaXXyRWbAqZz586x9UuXLo2W0w2/rwkaNWoExM8VVmzx66+/Blw0pibbY489AJcHmS4XzY6n2bNnl/o8FikuZDZ1jp93lOr2228HKlf0Nt+ly0G1Ho7x48cD6aPk9v1t+Z5+CYt///0XcOUa7Ge+USRLREREJAG6yBIRERFJQFF1Fz7++OMl1j3xxBNAfDj9tddeC7iQrT+voSXM3XfffYm1M59YhXYrV/Dcc88Bbi7CirIuMr/adyFIV4399ddfB1w5D58l5lp3tM23BfHjCNwM8ldeeWW0rrLvay74v9Ppp5+edp877rgjWi7U6vVVZXNxvvzyy0B8Prbp06cDLkF31qxZ2W1cjvmJ/vYeWLeQJSYPHTo02mfx4sWAG4ZfVpmXYqj4bt1e6UoTTJkyBYC+fftmtU3ZYOU7Nthgg2idpfC0aNECcIPNbM5LcKk9fnpCoVEkS0RERCQBQbo7+aw3Iggy0gibRw9chMLW+b9nrVq1APj1118BV2oA3DB0/7mKmQ2bTY2w+HcO7733HgDvvPNOicdb+Ys6deoA7r0F9/62adMGcHcl+cCSLG+88UYgHrEp6zNhCfLp9rHkTitmasUT7Q61UDz66KPRcvfu3WPb7LjwC/+lKzJZbKyMAMDgwYMBl/hugwPA3Z0X03yEleHPXWgJzX70AuLRPUtetvOGvafpdOnSJVoupBIOFqkB955Y+QKL5IHrTRg5cmQWWyfVMDkMw53K20mRLBEREZEEFFVOlj8M3yINVpLB78+3YpE2VNaKKdZENuWFRbT69OkDQOvWraN9bMoL+1kWf3j2bbfdBuRXBMv8888/gMt/eP/996Nt3bp1A+J3zqkWLlwIxCOAdjylK7xXSMoqwmrFZmtC9MrnR7JSoy1+VKUYp8qpDP+zbgVG/WnLIH20qqwI8Q8//AAUXnTQvnvsPAgugmX8KLciWMVJkSwRERGRBBRVTtatt94aLafmZN18883RtkK7I8qFTTbZJFq2fKWuXbsC8akghgwZArjpNOz/EM/tkcLh/31t5KAV1bSfo0ePzn7Dsshy9saMGQNA+/bto20WxbPpQPzzST6cT/OFjbZt2bIlAC+++CJQMpoDLpJl7ze4/DYbyVpok6rffffdQDxnz1i02EYpQ3xkphQE5WSJiIiI5IouskREREQSUFTdhSIimWBFde+5554S26zba//9989qm6SwWHen3yVoJSss9cKOJSlI6i4UERERyZWiKuEgIpIJ66yzTuz/999/f7R81llnZbs5UoCstIcfyRo0aBCgCFZNokiWiIiISAKqlZMVBMFawFCgGRACxwNfAiOARsAs4PAwDBeW8zzKyRIREZFCkZWcrFuAl8Iw3BrYDvgcOB8YF4ZhE2Dc8v+LiIiI1ChVjmQFQbAG8DGwWeg9SRAEXwIdwjCcFwRBfeC1MAy3Ku15lj9GkSwREREpFIlHsjYDfgDuD4LgwyAIhgZBsCqwQRiG8wCW/6xXjdcQERERKUjVuciqDewA3BmG4fbAIirRNRgEwUlBELwfBMH75e8tIiIiUliqc5E1B5gThuGk5f9/kmUXXfOXdxOy/OeCdA8Ow3BIGIY7VSTcJiIiIlJoqnyRFYbh98C3QRBYvtVewGfAs0Cv5et6AaOq1UIRERGRAlTdYqRnAMOCIFgJ+Ao4jmUXbo8HQdAbmA0cVs3XEBERESk4mrtQREREpHI0d6GIiIhIrugiS0RERCQBusgSERERSYAuskREREQSoIssERERkQToIktEREQkAbrIEhEREUmALrJEREREElDdiu81UufOnQGoVatWqfu8++67AMyfPz8rbcq1448/HoAePXpE6/baay8ARo8eDcBzzz0XbRsyZAgA+VAMV0Qk05o0aQJAx44do3U77rgjAOuvvz4Af//9d7Stbdu2AHz11VcAPPnkk9G2jz/+GIDXXnstuQZLIhTJEhEREUmAptUpxZprrgnACSecAEDz5s2jbUceeSQAK664YqmPX7BgAQCnn356tM6/MylEa6+9NgC9e/eO1u29994AtGjRAoANNtig1Md/+umn0fIOO+wAwL///pvxdkpuPfXUU9Hy1ltvDUCHDh0A+OGHH3LRpEQ0atQIgOOOOy5ad/HFFwOwwgrL7l//+++/Uh+fbp9rrrkGgIsuuiijbZXssWP+/vvvB6B169aVenwQBEA8ym/L1113HQADBw4EdP7MMU2rIyIiIpIrimSluOCCCwAYNGgQACuttFK5j5kxY0a0bNGeddddF4ApU6ZE21q2bJmxdmaT5QpcddVVALRv377az/nWW2/FnqusO/4krbrqqoCLSgDsvvvuAGy77bYAHHjggdG2wYMHA7DZZpsBcPPNN0fb/OOgJrLoy2WXXRats/PLqaeeCrhcPN8hhxwCuM8ewNNPPw24Yy5XWrVqBcB6660XrbO/+SqrrAJA/fr1SzwuXTSiIvv8888/gDvOLrzwwiq3Pd9YlNvOJ+A+a/vvvz/g8ph8FYkKLlq0KFru168f4CJJ2Xb77bcDcMopp1Tp8RU5doYPHw7A0UcfXaXXKAZ27rbeppNPPjnaZtHEDz74AIDHHnss2maf39q1a8d+/vHHH5VtgiJZIiIiIrmiSBbx/IfzzjsPgNVXX73U/X/88UcAjjjiCMCNJAQYNWoUAHvuuSdQuJEsP+/K7hBsZIzPcs8efvhhAHbayV3Y213qn3/+Cbg7BoAlS5YA0LVrVwAmTJiQsbZXhLXTIiuWUwYVu5M0f/31V7R87LHHAvGcJIAtttgiWi7GaNcVV1wBuEiUvX/gcrDS5erZnah9fpo2bRpts+OqrBy/JA0dOhSALl26APFIVkXYe5Du7th+74ocZ/5nplBZtPqJJ54AYJ111om2VTXil+qVV16Jli2yMWfOnCq2uHpOO+00AG699VbAnesAxowZA8CkSZMAmDZtWonHt2vXDnDRcoCGDRsCLjf466+/BlxuK8Bvv/2WmV8gD9kxc+aZZ0br7HvJeo3K4p+T7Lsq9T21/0OFo1qKZImIiIjkii6yRERERBJQ+LHoaqhTpw4QT2y2ZFbr4vruu+8A1yUCLuT7/fffl3jO1PC+351UCCzxtH///tE6vxsHYOzYsdFynz59AJg1a1aJfc855xzAhc0vueSSaNsBBxwAuOHvlqBYheTDCrNEdoAXXngBiHddpPrll18Al4ycjh+GtqKD1l1o5S387sMRI0YArrSH35VQqKybMF13TlmJ6/a4rbbaCognNo8cOTKTTaww69o6+OCDAVfKpaKsTMsbb7wBwDfffBNts2Nl0003BVx3uiX+FxtLubAk/nSfNTs/Pv/88yW2WZdtRc4J77zzTpXbmWn33HMP4JLT/c/FwoULy318ulI/devWBeB///sf4FITDjvssGife++9t2oNzjNWqBVcYn/fvn0B99kB917aQIOff/452mbf0fY93rhx42jb3XffDbgUADsnL126NIO/haNIloiIiEgCanQky6Y0OPfcc6N1dtdRmURsPyl2tdVWi2278cYbq9PErNltt90Al5zq303Y+2TDyR988MFoW+qd2WeffRYt21Q7ZsMNNyzxur/++isAixcvrnLbK8qPsqXeVb/33nvRsv3NXn/9daDqBTRPPPFEwCU6gxtQMH78eMDd7RYK+10eeuihaJ0fzQO45ZZb0i6nsuPJPnN+1NfuRLPN/uY2uMXaZsmyviuvvDK2b2XZHfiXX34Zrdt8882r9Fz56Pfffwdg+vTpQHxwibHBQzb1VjGw6LQfWakuOwdbwrvZZpttMvYauWY9Sn5EzkoiGTtvgkuC/+STT8p9br8ouH1fz507F3C9MUl9BymSJSIiIpKAGh3JMnb3Wlk2vNzP17IhtRMnTgTgzTffrGbrkmV3zjadhx/BMpYv4RferAy72/ILDdpdrr0/SfWHV5RfssKPxiXF7toKLZJluUp+HqNFe+xnWXlYfv5R6uP8PCwrRpor++yzT+z/s2fPzvhrHHPMMUA8z6SYrLXWWoDLc01X1qKYIlhJsrIQl156aWy9n/NXqKwQ7eOPPw7E85qtp8PKNViJJKhYPqvlsNlnDVzky1436dIXimSJiIiIJEAXWSIiIiIJUHdhNdgQfetCAdcNZsNE582bl/2GVYLN8+VXDk7ld4dWhg0CsERy6z4AePnllwEYN25clZ67ulKTtbP1Wtl83UyycgOWrJ7ud7KQfLpEcEuYv/zyy0s8zljZg3yQRPdgKptdYsUVVyyxraqDLfKJdcd06tQJcN3CVhZH0rMZIg499NBonf+58VV10EU+sC55SxOweYL9Sv3NmjUDXLdhWWrVqhUt33bbbYBLardSDgDdu3cHXOJ70hTJEhEREUmAIllVsMceewBw9dVXA/FyAOeffz5Q9WT6bNhuu+2i5ZNOOglwd9NWgLNHjx7RPp9++mmVXseGytr75ScqWgQrk8OcKyPJOTutcGCDBg0Sf62k2UAIi0Za4dB0v5MVF/UL2VoSvEWG7PH+c6RLfC9mFqGwASHp3ksrD1HISouOT548OcstyV9+yQ4rXWBz3JY1f66xuVfBlXkolM/RWWedBbhE9/nz5wPu+wIqFsFaY401gPiAG0uUt8+Wfc8BfPHFF9VpdqUpkiUiIiKSgKKPZNld48orr1zqPn6Zha+++irtPl27do2WbRitRSr84eZWpDHdlDu5Vr9+fQD+7//+L1pnESz7vf0yC9Vlr2N97f77bFNtZJNfmsH66G14uQ31BejcuTNQ9SmR7M6qTZs2pe7z/vvvV+m5s8Evs2D5eBaBSpdTZussMuPvY1OEWMHNyuZyFRPLSbTh+CussOwe159OyKIZVqi0kJUWycr3PNVssGivn2uVWuqiIhFwewy4c5iVxbDIVj7xSyl06NAhts3yp2bMmFGh59pyyy0BV1rIpjXzWQT90UcfrXRbM0WRLBEREZEE6CJLREREJAFF0V1Yr149AJ566qlonQ39tKQ6fwinhe2tq+yGG26Ittmwais/YEPX/crcderUAeC5554DoGfPntE2/3XyjSVEtm7dOlr37bffAnDAAQdk5DVs+DHAkUcemfb1Id5Fki1Tp06Nli0kbfOp2dyN4JKOL7vsMsANBiiPJax26dKl1H3s+Bg7dmxFm5011k3oz01p3RGpXRfpujLKWldWwvznn39exRYXFjuX2LFmn4EFCxZE+1jpl2JggyVatWoFuJQNS3gG2HrrrQE344L/XhQjO/ded911QLy6ualqmRebgcS6ofPJLrvsArjfG1waiXnmmWdKPM6+h/fbbz8AunXrFm2z76x0qUDvvPMOUPa5OFvy768hIiIiUgQKOpJl0aoxY8YALuEY4MMPPwRcZOrtt9+OttnM7/fffz8Qj2JYUrtdJVt0wvfss88CcNBBB2Xgt0he48aNgfRRNluXqQKBducBJe/IbIhuPrj44osBl4DvD6W2Aq02P98dd9wRbUtNhrfoBLiEeT8ZNZXNk5WPSd6W5O63P/VvaJ81f7CHLVsJh6OPPjratu6666Z9Hn+dFTg95ZRTom3FUIwTYOjQodFyaXfVL774YrRcTOUNbM5Tew9OP/30EvvYZ8aOq8MPPzzaNm3atKSbmHUNGzYEXOHMdJFdK//jR7tt4MikSZMAWHvttQE33x+4RHD7HqzqwJ1MsnPJE088AaSfG9csXLgQiL8n9j753yvGzh+LFi0C4sfXY489BuRH8r8iWSIiIiIJCPKhUGIQBJVqxJprrgm4kgDWr+8XI/PLFKSyu4CZM2cC8eleUln0xfILAG666Sag7LyifffdF3DT7EA8mpZNF110EeAKpfqRChtCfO211wLVv/vxC8nZ1DlW/M2PGFakyFw2bLzxxoB7j8BFOss6Loyf/5B6PNidmR1v4EplWITU7sLywb///gvE7yTtbtHy6Q477LByn2fTTTeNll944QUgfeHN1KHqt956a7TNz9spRLfccgsQj+rZectMnDgRiEfE8+VzkUlWVNOmH/PLpaTm/FkuDbhzaD7nuVaWRXYtSmcRHoAHHngAiL8HpbHzjkUAwZ17Tz31VCA/8vuOOuooAB5++OFy961M6Qpwx4X1Oo0fP74qTayOyWEY7lTeTopkiYiIiCSgWhdZQRCcGQTB1CAIPg2CYHgQBHWDIGgcBMGkIAimB0EwIgiClcp/JhEREZHiUuXE9yAINgb6Ak3DMPwrCILHge5AZ2BwGIaPBUFwF9AbuDMjrV3Ohpo3bdoUcHMIltVFaN1CAGeccQbgSjGk89ZbbwEuGfeTTz6JtlkyXtu2bYF4uN+qOduw0jfeeCPallrhNkl+F5WFbC00b92k4LpBq9tNaMOHr7/++midvU82I3o+doXYTOwWYgeXcGrdm1aJGEoOO/a7CK0chs2bZQmofvmPzTbbDICddloWZc6nOS4/+OADwHW/g+vq9bvyyuMnuVuia1mJ78af89C6mu1nPvO7lW0QgH3Wy0opsG6zfPxcZJKlTNhsGH5Ctw0i2n777QF3TgU3J2wxdRf+9NNPgOs2rCobMOCnZ5h8OqeMGDECcOkCflkfq/xv5x37DvK7QG0GhE022QSIHwuXXHIJkJNuwkqpbndhbWDlIAhqA6sA84A9gSeXb38QKIwheCIiIiIZVOVIVhiGc4MguAGYDfwFjAEmA7+EYbh0+W5zgI1LeYoqO/bYYwGYNWsW4Iax+nfgG220EQDHHXdc7P+Q/uo/lUUarKTDlClTom1WUK5Tp05APFHPrsqtoGVpcyEmzS9ylxqxe/XVV6NlPzG/KixaZdEeuyMFmD59OuBKZRQKi0bYT/tbgks47d69e4nHWRFPK9PQqFEjIF6uwSI7VqAxn+46rU3+58jaXpmSE/4AB7tjt89IRYuYWnQ4nyNZ7du3B+LHtyX9WwTL/91skINF7Gxex5rGn7vQ3hOLato5A+CPP/7IbsOqwYptgvudPv7444w9/3rrrQe4XolBgwaV2MeKJn/99dcZe93qWrp02aWAlcyxn2XxC2P7ZZkg3jPkD0bLZ1WOZAVBsDZwINAY2AhYFeiUZte0QwWCIDgpCIL3gyDI35lyRURERKqoOsVI9wa+DsPwB4AgCEYCuwBrBUFQe3k0qwGQtsplGIZDgCHLH1ulOhIWKXjllVeA+HB6PyepNFZI0vq3wZU5sNwbu6NOV3jU8nn8fubPPvsMcH3vuVK3bt1oOTVa5d8h2vtk5QYqa7vttgPS58PZcH+7mylU6f6WlmdWFou0+oU1LZJlkdJ8ZHlYVbXrrrtGyxahsJ9+mRXLzbEipn7ZhnyeasdysKzQsV+yIpVfaNSmFLHSDRXRpEmTaNmiABbV8IfoW25TobHiq3bM+NNyWQHKik5rlUs2PRe47xWbKqiy+a72N/ejPpbj16BBg9i+/nN37doVyI8CnFVhU29ZIVFw32MWST/mmGOy37Bqqk5O1mygbRAEqwTLzqB7AZ8B44FDl+/TCxhVvSaKiIiIFJ4qX2SFYTiJZQnuHwCfLH+uIcAA4KwgCGYA6wL3ZqCdIiIiIgWlWnMXhmE4CEjNwPsKaF2d5y2PVVO28GK9evWAeLeOddX8888/gJuXDVw40rqxrHI8wNSpUwEXqk1X5uGGG24AYMiQIbHnySdWTgBcVeHmzZsDcOaZZ0bbrBvHuvus9MShhx5KaWyOLHCJmMbv5rH5tqQkmyvRn0E+H+Yay7TUpHZ/zkMbJDJhwoSstqm6evbsCUDHjh1L3cd+J7/ie2mlGqwUDcBJJ50U22blZqDke+l3RRaqHXbYIfZ/f77C6g7KyRWbMcLOt36CtlV1twEhe+65Z7Rtn332AWC//fYD4mWHSuN3sfvfY4Wkfv36gJuX0U91sc9Mt27dgPyc87U8qvguIiIikoCCnLvQWAFMG3ruFwz95ptvMtCy4tCsWTPADX+1ucR8Nm+dsdIMFWXJln5CuA0iyIdjLJesQC3A7bffDrgh/htuuGG0zU+QL0Q2P5kf3bS/vUVW/YT/QrwrBVe6waJyqXMSghuEYyVgwEXarZhyRQqWljU3Zr9+/aJlO64KwZ13utrUPXr0AGDVVVcF4pGsnXfeGSiMYq1+NNJKuViUzj//LV68GHB/13Q9JWXN4WcDlOw8a3PPQtnHUT6zwWLp5je1HiV/wEwe0dyFIiIiIrlSrZysXJs/fz4Azz//fI5bkt8sQmJ93nvvvXeJfSobuTJW2NQK7w0YMKBKz1PM/DuzdEUqi4UVMfV/N1u2Y7BQo1c+KyBrP/3iicb+zmUVX6zIseBHJxYsWAC4qUXyPVpvpS6scO/AgQOBePTWojYjR44E4jlohRDBMhaNAbjpppsAeOSRR4D49FF+DmZ5/N9/2LBhgJtC7rvv0lZGKiinn3464PJTrTfF/z6vzHRe+UqRLBEREZEEFHQkSyrGIn5WvNBGc4C7s7J97C45XTFIu5O2aYzATRuUmtMlTjGOGixLusmgK1OAs1DcfPPNQLwQZcOGDSv8eCuyaZ+rdPyRwBYNLOszmis2Ws4KrgK0a9cOiBcYhXjkzqJxVpjWn3KnUKUW2T333HOjbTbptZk5c2aJx9soweuvvz5a50fKCpkfybMC3zYFnB3XJ5xwQrRPIU2tVBpFskREREQSoIssERERkQQUdAkHkUJjpQysy/bCCy+MtvldLYXo4IMPBuDJJ5+M1llC8ymnnAIUR+J7KkvwhnjxUYgnvtu59sorrwRgypQpQOEVY03H/ub+HK+llSIYPXp0tNynTx+gOLoJpXRWYNQGSgG0adMGcKWFLNHf5mAsACrhICIiIpIrimSJZJEVjbQ7eL8AqT8gQaSQWHFMi1gCLFq0CIDLLrsMgBEjRgDxqJUGzNQMq622GgCffvpptM4Gw/Tu3RuAJUuWZL9h1aNIloiIiEiuKJIlIiIiUjmKZImIiIjkii6yRERERBKgiywRERGRBOgiS0RERCQBusgSERERSYAuskREREQSoIssERERkQToIktEREQkAbrIEhEREUmALrJEREREEqCLLBEREZEE6CJLREREJAG6yBIRERFJgC6yRERERBKgiywRERGRBOgiS0RERCQBusgSERERSYAuskREREQSUDvXDcg3u+yyCwBnnHEGALvuuisA//d//xftc//992e/YTl02223AXDqqacCsMIKK5TY1rdv3+w3TEREaqQDDzwQgGeeeQaAZs2aRdumTp2akzalo0iWiIiISAIUyQIGDBgQLV966aUArLTSSrF97rnnnmi5SZMmAFx44YVZaF3uhWEY+/nff/+V2CZSzE466aRoed111wVgn332AWD99dePtjVt2rTU57AI8CWXXALAkiVLABgyZEi0z08//ZSZBkuN06hRIwBeeeUVADbffPNom32f/fPPP1lvV1LOP/98IP59lI8UyRIRERFJgC6yRERERBIQ5EN3TxAEOWmEhfuff/75aN2KK65obQLSd4d98cUXAOy4444A/PXXX4m2M9duvfVWwCW+23sD8McffwCw5ZZbAjB//vwst04k86x78LrrrgNglVVWibb5Az8qo7Rzyp9//hktT5s2DYCDDjoIgDlz5lTptSR/rL766tFy+/btAdelt3jx4oy9zmeffQbAVlttBcALL7wQbbMk8XzvWitPy5Yto+XRo0cDsOGGGwLQokWLaFuWEt8nh2G4U3k7lXu2CILgviAIFgRB8Km3bp0gCMYGQTB9+c+1l68PgiC4NQiCGUEQTAmCYIfq/Q4iIiIihakiie8PALcDD3nrzgfGhWF4TRAE5y///wCgE9Bk+b82wJ3Lf+YVSwK0BFSLXoFLPO3atSsA66yzDgBPP/10tM/WW28NQOfOnQF46qmnkm1wHrPIVTElVGbKFltsAUC7du2idUOHDgWgX79+ANxxxx3Zb5iUy84Jq622WuKv5UfJ7E591KhRABx88MHRttmzZyfeliTZ73nxxRdH67p162zTEdIAABNOSURBVAa4wUS+X375BYDevXsD8NZbb0XbCiliblFJgAceeACATTfdFIC5c+dW67n9sgWbbbZZbJvfQ1PoESxz8803R8sWwcp35UaywjCcAPycsvpA4MHlyw8CB3nrHwqXeQdYKwiC+plqrIiIiEihqGoJhw3CMJwHEIbhvCAI6i1fvzHwrbffnOXr5lW9iZm31157AbDzzjsD8O6770bbrO869U7Jz42wobJnn302ULMjWS+++CIAP/+ceh1e3CzCYRFPcJFNy4nYYYdlveV+Dptp1apV0k2Uanj00UcBuOiiiwCoV69eqftOmjQpWp43r/RTXWk5WXvvvXe0bMfVdtttB8DRRx8dbbvqqqsq1PZ8Y9GWESNGAK4nAGDp0qUAvPrqqwC899570ba2bdsC8OSTTwLuXAPQpUuXBFucGXbM+CWCMmXVVVcF4kWyLfr69ttvA/D4449n/HVzxc6plvvrs5zGfO1NyXSdrJLfJpA2qT0IgpOAk9JtExERESl0Vb3Imh8EQf3lUaz6wILl6+cAm3j7NQC+S/cEYRgOAYZA9kcXduzYMbUt0XJpff1+bsSECRMAaNy4MRAvRvjDDz9krJ2SPZZ7B+4O2vijVtq0WZZi2KlTJ6Bk0dqKKivikU8sYnDfffcBsN566wEwbNiwaJ/JkycDrmDvokWLstnERCxcuBCA1157DXAFSAHuvPNOwOXTzJgxo8TjKqN58+bR8mWXXQbAAQccAMQLHv/++++Am8oq39WpUwdwx4VFsCZOnBjtY/lZb7zxRonHW1TvnXfeAWDNNddMrrEJsCjkNttsE6377rtlX4fVHZG+/fbbAy6nzXfjjTcCVTsW85UV+d1ggw1KbBs+fDjgRubmm6rWyXoW6LV8uRcwylt/zPJRhm2BX61bUURERKQmKTeSFQTBcKADsF4QBHOAQcA1wONBEPQGZgOHLd/9BaAzMAP4EzgugTaLiIiI5L1yL7LCMDyylE17pdk3BE6rbqOSULu2+1UtPG+F4K644opyH//xxx9Hyxa232ijjYB4CLNYugv97hHrGkvH7zbKd36338MPPwy4Lt9atWpF21ZeeeUKP6cVFQSX2Dtu3DjAJTrPnDmzxOP8LpN8c/XVV0fLNrjD3h/rWrduU4DDDz8ccHOlWXcFwKxZsxJta9J69OiR+Gt88skn0fLtt98OuO7CunXrRtvOOeccIL+7C/1yFNbF3Lp1a8B16/iJ4KklDHbZZZdouUOHDoA7T/tJ8YXKBklUdaCQdZlee+21JbZZYeh87TarCjuezj333BLb7Ht48ODBWW1TZWlaHREREZEEZHp0Yd6ysgsAe+yxB+BKN1h5/oqySEXPnj0z07g85N9BW7Qnnc8//zwbzcmIE088MVr2k41T/fbbb4BLQvanPfn1118BeOmllwB3NwVuOLpJN+zf7jb9CFi+uOCCCwA4+eSTo3UWwfr3338B+Oqrr4B4CQpLaLZpPPxBIhbFsIRfKZs/iKYQHXHEEdHyYYctyyL55ptvAOjTpw/gPgM+O9/4UTp/ChWA/v37Z7axObDWWmsBbsDMkiVLKvX4a665Big5OAfg7rvvBrI2pUxWWO+DHzk31huQ799BimSJiIiIJKDGRLJ8lldS1WJtVhzPIll+Po/kr002cdVFpk+fDrh8mCeeeCLaNmbMGKD6Q6CtGKnPjr18LJy32267AemHyttdoxXy9VmuzLHHHgvAyJEjo21WPFCRrJIs3+TSSy+N1h1zzDGl7u8fo/nGjh3LwwIXEbbcqnQRLNO3b1/AlSbwTZkyBUhf5qHQWC/K8ccfD7hzDbgocTpnnHEG4KYYSsemiZP8okiWiIiISAJ0kSUiIiKSgBrTXWjhbHBdFzfddFOVnuv777+P/f/QQw+Nlv1SD5JfunfvHi2vsMKy+ws/qT1TLKl10KBBJbbdf//9GX+9THnuuecA16UBrovKunPKYkPH/XIp9lxWOb2yrMK8lS/w5y6zOUMLqYwIQPv27QFXzT1dF6yxpHFwldPzUZMmTYD47BnWjTx79uxSH7f66qsDbq5G//GW0GxzzBYa+/395HY7N/zvf/8D4l2olrrw8ssvA7DPPvtE2yzx21JTvv122RTB559/frRPdavI56OGDRuWuq1QPveKZImIiIgkoMZEsvxokxWJrCq7Q7FExWJP6k19vywKVGisqGHSbBi+3X36d5g333xzVtpQFTYnn/2sKBuWvvvuuwPx42XgwIGAK5PiF7m14dk2QMCG/PvP4Uc2Ull5iHy8o02dHxVcBMuSu8uKYM2ZMweIFwLO5yKT6X4XKzeQyu9VsOMj3eMffPBBoHAjNDZYxOYwhJKDrWx+RnARu7Iid1Ym5vrrrwfgsccey0xj88gaa6wRLZ955pmxbX4R148++ihrbaqOwvy2FBEREclzRR/Jsj5/fyZ0y++oLrvLTjczeDFJjSb8999/OWpJYTjqqKNi/7fcISj8aWaMRa/ARbBsOp100aexY8cC8Tv3VOkeZ+sssvP1119H2x599NHKNjsRfsmKAw88sNT9LAKc+vkZMmRItGxlHXr16pXJJiYuXdkPO89utdVWABxyyCEAtGvXLtrHcpTKilgWuhdffDFatnODTZvk54lWhBUctZyuYuRHb/2ix+BKWQB88cUXWWtTdSiSJSIiIpIAXWSJiIiIJKDouwttCKjfpVfdhDmbB9GGl1d1RvV8lq5auZRto402AlzlZZvvzxJ4C5ElqtuccjvuuCMAq666arRP/fr1y30e67b3u4V++uknIN6daqwLzroH7TOWj5+1d955J1ru2rVrqftZN2Fq19ikSZOi5eHDh2e4ddlhf6/OnTtH66wEgV+KAOIJ/Ndeey0ARx55JAB77713ou3MBb9MjCWqjxgxAoDTTjst2mZzfg4dOrTEc/z9998AjBo1KrF25gt//kvz448/Aq76fyFRJEtEREQkAUUfybLh0n6yqQ2DrSobom/Jnq+++mq1ni8fdevWLddNKAiWuAuu+GjdunUBV9xz3Lhx2W9YhrRs2RKALl26AC6C5ZdpSI3M+KUyrIDm22+/DcQT1+fOnRv7Wajs7wxw1VVXAW5uyi+//DLa9uGHHwIuEdyKtt5+++3RPva+5nPR2nRszsJff/01Wte/f//YPlYaJN2AhQEDBgDx48qfB7HY2Gfml19+idb5g0lSnXDCCUBhn0uq47PPPov9LCSKZImIiIgkoOgjWTYNx7x586J1M2bMqNZzXnDBBYC7Y89Wkct8tPnmmwOFUxgu0/bdd99o+cQTTwRctKZHjx45aVMm2Z3zTjvtBMDFF18MpI9kWUTHz5sq9DtvG04+c+bMaN0PP/wAwMKFCwFXlBhg2223BVzk3I9U2OPWXnttwJV0uOKKK6J9Nt1008z+Alnm59ely7UrjR1DxVzKIR2/OG+fPn1i2ywPCVwkuJjZ52KTTTbJcUsyS5EsERERkQToIktEREQkAUXfXWhWXHHFaNkSkyvTzde0adNoec899wTccNrqdj8WsuOOOw6Afv365bgl2WVlPO66664S2yyxd9GiRdlsUqJs2P0xxxyT45Zkl1XYHj9+fLTu1FNPje1jw+shnuheGutmNKecckqJfWxQjT+wwrobi4l1l9lMAH45i99//z0nbcomGygBsMUWWwBu8ICVH4KakZKy8cYbA+nLB9k8kIVIkSwRERGRBBR9JMvuwP27Aiv21rt3byB+J2pWXnllwEWwnn766WibRcKuu+66BFqcH4YNGxYt+7PI13QWET333HMBV4AUYP78+UBxHxc1VYMGDaJlP+k/0yyCZeUOrOAxuIEUfkJ0oWvTpg3gohjvv/9+tG3JkiU5aVM2WNTKHzhjvv32W6BmRK8qarfddst1E6pMkSwRERGRBBR9JMsKAPrsjtCGW/vlB+rUqQPAdtttB8A222xT4vFWGC7dcxeLQiz6lg0tWrQAXB6NX+R28ODBQM3IJakpLKI0cODAaJ3lYt50000ATJgwIdpW3b+9lco4/PDDS2yzchIPP/xwtV4jnxxyyCG5bkJOWHkLv2SHlQkpa2omKTyKZImIiIgkQBdZIiIiIgko+u7Cb775BohXbLYq5dYlaD/T+euvvwC46KKLonV+Unix8ucgs+6Q3XffPVfNyRtnnXVW7P82sAKU8F6Mrr766hLrrFyJdRv65wZLL7Dq95bI7vvtt98AV57BT6q3eQ3Nd999Fy2/+eablf8FCoQNJkhyUEE+sLk/69WrV2KblQmx7ywpDopkiYiIiCQgyIe5ooIgSLwRO+64Y7Rs868deOCBAPz777/RtieeeAKAuXPnAnD++eeX2KemOeCAAwBXxsK/27QkzdGjR2e/YVmy9dZbR8s2xNxKfHTs2DHa9sorr2S3YZITFnmywQ9nn312tK127dI7B+xzU9Y5d+nSpYCLkPqRtOHDh1exxfmrc+fOgIv8/fHHH9E2K7vjz/9Y6Pr37w/AjTfeCMQHSrRr1w6AqVOnZr9heaBZs2YAfPzxxyW22aCPMWPGZLVN5ZgchuFO5e2kSJaIiIhIAmpMJEuksqwQ5Lhx46J1VsLBSlxYGRCpufwyBDYliEXAfamRrA8++ABww/nB5eM89thjyTQ2z1j0xvI+/Wl1OnToABR+UVKbOgjgrbfeAty0XNabAvDSSy9ltV1SbYpkiYiIiORK0Y8uFKmqxo0bA9C8efNonUUhLLdCZOTIkSWWLe9TytaqVavY/7///vtoudAjWOann36KllNHj0rxUyRLREREJAG6yBIRERFJgLoLRUrx888/A7Bo0aJo3bXXXgvA2LFjc9ImkWJiif6LFy8G4K677splc0QyTpEsERERkQSUW8IhCIL7gC7AgjAMmy1fdz1wALAEmAkcF4bhL8u3XQD0Bv4F+oZh+HK5jVAJBxERESkcGSvh8ADw/+3dX2hWdRzH8fcHLS8qqJAkdJTELpoRywsRhDCCml60ugjWRY0KVjChoIu0LuyymwqCFIyGBuYQSvLCSpHAq6UrJJ1ijRRdEyWEigRj69vF+Q0f5vO4+WzPzh8+r5tzzvecjS98+R2+nN/vnKdrWuwQ8EhEPAr8AmwBkNQB9ACr0t9sk7ToFpI2MzMzq4QZm6yIOAJcmRY7GBET6XAImPqF025gMCKuRcRZYBRYM4/5mpmZmZXCfKzJegX4Ju0vBy7UnBtLsRtI6pM0LGl4HnIwMzMzK5Q5vV0o6V1gAtg9FapzWd31VhGxA9iR/o/XZJmZmVmlNN1kSeolWxD/ZFxfPT8GtNVctgIYbz49MzMzs3JqarpQUhfwNvBMRFytObUf6JG0RNJKoB04Ovc0zczMzMplxidZkvYA64GlksaArWRvEy4BDqVflh+KiNcjYkTSXuAU2TRif0RMziKPP4B/0taqYSmuZ1W4ltXhWlaL65mfB2Zz0YzfyVookoZn880JKwfXszpcy+pwLavF9Sw+f/HdzMzMrAXcZJmZmZm1QJGarB15J2DzyvWsDteyOlzLanE9C64wa7LMzMzMqqRIT7LMzMzMKqMQTZakLklnJI1K2px3PnZrJJ2TdELS8amfSZJ0r6RDkn5N23vyztPqkzQg6bKkkzWxuvVT5uM0Vn+WtDq/zG26BrV8T9LvaXwel7Sx5tyWVMszkp7OJ2urR1KbpO8lnZY0IumNFPfYLJHcmyxJi4BPgA1AB/CCpI58s7ImPBERnTWvE28GDkdEO3A4HVsx7QS6psUa1W8D2UeG24E+YPsC5Wizs5MbawnwURqfnRFxACDdZ3uAVelvtqX7sRXDBPBWRDwMrAX6U808Nksk9yYLWAOMRsRvEfEvMAh055yTzV03sCvt7wKezTEXu4mIOAJcmRZuVL9u4PPIDAF3S7p/YTK1mTSoZSPdwGBEXIuIs8Ao2f3YCiAiLkbET2n/b+A0sByPzVIpQpO1HLhQczyWYlYeARyU9KOkvhRbFhEXIbtZAPfllp01o1H9PF7LaVOaQhqombp3LUtC0oPAY8APeGyWShGaLNWJ+ZXHclkXEavJHlf3S3o874SsZTxey2c78BDQCVwEPkhx17IEJN0JfAm8GRF/3ezSOjHXM2dFaLLGgLaa4xXAeE65WBMiYjxtLwP7yKYcLk09qk7by/llaE1oVD+P15KJiEsRMRkR/wGfcn1K0LUsOEm3kTVYuyPiqxT22CyRIjRZx4B2SSsl3U62EHN/zjnZLEm6Q9JdU/vAU8BJshr2pst6ga/zydCa1Kh++4GX0ptMa4E/p6YurJimrct5jmx8QlbLHklLJK0kWzB9dKHzs/okCfgMOB0RH9ac8tgskcV5JxARE5I2Ad8Bi4CBiBjJOS2bvWXAvux+wGLgi4j4VtIxYK+kV4HzwPM55mg3IWkPsB5YKmkM2Aq8T/36HQA2ki2Svgq8vOAJW0MNarleUifZ1NE54DWAiBiRtBc4RfYmW39ETOaRt9W1DngROCHpeIq9g8dmqfiL72ZmZmYtUITpQjMzM7PKcZNlZmZm1gJusszMzMxawE2WmZmZWQu4yTIzMzNrATdZZmZmZi3gJsvMzMysBdxkmZmZmbXA/2jnIQPla73NAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the first batch in our train DataLoader and \n",
    "# format it in grid.\n",
    "inputs = next(iter(train_loader))[0]\n",
    "input_grid = utils.make_grid(inputs)\n",
    "\n",
    "# Plot the images.\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "inp = input_grid.numpy().transpose((1, 2, 0))\n",
    "plt.imshow(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architectures\n",
    "\n",
    "We train a fully-connected neural network and a convolutional neural network with approximately the same number of parameters.\n",
    "\n",
    "### Fully-connected Network\n",
    "In short, the fully-connected network follows this architecture: ``Input -> [Linear -> ReLU]*3 -> Linear``. The following table shows it in details:\n",
    "\n",
    "| Layer Type                  | Output Size |    # of Parameters   |\n",
    "|-----------------------------|:-----------:|:--------------------:|\n",
    "| Input                       |   1x28x28   |           0          |\n",
    "| Flatten                     |  1\\*28\\*28  |           0          |\n",
    "| **Linear with 256 neurons** |     256     | 28\\*28*256 = 200,704 |\n",
    "| ReLU                        |      *      |           0          |\n",
    "| **Linear with 128 neurons** |     128     |   256*128 = 32,768   |\n",
    "| ReLU                        |      *      |           0          |\n",
    "| **Linear with 64 neurons**  |     64      |    128*64 = 8,192    |\n",
    "| ReLU                        |      *      |           0          |\n",
    "| **Linear with 10 neurons**  |      10     |     64*10 = 640      |\n",
    "\n",
    "Total # of parameters of the fully-connected network: 242,304\n",
    "\n",
    "### Convolutional Network\n",
    "\n",
    "The convolutional neural network architecture starts with some convolution and max-pooling layers. These are then followed by fully-connected layers. We calculate the total number of parameters that the network needs. In short, the convolutional network follows this architecture: ``Input -> [Conv -> ReLU -> MaxPool]*2 -> Dropout -> Linear -> ReLU -> Dropout -> Linear``. The following table shows it in details:\n",
    "\n",
    "| Layer Type                                     | Output Size |     # of Parameters     |\n",
    "|------------------------------------------------|:-----------:|:-----------------------:|\n",
    "| Input                                          |   1x28x28   |            0            |\n",
    "| **Conv with 16 3x3 filters with padding of 1** |   16x28x28  |      16\\*3\\*3 = 144     |\n",
    "| ReLU                                           |   16x28x28  |            0            |\n",
    "| MaxPool 2x2                                    |   16x14x14  |            0            |\n",
    "| **Conv with 32 3x3 filters with padding of 1** |   32x14x14  |      32\\*3\\*3 = 288     |\n",
    "| ReLU                                           |   32x14x14  |            0            |\n",
    "| MaxPool 2x2                                    |    32x7x7   |            0            |\n",
    "| Dropout of 0.25                                |    32x7x7   |            0            |\n",
    "| Flatten                                        |   32\\*7\\*7  |            0            |\n",
    "| **Linear with 128 neurons**                    |     128     | 32\\*7\\*7\\*128 = 200,704 |\n",
    "| ReLU                                           |     128     |            0            |\n",
    "| Dropout of 0.5                                 |     128     |            0            |\n",
    "| **Linear with 10 neurons**                     |      10     |      128\\*10 = 1280     |\n",
    "\n",
    "Total # of parameters of the convolutional network: 202,416"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_fully_connected_network():\n",
    "    \"\"\"\n",
    "    This function returns the fully-connected network layed out above.\n",
    "    \"\"\"\n",
    "    return nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(28*28, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, num_classes)\n",
    "    )\n",
    "\n",
    "def create_convolutional_network():\n",
    "    \"\"\"\n",
    "    This function returns the convolutional network layed out above.\n",
    "    \"\"\"\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.Dropout(0.25),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(32*7*7, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(128, num_classes)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the PyTorch way \n",
    "\n",
    "That is, doing your own training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pytorch_accuracy(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Computes the accuracy for a batch of predictions\n",
    "    \n",
    "    Args:\n",
    "        y_pred (torch.Tensor): the logit predictions of the neural network.\n",
    "        y_true (torch.Tensor): the ground truths.\n",
    "        \n",
    "    Returns:\n",
    "        The average accuracy of the batch.\n",
    "    \"\"\"\n",
    "    y_pred = y_pred.argmax(1)\n",
    "    return (y_pred == y_true).float().mean() * 100\n",
    "\n",
    "def pytorch_train_one_epoch(pytorch_module, optimizer, loss_function):\n",
    "    \"\"\"\n",
    "    Trains the neural network for one epoch on the train DataLoader.\n",
    "    \n",
    "    Args:\n",
    "        pytorch_module (torch.nn.Module): The neural network to train.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer of the neural network\n",
    "        loss_function: The loss function.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple (loss, accuracy) corresponding to an average of the losses and\n",
    "        an average of the accuracy, respectively, on the train DataLoader.\n",
    "    \"\"\"\n",
    "    pytorch_module.train(True)\n",
    "    with torch.enable_grad():\n",
    "        loss_sum = 0.\n",
    "        acc_sum = 0.\n",
    "        example_count = 0\n",
    "        for (x, y) in train_loader:\n",
    "            # Transfer batch on GPU if needed.\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = pytorch_module(x)\n",
    "\n",
    "            loss = loss_function(y_pred, y)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # Since the loss and accuracy are averages for the batch, we multiply \n",
    "            # them by the the number of examples so that we can do the right \n",
    "            # averages at the end of the epoch.\n",
    "            loss_sum += float(loss) * len(x)\n",
    "            acc_sum += float(pytorch_accuracy(y_pred, y)) * len(x)\n",
    "            example_count += len(x)\n",
    "\n",
    "    avg_loss = loss_sum / example_count\n",
    "    avg_acc = acc_sum / example_count\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "def pytorch_test(pytorch_module, loader, loss_function):\n",
    "    \"\"\"\n",
    "    Tests the neural network on a DataLoader.\n",
    "    \n",
    "    Args:\n",
    "        pytorch_module (torch.nn.Module): The neural network to test.\n",
    "        loader (torch.utils.data.DataLoader): The DataLoader to test on.\n",
    "        loss_function: The loss function.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple (loss, accuracy) corresponding to an average of the losses and\n",
    "        an average of the accuracy, respectively, on the DataLoader.\n",
    "    \"\"\"\n",
    "    pytorch_module.eval()\n",
    "    with torch.no_grad():\n",
    "        loss_sum = 0.\n",
    "        acc_sum = 0.\n",
    "        example_count = 0\n",
    "        for (x, y) in loader:\n",
    "            # Transfer batch on GPU if needed.\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            y_pred = pytorch_module(x)\n",
    "            loss = loss_function(y_pred, y)\n",
    "\n",
    "            # Since the loss and accuracy are averages for the batch, we multiply \n",
    "            # them by the the number of examples so that we can do the right \n",
    "            # averages at the end of the test.\n",
    "            loss_sum += float(loss) * len(x)\n",
    "            acc_sum += float(pytorch_accuracy(y_pred, y)) * len(x)\n",
    "            example_count += len(x)\n",
    "    \n",
    "    avg_loss = loss_sum / example_count\n",
    "    avg_acc = acc_sum / example_count\n",
    "    return avg_loss, avg_acc\n",
    "        \n",
    "    \n",
    "def pytorch_train(pytorch_module):   \n",
    "    \"\"\"\n",
    "    This function transfers the neural network to the right device, \n",
    "    trains it for a certain number of epochs, tests at each epoch on\n",
    "    the validation set and outputs the results on the test set at the\n",
    "    end of training.\n",
    "    \n",
    "    Args:\n",
    "        pytorch_module (torch.nn.Module): The neural network to train.\n",
    "        \n",
    "    Example:\n",
    "        This function displays something like this:\n",
    "        \n",
    "        .. code-block:: python\n",
    "\n",
    "            Epoch 1/5: loss: 0.5026924496193726, acc: 84.26666259765625, val_loss: 0.17258917854229608, val_acc: 94.75\n",
    "            Epoch 2/5: loss: 0.13690324830015502, acc: 95.73332977294922, val_loss: 0.14024296019474666, val_acc: 95.68333435058594\n",
    "            Epoch 3/5: loss: 0.08836929737279813, acc: 97.29582977294922, val_loss: 0.10380942322810491, val_acc: 96.66666412353516\n",
    "            Epoch 4/5: loss: 0.06714504160980383, acc: 97.91874694824219, val_loss: 0.09626663728555043, val_acc: 97.18333435058594\n",
    "            Epoch 5/5: loss: 0.05063822727650404, acc: 98.42708587646484, val_loss: 0.10017542181412378, val_acc: 96.95833587646484\n",
    "            Test:\n",
    "                Loss: 0.09501855444908142\n",
    "                Accuracy: 97.12999725341797\n",
    "    \"\"\"\n",
    "    print(pytorch_module)\n",
    "    \n",
    "    # Transfer weights on GPU if needed.\n",
    "    pytorch_module.to(device)\n",
    "    \n",
    "    optimizer = optim.SGD(pytorch_module.parameters(), lr=learning_rate)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # Training the neural network via backpropagation\n",
    "        train_loss, train_acc = pytorch_train_one_epoch(pytorch_module, optimizer, loss_function)\n",
    "        \n",
    "        # Validation at the end of the epoch\n",
    "        valid_loss, valid_acc = pytorch_test(pytorch_module, valid_loader, loss_function)\n",
    "\n",
    "        print(\"Epoch {}/{}: loss: {}, acc: {}, val_loss: {}, val_acc: {}\".format(\n",
    "            epoch, num_epochs, train_loss, train_acc, valid_loss, valid_acc\n",
    "        ))\n",
    "    \n",
    "    # Test at the end of the training\n",
    "    test_loss, test_acc = pytorch_test(pytorch_module, test_loader, loss_function)\n",
    "    print('Test:\\n\\tLoss: {}\\n\\tAccuracy: {}'.format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Flatten()\n",
      "  (1): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (6): ReLU()\n",
      "  (7): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n",
      "Epoch 1/5: loss: 0.5026924486209949, acc: 84.26666666666667, val_loss: 0.17258916554848353, val_acc: 94.75\n",
      "Epoch 2/5: loss: 0.13673794320225716, acc: 95.76041666666667, val_loss: 0.15051656740903854, val_acc: 95.5\n",
      "Epoch 3/5: loss: 0.08862295651187499, acc: 97.275, val_loss: 0.10005557791392009, val_acc: 96.70833333333333\n",
      "Epoch 4/5: loss: 0.06701619473099708, acc: 97.91458333333334, val_loss: 0.100683355341355, val_acc: 97.0\n",
      "Epoch 5/5: loss: 0.05082535463074843, acc: 98.45, val_loss: 0.09013855959971746, val_acc: 97.25\n",
      "Test:\n",
      "\tLoss: 0.08079261252880096\n",
      "\tAccuracy: 97.43\n"
     ]
    }
   ],
   "source": [
    "fc_net = create_fully_connected_network()\n",
    "pytorch_train(fc_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (4): ReLU()\n",
      "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (6): Dropout(p=0.25, inplace=False)\n",
      "  (7): Flatten()\n",
      "  (8): Linear(in_features=1568, out_features=128, bias=True)\n",
      "  (9): ReLU()\n",
      "  (10): Dropout(p=0.5, inplace=False)\n",
      "  (11): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n",
      "Epoch 1/5: loss: 0.39368337676674126, acc: 87.33333333333333, val_loss: 0.08271710021297136, val_acc: 97.43333333333334\n",
      "Epoch 2/5: loss: 0.13093601385007303, acc: 96.03333333333333, val_loss: 0.060145569880803426, val_acc: 98.275\n",
      "Epoch 3/5: loss: 0.10285072015970945, acc: 96.88333333333334, val_loss: 0.05715340468287468, val_acc: 98.275\n",
      "Epoch 4/5: loss: 0.08574063800772032, acc: 97.40416666666667, val_loss: 0.04314564943313599, val_acc: 98.5\n",
      "Epoch 5/5: loss: 0.07594804890702168, acc: 97.72291666666666, val_loss: 0.04446266069014867, val_acc: 98.65\n",
      "Test:\n",
      "\tLoss: 0.034414319026470186\n",
      "\tAccuracy: 98.92\n"
     ]
    }
   ],
   "source": [
    "conv_net = create_convolutional_network()\n",
    "pytorch_train(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Poutyne way\n",
    "\n",
    "That is, only 8 lines of code with a better output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poutyne_train(pytorch_module):\n",
    "    \"\"\"\n",
    "    This function creates a Poutyne Model (see https://poutyne.org/model.html), sends the Model\n",
    "    on the specified device, and uses the `fit_generator` method to train the neural network. \n",
    "    At the end, the `evaluate_generator` is used on  the test set.\n",
    "    \n",
    "    Args:\n",
    "        pytorch_module (torch.nn.Module): The neural network to train.\n",
    "    \"\"\"\n",
    "    print(pytorch_module)\n",
    "    \n",
    "    optimizer = optim.SGD(pytorch_module.parameters(), lr=learning_rate)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Poutyne Model\n",
    "    model = Model(pytorch_module, optimizer, loss_function, batch_metrics=['accuracy'])\n",
    "\n",
    "    # Send model on GPU\n",
    "    model.to(device)\n",
    "\n",
    "    # Train\n",
    "    model.fit_generator(train_loader, valid_loader, epochs=num_epochs)\n",
    "\n",
    "    # Test\n",
    "    test_loss, test_acc = model.evaluate_generator(test_loader)\n",
    "    print('Test:\\n\\tLoss: {}\\n\\tAccuracy: {}'.format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Flatten()\n",
      "  (1): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (6): ReLU()\n",
      "  (7): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n",
      "Epoch 1/5 10.57s Step 1500/1500: loss: 0.489895, acc: 84.668750, val_loss: 0.170369, val_acc: 94.750000\n",
      "Epoch 2/5 10.73s Step 1500/1500: loss: 0.136227, acc: 95.943750, val_loss: 0.115536, val_acc: 96.333333\n",
      "Epoch 3/5 9.74s Step 1500/1500: loss: 0.089760, acc: 97.195833, val_loss: 0.094134, val_acc: 97.075000\n",
      "Epoch 4/5 10.92s Step 1500/1500: loss: 0.065264, acc: 97.983333, val_loss: 0.103249, val_acc: 96.708333\n",
      "Epoch 5/5 10.50s Step 1500/1500: loss: 0.050911, acc: 98.408333, val_loss: 0.105603, val_acc: 96.633333\n",
      "Test:\n",
      "\tLoss: 0.09738772313594818\n",
      "\tAccuracy: 97.05\n"
     ]
    }
   ],
   "source": [
    "fc_net = create_fully_connected_network()\n",
    "poutyne_train(fc_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (4): ReLU()\n",
      "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (6): Dropout(p=0.25, inplace=False)\n",
      "  (7): Flatten()\n",
      "  (8): Linear(in_features=1568, out_features=128, bias=True)\n",
      "  (9): ReLU()\n",
      "  (10): Dropout(p=0.5, inplace=False)\n",
      "  (11): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n",
      "Epoch 1/5 12.08s Step 1500/1500: loss: 0.396106, acc: 87.035417, val_loss: 0.077737, val_acc: 97.375000\n",
      "Epoch 2/5 12.11s Step 1500/1500: loss: 0.130993, acc: 96.047917, val_loss: 0.055515, val_acc: 98.183333\n",
      "Epoch 3/5 11.47s Step 1500/1500: loss: 0.101883, acc: 96.870833, val_loss: 0.066538, val_acc: 97.908333\n",
      "Epoch 4/5 11.56s Step 1500/1500: loss: 0.084212, acc: 97.422917, val_loss: 0.044709, val_acc: 98.566667\n",
      "Epoch 5/5 11.69s Step 1500/1500: loss: 0.075197, acc: 97.722917, val_loss: 0.041355, val_acc: 98.708333\n",
      "Test:\n",
      "\tLoss: 0.029043858456611633\n",
      "\tAccuracy: 98.93\n"
     ]
    }
   ],
   "source": [
    "conv_net = create_convolutional_network()\n",
    "poutyne_train(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poutyne Callbacks\n",
    "\n",
    "One nice feature of Poutyne is [callbacks](https://poutyne.org/callbacks.html). Callbacks allow to do actions during training of the neural network. In the following example, we use 3 callbacks. One that saves the latest weights in a file to be able to continue the optimization at the end of training if more epochs are needed. Another one that saves the best weights according to the performance on the validation dataset. Finally, another one that saves the displayed logs into TSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_callbacks(name, pytorch_module):\n",
    "    \"\"\"\n",
    "    In addition to the the `poutyne_train`, this function saves checkpoints and logs as described above.\n",
    "\n",
    "    Args:\n",
    "        name (str): a name used to save logs and checkpoints.\n",
    "        pytorch_module (torch.nn.Module): The neural network to train.\n",
    "    \"\"\"\n",
    "    print(pytorch_module)\n",
    "    \n",
    "    callbacks = [\n",
    "        # Save the latest weights to be able to continue the optimization at the end for more epochs.\n",
    "        ModelCheckpoint(name + '_last_epoch.ckpt', temporary_filename='last_epoch.ckpt.tmp'),\n",
    "\n",
    "        # Save the weights in a new file when the current model is better than all previous models.\n",
    "        ModelCheckpoint(name + '_best_epoch_{epoch}.ckpt', monitor='val_acc', mode='max', save_best_only=True, restore_best=True, verbose=True, temporary_filename='best_epoch.ckpt.tmp'),\n",
    "\n",
    "        # Save the losses and accuracies for each epoch in a TSV.\n",
    "        CSVLogger(name + '_log.tsv', separator='\\t'),\n",
    "    ]\n",
    "    \n",
    "    train_loader, valid_loader, test_loader = loaders\n",
    "    optimizer = optim.SGD(pytorch_module.parameters(), lr=learning_rate)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    model = Model(pytorch_module, optimizer, loss_function, batch_metrics=['accuracy'])\n",
    "    model.to(device)\n",
    "    model.fit_generator(train_loader, valid_loader, epochs=num_epochs, callbacks=callbacks)\n",
    "    test_loss, test_acc = model.evaluate_generator(test_loader)\n",
    "    print('Test:\\n\\tLoss: {}\\n\\tAccuracy: {}'.format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Flatten()\n",
      "  (1): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (6): ReLU()\n",
      "  (7): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n",
      "Epoch 1/5 10.39s Step 1500/1500: loss: 0.508231, acc: 84.181250, val_loss: 0.191087, val_acc: 94.233333\n",
      "Epoch 1: val_acc improved from -inf to 94.23333, saving file to fc_best_epoch_1.ckpt\n",
      "Epoch 2/5 10.41s Step 1500/1500: loss: 0.135742, acc: 95.858333, val_loss: 0.133668, val_acc: 95.975000\n",
      "Epoch 2: val_acc improved from 94.23333 to 95.97500, saving file to fc_best_epoch_2.ckpt\n",
      "Epoch 3/5 10.28s Step 1500/1500: loss: 0.091761, acc: 97.156250, val_loss: 0.110453, val_acc: 96.575000\n",
      "Epoch 3: val_acc improved from 95.97500 to 96.57500, saving file to fc_best_epoch_3.ckpt\n",
      "Epoch 4/5 10.88s Step 1500/1500: loss: 0.068618, acc: 97.831250, val_loss: 0.086133, val_acc: 97.333333\n",
      "Epoch 4: val_acc improved from 96.57500 to 97.33333, saving file to fc_best_epoch_4.ckpt\n",
      "Epoch 5/5 10.75s Step 1500/1500: loss: 0.052458, acc: 98.379167, val_loss: 0.095458, val_acc: 97.108333\n",
      "Restoring model from fc_best_epoch_4.ckpt\n",
      "Test:\n",
      "\tLoss: 0.07873534116744996\n",
      "\tAccuracy: 97.54\n"
     ]
    }
   ],
   "source": [
    "fc_net = create_fully_connected_network()\n",
    "train_with_callbacks('fc', fc_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (4): ReLU()\n",
      "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (6): Dropout(p=0.25, inplace=False)\n",
      "  (7): Flatten()\n",
      "  (8): Linear(in_features=1568, out_features=128, bias=True)\n",
      "  (9): ReLU()\n",
      "  (10): Dropout(p=0.5, inplace=False)\n",
      "  (11): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n",
      "Epoch 1/5 12.30s Step 1500/1500: loss: 0.360715, acc: 88.322917, val_loss: 0.081634, val_acc: 97.525000\n",
      "Epoch 1: val_acc improved from -inf to 97.52500, saving file to conv_best_epoch_1.ckpt\n",
      "Epoch 2/5 11.99s Step 1500/1500: loss: 0.133433, acc: 95.979167, val_loss: 0.056934, val_acc: 98.166667\n",
      "Epoch 2: val_acc improved from 97.52500 to 98.16667, saving file to conv_best_epoch_2.ckpt\n",
      "Epoch 3/5 12.19s Step 1500/1500: loss: 0.098064, acc: 97.066667, val_loss: 0.044310, val_acc: 98.600000\n",
      "Epoch 3: val_acc improved from 98.16667 to 98.60000, saving file to conv_best_epoch_3.ckpt\n",
      "Epoch 4/5 12.60s Step 1500/1500: loss: 0.084538, acc: 97.429167, val_loss: 0.041837, val_acc: 98.608333\n",
      "Epoch 4: val_acc improved from 98.60000 to 98.60833, saving file to conv_best_epoch_4.ckpt\n",
      "Epoch 5/5 12.12s Step 1500/1500: loss: 0.073270, acc: 97.768750, val_loss: 0.038199, val_acc: 98.825000\n",
      "Epoch 5: val_acc improved from 98.60833 to 98.82500, saving file to conv_best_epoch_5.ckpt\n",
      "Restoring model from conv_best_epoch_5.ckpt\n",
      "Test:\n",
      "\tLoss: 0.029612627041339875\n",
      "\tAccuracy: 99.05\n"
     ]
    }
   ],
   "source": [
    "conv_net = create_convolutional_network()\n",
    "train_with_callbacks('conv', conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Your Own Callback\n",
    "\n",
    "While Poutyne provides a great number of [predefined callbacks](https://poutyne.org/callbacks.html), it is sometimes useful to make your own callback. \n",
    "\n",
    "In the following example, we want to see the effect of temperature on the optimization of our neural network. To do so, we either increase or decrease the temperature during the optimization. As one can see in the result, temperature either as no effect or has detrimental effect on the performance of the neural network. This is so because the temperature has for effect to artificially changing the learning rates. Since we have found the right learning rate, increasing or decreasing it shows no improvement on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLossWithTemperature(nn.Module):\n",
    "    \"\"\"\n",
    "    This loss module is the cross-entropy loss function\n",
    "    with temperature. It divides the logits by a temperature\n",
    "    value before computing the cross-entropy loss.\n",
    "    \n",
    "    Args:\n",
    "        initial_temperature (float): The initial value of the temperature.\n",
    "    \"\"\"\n",
    "    def __init__(self, initial_temperature):\n",
    "        super().__init__()\n",
    "        self.temperature = initial_temperature\n",
    "        self.celoss = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_pred = y_pred / self.temperature\n",
    "        return self.celoss(y_pred, y_true)\n",
    "\n",
    "class TemperatureCallback(Callback):\n",
    "    \"\"\"\n",
    "    This callback multiply the loss temperature with a decay before\n",
    "    each batch.\n",
    "    \n",
    "    Args:\n",
    "        celoss_with_temp (CrossEntropyLossWithTemperature): the loss module.\n",
    "        decay (float): The value of the temperature decay.\n",
    "    \"\"\"\n",
    "    def __init__(self, celoss_with_temp, decay):\n",
    "        super().__init__()\n",
    "        self.celoss_with_temp = celoss_with_temp\n",
    "        self.decay = decay\n",
    "    \n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        self.celoss_with_temp.temperature *= self.decay\n",
    "\n",
    "def train_with_temperature(pytorch_module, initial_temperature, temperature_decay):\n",
    "    \"\"\"\n",
    "    In addition to the the `poutyne_train`, this function uses a cross-entropy loss\n",
    "    with temperature and decays the temperature at each batch.\n",
    "\n",
    "    Args:\n",
    "        pytorch_module (torch.nn.Module): The neural network to train.\n",
    "        initial_temperature (float): The initial value of the temperature.\n",
    "        decay (float): The value of the temperature decay.\n",
    "    \"\"\"\n",
    "    print(pytorch_module)\n",
    "    train_loader, valid_loader, test_loader = loaders\n",
    "    optimizer = optim.SGD(pytorch_module.parameters(), lr=learning_rate)\n",
    "    \n",
    "    loss_function = CrossEntropyLossWithTemperature(initial_temperature)\n",
    "    callbacks = [TemperatureCallback(loss_function, temperature_decay)]\n",
    "    model = Model(pytorch_module, optimizer, loss_function, batch_metrics=['accuracy'])\n",
    "    model.to(device)\n",
    "    model.fit_generator(train_loader, valid_loader, epochs=num_epochs, callbacks=callbacks)\n",
    "    test_loss, test_acc = model.evaluate_generator(test_loader)\n",
    "    print('Test:\\n\\tLoss: {}\\n\\tAccuracy: {}'.format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (4): ReLU()\n",
      "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (6): Dropout(p=0.25, inplace=False)\n",
      "  (7): Flatten()\n",
      "  (8): Linear(in_features=1568, out_features=128, bias=True)\n",
      "  (9): ReLU()\n",
      "  (10): Dropout(p=0.5, inplace=False)\n",
      "  (11): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n",
      "Epoch 1/5 12.23s Step 1500/1500: loss: 0.431915, acc: 86.218750, val_loss: 0.077116, val_acc: 97.566667\n",
      "Epoch 2/5 11.79s Step 1500/1500: loss: 0.131324, acc: 96.154167, val_loss: 0.057988, val_acc: 98.150000\n",
      "Epoch 3/5 11.89s Step 1500/1500: loss: 0.106748, acc: 96.862500, val_loss: 0.052646, val_acc: 98.391667\n",
      "Epoch 4/5 12.25s Step 1500/1500: loss: 0.107931, acc: 96.958333, val_loss: 0.056798, val_acc: 98.166667\n",
      "Epoch 5/5 11.84s Step 1500/1500: loss: 0.123481, acc: 96.583333, val_loss: 0.071415, val_acc: 98.083333\n",
      "Test:\n",
      "\tLoss: 0.06291989715099335\n",
      "\tAccuracy: 98.24\n"
     ]
    }
   ],
   "source": [
    "conv_net = create_convolutional_network()\n",
    "# Initial temperature = 0.1\n",
    "# Final temperature  0.1 * 1.0008^7500  40.25\n",
    "train_with_temperature(conv_net, \n",
    "                       initial_temperature=0.1, \n",
    "                       temperature_decay=1.0008)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (4): ReLU()\n",
      "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (6): Dropout(p=0.25, inplace=False)\n",
      "  (7): Flatten()\n",
      "  (8): Linear(in_features=1568, out_features=128, bias=True)\n",
      "  (9): ReLU()\n",
      "  (10): Dropout(p=0.5, inplace=False)\n",
      "  (11): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n",
      "Epoch 1/5 11.77s Step 1500/1500: loss: 0.588329, acc: 80.539583, val_loss: 0.116366, val_acc: 96.383333\n",
      "Epoch 2/5 11.87s Step 1500/1500: loss: 0.159414, acc: 95.258333, val_loss: 0.073682, val_acc: 97.616667\n",
      "Epoch 3/5 12.17s Step 1500/1500: loss: 0.127515, acc: 96.158333, val_loss: 0.054854, val_acc: 98.275000\n",
      "Epoch 4/5 12.12s Step 1500/1500: loss: 0.112578, acc: 96.606250, val_loss: 0.055366, val_acc: 98.283333\n",
      "Epoch 5/5 12.03s Step 1500/1500: loss: 0.134833, acc: 96.231250, val_loss: 0.084162, val_acc: 97.625000\n",
      "Test:\n",
      "\tLoss: 0.07264839706420899\n",
      "\tAccuracy: 97.99\n"
     ]
    }
   ],
   "source": [
    "conv_net = create_convolutional_network()\n",
    "# Initial temperature = 40.25\n",
    "# Final temperature  40.25 * 0.9992^7500  0.1\n",
    "train_with_temperature(conv_net,\n",
    "                       initial_temperature=4.25, \n",
    "                       temperature_decay=0.9995)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poutyne Experiment\n",
    "Most of the time when using Poutyne (or even Pytorch in general), we will find ourselves in an iterative model hyperparameters finetuning loop. For efficient model search, we will usually wish to save our best performing models, their training and testing statistics and even sometimes wish to retrain an already trained model for further tuning. All of the above can be easily implemented with the flexibility of Poutyne Callbacks, but having to define and initialize each and every Callback object we wish for our model quickly feels cumbersome.\n",
    "\n",
    "This is why Poutyne provides an [Experiment class](https://poutyne.org/experiment.html), which aims specifically at enabling quick model iteration search, while not sacrifying on the quality of a single experiment - statistics logging, best models saving, etc. Experiment is actually a simple wrapper between a Pytorch module and Poutyne's core Callback objects for logging and saving. Given a working directory where to output the various logging files and a Pytorch module, the Experiment class reduces the whole training loop to a single line.\n",
    "\n",
    "The following code should use [Poutyne's Experiment class](https://poutyne.org/experiment.html) to train a network for 5 epochs. The code should be quite simpler than the code in the Poutyne Callbacks section while doing more (only 3 lines). Once trained for 5 epochs, it is then possible to resume the optimization at the 5th epoch for 5 more epochs until the 10th epoch using the same function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_train(pytorch_module, working_directory, epochs=5):\n",
    "    \"\"\"\n",
    "    This function creates a Poutyne Experiment, trains the input module\n",
    "    on the train loader and then tests its performance on the test loader.\n",
    "    All training and testing statistics are saved, as well as best model\n",
    "    checkpoints.\n",
    "    \n",
    "    Args:\n",
    "        pytorch_module (torch.nn.Module): The neural network to train.\n",
    "        working_directory (str): The directory where to output files to save.\n",
    "        epochs (int): The number of epochs. (Default: 5)\n",
    "    \"\"\"\n",
    "    print(pytorch_module)\n",
    "\n",
    "    # Poutyne Experiment\n",
    "    exp = Experiment(working_directory, pytorch_module, device=device, optimizer='sgd', task='classif')\n",
    "\n",
    "    # Train\n",
    "    exp.train(train_loader, valid_loader, epochs=epochs)\n",
    "\n",
    "    # Test\n",
    "    exp.test(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (4): ReLU()\n",
      "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (6): Dropout(p=0.25, inplace=False)\n",
      "  (7): Flatten()\n",
      "  (8): Linear(in_features=1568, out_features=128, bias=True)\n",
      "  (9): ReLU()\n",
      "  (10): Dropout(p=0.5, inplace=False)\n",
      "  (11): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n",
      "Epoch 1/5 15.39s Step 1500/1500: loss: 1.025398, acc: 66.664583, fscore_micro: 0.666646, val_loss: 0.317650, val_acc: 90.458333, val_fscore_micro: 0.904583\n",
      "Epoch 1: val_acc improved from -inf to 90.45833, saving file to ./conv_net_experiment/checkpoint_epoch_1.ckpt\n",
      "Epoch 2/5 14.07s Step 1500/1500: loss: 0.374973, acc: 88.418750, fscore_micro: 0.884188, val_loss: 0.178774, val_acc: 94.558333, val_fscore_micro: 0.945583\n",
      "Epoch 2: val_acc improved from 90.45833 to 94.55833, saving file to ./conv_net_experiment/checkpoint_epoch_2.ckpt\n",
      "Epoch 3/5 14.91s Step 1500/1500: loss: 0.251193, acc: 92.445833, fscore_micro: 0.924458, val_loss: 0.127881, val_acc: 96.125000, val_fscore_micro: 0.961250\n",
      "Epoch 3: val_acc improved from 94.55833 to 96.12500, saving file to ./conv_net_experiment/checkpoint_epoch_3.ckpt\n",
      "Epoch 4/5 14.87s Step 1500/1500: loss: 0.197997, acc: 94.052083, fscore_micro: 0.940521, val_loss: 0.101625, val_acc: 96.775000, val_fscore_micro: 0.967750\n",
      "Epoch 4: val_acc improved from 96.12500 to 96.77500, saving file to ./conv_net_experiment/checkpoint_epoch_4.ckpt\n",
      "Epoch 5/5 14.70s Step 1500/1500: loss: 0.170571, acc: 94.862500, fscore_micro: 0.948625, val_loss: 0.090143, val_acc: 97.075000, val_fscore_micro: 0.970750\n",
      "Epoch 5: val_acc improved from 96.77500 to 97.07500, saving file to ./conv_net_experiment/checkpoint_epoch_5.ckpt\n",
      "Restoring model from ./conv_net_experiment/checkpoint_epoch_5.ckpt\n",
      "On best model: test_loss: 0.0765609, test_acc: 97.48, test_fscore_micro: 0.9748\n"
     ]
    }
   ],
   "source": [
    "conv_net = create_convolutional_network()\n",
    "experiment_train(conv_net, './conv_net_experiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (4): ReLU()\n",
      "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (6): Dropout(p=0.25, inplace=False)\n",
      "  (7): Flatten()\n",
      "  (8): Linear(in_features=1568, out_features=128, bias=True)\n",
      "  (9): ReLU()\n",
      "  (10): Dropout(p=0.5, inplace=False)\n",
      "  (11): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n",
      "Loading weights from ./conv_net_experiment/checkpoint.ckpt and starting at epoch 6.\n",
      "Loading optimizer state from ./conv_net_experiment/checkpoint.optim and starting at epoch 6.\n",
      "Epoch 6/10 15.05s Step 1500/1500: loss: 0.141787, acc: 95.747917, fscore_micro: 0.957479, val_loss: 0.080299, val_acc: 97.408333, val_fscore_micro: 0.974083\n",
      "Epoch 6: val_acc improved from 97.07500 to 97.40833, saving file to ./conv_net_experiment/checkpoint_epoch_6.ckpt\n",
      "Epoch 7/10 15.16s Step 1500/1500: loss: 0.125111, acc: 96.281250, fscore_micro: 0.962812, val_loss: 0.071671, val_acc: 97.516667, val_fscore_micro: 0.975167\n",
      "Epoch 7: val_acc improved from 97.40833 to 97.51667, saving file to ./conv_net_experiment/checkpoint_epoch_7.ckpt\n",
      "Epoch 8/10 15.54s Step 1500/1500: loss: 0.114591, acc: 96.568750, fscore_micro: 0.965688, val_loss: 0.068869, val_acc: 97.816667, val_fscore_micro: 0.978167\n",
      "Epoch 8: val_acc improved from 97.51667 to 97.81667, saving file to ./conv_net_experiment/checkpoint_epoch_8.ckpt\n",
      "Epoch 9/10 ETA 1s Step 1326/1500: loss: 0.023140, acc: 100.000000"
     ]
    }
   ],
   "source": [
    "conv_net = create_convolutional_network()\n",
    "experiment_train(conv_net, './conv_net_experiment', epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
