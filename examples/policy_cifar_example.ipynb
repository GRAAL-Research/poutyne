{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train CIFAR with the `policy` module\n",
    "\n",
    "Let's install the latest version of Poutyne (if it's not already) and import all the needed packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --upgrade poutyne\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "from poutyne import Model, OptimizerPolicy, one_cycle_phases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training constant\n",
    "But first, let's set the training constants, the CUDA device used for training if one is present, we set the batch size (i.e. the number of elements to see before updating the model) and the number of epochs (i.e. the number of times we see the full dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_device = 0\n",
    "device = torch.device(\"cuda:%d\" % cuda_device if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batch_size = 1024\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_mean = [0.485, 0.456, 0.406]\n",
    "_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(.3, .3, .3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(_mean, _std),\n",
    "])\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(_mean, _std),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "root = \"datasets\"\n",
    "train_ds = datasets.CIFAR10(root, train=True, transform=train_transform, download=True)\n",
    "val_ds = datasets.CIFAR10(root, train=False, transform=val_transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=8\n",
    ")\n",
    "val_dl = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model\n",
    "We'll train a simple `resNet-18` network.\n",
    "This takes a while without GPU but is pretty quick with GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_module():\n",
    "    model = resnet18(pretrained=False)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    model.fc = nn.Linear(512, 10)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training without the `policies` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35mEpoch: \u001b[36m1/5 \u001b[35mStep: \u001b[36m49/49 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m10.97s \u001b[35mloss:\u001b[94m 2.114963\u001b[35m acc:\u001b[94m 23.278000\u001b[35m val_loss:\u001b[94m 1.864067\u001b[35m val_acc:\u001b[94m 32.720000\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m2/5 \u001b[35mStep: \u001b[36m49/49 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m10.26s \u001b[35mloss:\u001b[94m 1.798623\u001b[35m acc:\u001b[94m 34.332000\u001b[35m val_loss:\u001b[94m 1.666151\u001b[35m val_acc:\u001b[94m 39.590000\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m3/5 \u001b[35mStep: \u001b[36m49/49 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m10.25s \u001b[35mloss:\u001b[94m 1.646562\u001b[35m acc:\u001b[94m 39.752000\u001b[35m val_loss:\u001b[94m 1.557361\u001b[35m val_acc:\u001b[94m 42.800000\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m4/5 \u001b[35mStep: \u001b[36m49/49 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m10.30s \u001b[35mloss:\u001b[94m 1.556004\u001b[35m acc:\u001b[94m 43.340000\u001b[35m val_loss:\u001b[94m 1.495678\u001b[35m val_acc:\u001b[94m 45.790000\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m5/5 \u001b[35mStep: \u001b[36m49/49 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m10.26s \u001b[35mloss:\u001b[94m 1.492378\u001b[35m acc:\u001b[94m 45.702000\u001b[35m val_loss:\u001b[94m 1.441630\u001b[35m val_acc:\u001b[94m 47.500000\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "pytorch_network = get_module()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(pytorch_network.parameters(), lr=0.01)\n",
    "\n",
    "model = Model(\n",
    "    pytorch_network,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    batch_metrics=[\"acc\"],\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_dl,\n",
    "    val_dl,\n",
    "    epochs=epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with the `policies` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch = len(train_dl)\n",
    "steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35mEpoch: \u001b[36m1/5 \u001b[35mStep: \u001b[36m49/49 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m10.32s \u001b[35mloss:\u001b[94m 1.843946\u001b[35m acc:\u001b[94m 33.514000\u001b[35m val_loss:\u001b[94m 1.542911\u001b[35m val_acc:\u001b[94m 45.300000\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m2/5 \u001b[35mStep: \u001b[36m49/49 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m10.13s \u001b[35mloss:\u001b[94m 1.396645\u001b[35m acc:\u001b[94m 49.988000\u001b[35m val_loss:\u001b[94m 1.287964\u001b[35m val_acc:\u001b[94m 55.170000\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m3/5 \u001b[35mStep: \u001b[36m49/49 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m10.44s \u001b[35mloss:\u001b[94m 1.163528\u001b[35m acc:\u001b[94m 58.790000\u001b[35m val_loss:\u001b[94m 1.087473\u001b[35m val_acc:\u001b[94m 61.030000\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m4/5 \u001b[35mStep: \u001b[36m49/49 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m10.24s \u001b[35mloss:\u001b[94m 0.973432\u001b[35m acc:\u001b[94m 65.746000\u001b[35m val_loss:\u001b[94m 0.998768\u001b[35m val_acc:\u001b[94m 65.050000\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m5/5 \u001b[35mStep: \u001b[36m49/49 \u001b[35m100.00% |\u001b[35m█████████████████████████\u001b[35m|\u001b[32m10.31s \u001b[35mloss:\u001b[94m 0.815666\u001b[35m acc:\u001b[94m 71.212000\u001b[35m val_loss:\u001b[94m 0.886250\u001b[35m val_acc:\u001b[94m 69.210000\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "pytorch_network = get_module()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(pytorch_network.parameters(), lr=0.01)\n",
    "\n",
    "model = Model(\n",
    "    pytorch_network,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    batch_metrics=[\"acc\"],\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "policy = OptimizerPolicy(\n",
    "    one_cycle_phases(epochs * steps_per_epoch, lr=(0.01, 0.1, 0.008)),\n",
    ")\n",
    "history = model.fit_generator(\n",
    "    train_dl,\n",
    "    val_dl,\n",
    "    epochs=epochs,\n",
    "    callbacks=[policy],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
