<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Sequence Tagging With an RNN &mdash; Poutyne 1.12.1 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Interface of policy" href="policy_interface.html" />
    <link rel="prev" title="Tips and Tricks" href="tips_and_tricks.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html">
            <img src="../_static/poutyne-light.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                1.12.1
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../model.html">Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../experiment.html">Experiment and ModelBundle</a></li>
<li class="toctree-l1"><a class="reference internal" href="../metrics.html">Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../callbacks.html">Callbacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../layers.html">Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils.html">Utils</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction to PyTorch and Poutyne</a></li>
<li class="toctree-l1"><a class="reference internal" href="tips_and_tricks.html">Tips and Tricks</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Sequence Tagging With an RNN</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#train-a-recurrent-neural-network-rnn">Train a Recurrent Neural Network (RNN)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#training-constants">Training Constants</a></li>
<li class="toctree-l3"><a class="reference internal" href="#rnn">RNN</a></li>
<li class="toctree-l3"><a class="reference internal" href="#fully-connected-layer">Fully-connected Layer</a></li>
<li class="toctree-l3"><a class="reference internal" href="#the-dataset">The Dataset</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#dataloader">DataLoader</a></li>
<li class="toctree-l4"><a class="reference internal" href="#full-network">Full Network</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#the-training-loop">The Training Loop</a></li>
<li class="toctree-l2"><a class="reference internal" href="#predict-on-new-data">Predict on New Data</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="policy_interface.html">Interface of <code class="docutils literal notranslate"><span class="pre">policy</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="transfer_learning.html">Transfer learning example</a></li>
<li class="toctree-l1"><a class="reference internal" href="image_reconstruction.html">Image Reconstruction Using Poutyne</a></li>
<li class="toctree-l1"><a class="reference internal" href="classification_and_regression.html">Gender Classification and Eyes Location Detection: A Two Task Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="semantic_segmentation.html">Semantic segmentation using Poutyne</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Poutyne</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Sequence Tagging With an RNN</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/examples/sequence_tagging.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="sequence-tagging-with-an-rnn">
<span id="sequence-tagging"></span><h1>Sequence Tagging With an RNN<a class="headerlink" href="#sequence-tagging-with-an-rnn" title="Permalink to this heading"></a></h1>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>See the notebook <a class="reference external" href="https://github.com/GRAAL-Research/poutyne/blob/master/examples/sequence_tagging.ipynb">here</a></p></li>
<li><p>Run in <a class="reference external" href="https://colab.research.google.com/github/GRAAL-Research/poutyne/blob/master/examples/sequence_tagging.ipynb">Google Colab</a></p></li>
</ul>
</div>
<p>In this example, we will do sequence tagging with RNNs using Poutyne.</p>
<p>Let’s import all the needed packages.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">contextlib</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pickle</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">from</span> <span class="nn">io</span> <span class="kn">import</span> <span class="n">TextIOBase</span>

<span class="kn">import</span> <span class="nn">fasttext</span>
<span class="kn">import</span> <span class="nn">fasttext.util</span>
<span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">from</span> <span class="nn">torch.nn.utils.rnn</span> <span class="kn">import</span> <span class="n">pad_packed_sequence</span><span class="p">,</span> <span class="n">pack_padded_sequence</span><span class="p">,</span> <span class="n">pad_sequence</span><span class="p">,</span> <span class="n">PackedSequence</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="kn">from</span> <span class="nn">poutyne</span> <span class="kn">import</span> <span class="n">set_seeds</span><span class="p">,</span> <span class="n">Model</span>
</pre></div>
</div>
<p>Also, we need to set Python’s, NumPy’s and PyTorch’s seeds by using Poutyne function so that our training is (almost) reproducible.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">set_seeds</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
<section id="train-a-recurrent-neural-network-rnn">
<h2>Train a Recurrent Neural Network (RNN)<a class="headerlink" href="#train-a-recurrent-neural-network-rnn" title="Permalink to this heading"></a></h2>
<p>We train an RNN, or more precisely, an LSTM, to predict the sequence of tags associated with a given address, which is also known as address parsing.</p>
<p>This task consists of detecting, by tagging, the different parts of an address such as the civic number, the street name or the postal code (or zip code). The following figure shows an example of such a tagging.</p>
<img alt="../_images/address_parsing.png" src="../_images/address_parsing.png" />
<p>Since addresses are written in a predetermined sequence, RNN is the best way to crack this problem. For our architecture, we will use two components, an RNN and a fully-connected layer.</p>
<section id="training-constants">
<h3>Training Constants<a class="headerlink" href="#training-constants" title="Permalink to this heading"></a></h3>
<p>Now, let’s set our training constants. We first have the CUDA device used for training if one is present. Second, we set the batch size (i.e. the number of elements to see before updating the model) and the learning rate for the optimizer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cuda_device</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">cuda_device</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.1</span>
</pre></div>
</div>
</section>
<section id="rnn">
<h3>RNN<a class="headerlink" href="#rnn" title="Permalink to this heading"></a></h3>
<p>For the first component, instead of using a vanilla RNN, we use a variant of it, known as a long short-term memory (LSTM) (to learn more about <a class="reference external" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">LSTM</a>. For now, we use a single-layer unidirectional LSTM.</p>
<p>Also, since our data is textual, we will use the well-known word embeddings to encode the textual information. The LSTM input and hidden state dimensions will be of the same size. This size corresponds to the word embeddings dimension, which in our case will be the <a class="reference external" href="https://fasttext.cc/docs/en/crawl-vectors.html">French pre trained</a> fastText embeddings of dimension <code class="docutils literal notranslate"><span class="pre">300</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See this <a class="reference external" href="https://discuss.pytorch.org/t/could-someone-explain-batch-first-true-in-lstm/15402">discussion</a> for the explanation why we use the <code class="docutils literal notranslate"><span class="pre">batch_first</span></code> argument.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dimension</span> <span class="o">=</span> <span class="mi">300</span>
<span class="n">num_layer</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">bidirectional</span> <span class="o">=</span> <span class="kc">False</span>

<span class="n">lstm_network</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="n">dimension</span><span class="p">,</span>
                       <span class="n">hidden_size</span><span class="o">=</span><span class="n">dimension</span><span class="p">,</span>
                       <span class="n">num_layers</span><span class="o">=</span><span class="n">num_layer</span><span class="p">,</span>
                       <span class="n">bidirectional</span><span class="o">=</span><span class="n">bidirectional</span><span class="p">,</span>
                       <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="fully-connected-layer">
<h3>Fully-connected Layer<a class="headerlink" href="#fully-connected-layer" title="Permalink to this heading"></a></h3>
<p>We use this layer to map the representation of the LSTM (<code class="docutils literal notranslate"><span class="pre">300</span></code>) to the tag space (8, the number of tags) and predict the most likely tag using a softmax.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">dimension</span> <span class="c1"># the output of the LSTM</span>
<span class="n">tag_dimension</span> <span class="o">=</span> <span class="mi">8</span>

<span class="n">fully_connected_network</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">tag_dimension</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="the-dataset">
<h3>The Dataset<a class="headerlink" href="#the-dataset" title="Permalink to this heading"></a></h3>
<p>Now let’s download our dataset; it’s already split into a train, valid and test set using the following.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">download_data</span><span class="p">(</span><span class="n">saving_dir</span><span class="p">,</span> <span class="n">data_type</span><span class="p">):</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Function to download the dataset using data_type to specify if we want the train, valid or test.</span>
<span class="sd">&quot;&quot;&quot;</span>
    <span class="n">root_url</span> <span class="o">=</span> <span class="s2">&quot;https://graal-research.github.io/poutyne-external-assets/tips_and_tricks_assets/</span><span class="si">{}</span><span class="s2">.p&quot;</span>

    <span class="n">url</span> <span class="o">=</span> <span class="n">root_url</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">data_type</span><span class="p">)</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">saving_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">saving_dir</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">data_type</span><span class="si">}</span><span class="s2">.p&quot;</span><span class="p">),</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">r</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>

<span class="n">download_data</span><span class="p">(</span><span class="s1">&#39;./datasets/addresses/&#39;</span><span class="p">,</span> <span class="s2">&quot;train&quot;</span><span class="p">)</span>
<span class="n">download_data</span><span class="p">(</span><span class="s1">&#39;./datasets/addresses/&#39;</span><span class="p">,</span> <span class="s2">&quot;valid&quot;</span><span class="p">)</span>
<span class="n">download_data</span><span class="p">(</span><span class="s1">&#39;./datasets/addresses/&#39;</span><span class="p">,</span> <span class="s2">&quot;test&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Now let’s load in memory the data.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">train_data</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="s2">&quot;./datasets/addresses/train.p&quot;</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">))</span>  <span class="c1"># 80,000 examples</span>
<span class="n">valid_data</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="s2">&quot;./datasets/addresses/valid.p&quot;</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">))</span>  <span class="c1"># 20,000 examples</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="s2">&quot;./datasets/addresses/test.p&quot;</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">))</span>  <span class="c1"># 30,000 examples</span>
</pre></div>
</div>
<p>If we take a look at the training dataset, it’s a list of <code class="docutils literal notranslate"><span class="pre">80,000</span></code> tuples where the first element is the full address, and the second element is a list of the tag (the ground truth).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">train_data</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span>
</pre></div>
</div>
<p>Here a snapshot of the output:</p>
<img alt="../_images/train_data_snapshot.png" src="../_images/train_data_snapshot.png" />
<p>Since the address is a text, we need to <em>convert</em> it into categorical value, such as word embeddings, for that we will use a vectorizer. This embedding vectorizer will be able to extract for every word embedding value.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">EmbeddingVectorizer</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Embedding vectorizer</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">fasttext</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">download_model</span><span class="p">(</span><span class="s1">&#39;fr&#39;</span><span class="p">,</span> <span class="n">if_exists</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_model</span> <span class="o">=</span> <span class="n">fasttext</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="s2">&quot;./cc.fr.``300``.bin&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">address</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Convert address to embedding vectors</span>
<span class="sd">        :param address: The address to convert</span>
<span class="sd">        :return: The embeddings vectors</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">address</span><span class="o">.</span><span class="n">split</span><span class="p">():</span>
            <span class="n">embeddings</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding_model</span><span class="p">[</span><span class="n">word</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">embeddings</span>

<span class="n">embedding_model</span> <span class="o">=</span> <span class="n">EmbeddingVectorizer</span><span class="p">()</span>
</pre></div>
</div>
<p>We also need a vectorizer to convert the address tag (e.g. StreetNumber, StreetName) into categorical values. So we will use a Vectorizer class that can use the embedding vectorizer and convert the address tag. We will explain and use the argument <code class="docutils literal notranslate"><span class="pre">predict</span></code> later.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Vectorizer</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">embedding_model</span><span class="p">,</span> <span class="n">predict</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">dataset</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_model</span> <span class="o">=</span> <span class="n">embedding_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">predict</span> <span class="o">=</span> <span class="n">predict</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tags_set</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;StreetNumber&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
            <span class="s2">&quot;StreetName&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
            <span class="s2">&quot;Unit&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
            <span class="s2">&quot;Municipality&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
            <span class="s2">&quot;Province&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
            <span class="s2">&quot;PostalCode&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
            <span class="s2">&quot;Orientation&quot;</span><span class="p">:</span> <span class="mi">6</span><span class="p">,</span>
            <span class="s2">&quot;GeneralDelivery&quot;</span><span class="p">:</span> <span class="mi">7</span>
        <span class="p">}</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># for the dataloader</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>
        <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">item</span><span class="p">]</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">:</span>
            <span class="n">address</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">address_vector</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding_model</span><span class="p">(</span><span class="n">address</span><span class="p">)</span>

            <span class="n">tags</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">idx_tags</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_tags_to_idx</span><span class="p">(</span><span class="n">tags</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">address_vector</span><span class="p">,</span> <span class="n">idx_tags</span>

        <span class="n">address_vector</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding_model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">address_vector</span>

    <span class="k">def</span> <span class="nf">_convert_tags_to_idx</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tags</span><span class="p">):</span>
        <span class="n">idx_tags</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">tag</span> <span class="ow">in</span> <span class="n">tags</span><span class="p">:</span>
            <span class="n">idx_tags</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tags_set</span><span class="p">[</span><span class="n">tag</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">idx_tags</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">train_data_vectorize</span> <span class="o">=</span> <span class="n">Vectorizer</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">embedding_model</span><span class="p">)</span>
<span class="n">valid_data_vectorize</span> <span class="o">=</span> <span class="n">Vectorizer</span><span class="p">(</span><span class="n">valid_data</span><span class="p">,</span> <span class="n">embedding_model</span><span class="p">)</span>
<span class="n">test_data_vectorize</span> <span class="o">=</span> <span class="n">Vectorizer</span><span class="p">(</span><span class="n">test_data</span><span class="p">,</span> <span class="n">embedding_model</span><span class="p">)</span>
</pre></div>
</div>
<section id="dataloader">
<h4>DataLoader<a class="headerlink" href="#dataloader" title="Permalink to this heading"></a></h4>
<p>Now, since all the addresses are not of the same size, it is impossible to batch them together since all elements of a tensor must have the same lengths. But there is a trick, padding!</p>
<p>The idea is simple. We add <em>empty</em> tokens at the end of each sequence up to the longest one in a batch. For the word vectors, we add vectors of 0 as padding. For the tag indices, we pad with -100s. We do so because of the <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss" title="(in PyTorch v1.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrossEntropyLoss</span></code></a>, the accuracy metric and the <a class="reference internal" href="../metrics.html#poutyne.F1" title="poutyne.F1"><code class="xref py py-class docutils literal notranslate"><span class="pre">F1</span></code></a> metric all ignore targets with values of <code class="docutils literal notranslate"><span class="pre">-100</span></code>.</p>
<p>To do this padding, we use the <code class="docutils literal notranslate"><span class="pre">collate_fn</span></code> argument of the PyTorch <a class="reference external" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader" title="(in PyTorch v1.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code></a> and on running time, that process will be done. One thing to take into account, since we pad the sequence, we need each sequence’s lengths to unpad them in the forward pass. That way, we can pad and pack the sequence to minimize the training time (read <a class="reference external" href="https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch">this good explanation</a> of why we pad and pack sequences).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">pad_collate_fn</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The collate_fn that can add padding to the sequences so all can have</span>
<span class="sd">    the same length as the longest one.</span>

<span class="sd">    Args:</span>
<span class="sd">        batch (List[List, List]): The batch data, where the first element</span>
<span class="sd">        of the tuple are the word idx and the second element are the target</span>
<span class="sd">        label.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A tuple (x, y). The element x is a tensor of packed sequence .</span>
<span class="sd">        The element y is a tensor of padded tag indices. The word vectors are</span>
<span class="sd">        padded with vectors of 0s and the tag indices are padded with -100s.</span>
<span class="sd">        Padding with -100 is done because of the cross-entropy loss and the</span>
<span class="sd">        accuracy metric ignores the targets with values -100.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># This gets us two lists of tensors and a list of integer.</span>
    <span class="c1"># Each tensor in the first list is a sequence of word vectors.</span>
    <span class="c1"># Each tensor in the second list is a sequence of tag indices.</span>
    <span class="c1"># The list of integer consist of the lengths of the sequences in order.</span>
    <span class="n">sequences_vectors</span><span class="p">,</span> <span class="n">sequences_labels</span><span class="p">,</span> <span class="n">lengths</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="p">[</span>
        <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">seq_vectors</span><span class="p">)),</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">labels</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">seq_vectors</span><span class="p">))</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">seq_vectors</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="p">])</span>

    <span class="n">lengths</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">lengths</span><span class="p">)</span>

    <span class="n">padded_sequences_vectors</span> <span class="o">=</span> <span class="n">pad_sequence</span><span class="p">(</span><span class="n">sequences_vectors</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">pack_padded_sequences_vectors</span> <span class="o">=</span> <span class="n">pack_padded_sequence</span><span class="p">(</span>
        <span class="n">padded_sequences_vectors</span><span class="p">,</span> <span class="n">lengths</span><span class="o">.</span><span class="n">cpu</span><span class="p">(),</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>  <span class="c1"># We pack the padded sequence to improve the computational speed during training</span>

    <span class="n">padded_sequences_labels</span> <span class="o">=</span> <span class="n">pad_sequence</span><span class="p">(</span><span class="n">sequences_labels</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=-</span><span class="mi">100</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">pack_padded_sequences_vectors</span><span class="p">,</span> <span class="n">padded_sequences_labels</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_data_vectorize</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">pad_collate_fn</span><span class="p">)</span>
<span class="n">valid_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">valid_data_vectorize</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">pad_collate_fn</span><span class="p">)</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">test_data_vectorize</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">pad_collate_fn</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="full-network">
<h4>Full Network<a class="headerlink" href="#full-network" title="Permalink to this heading"></a></h4>
<p>Since our sequences are of variable lengths and we want to be the most efficient possible by packing them, we cannot use the PyTorch <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential" title="(in PyTorch v1.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Sequential</span></code></a> class to define our model, so we define the forward pass for it to pack and unpack the sequences (again, you can read <a class="reference external" href="https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch">this good explanation</a> of why we pad and pack sequences).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">FullNetWork</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lstm_network</span><span class="p">,</span> <span class="n">fully_connected_network</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_state</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">lstm_network</span> <span class="o">=</span> <span class="n">lstm_network</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fully_connected_network</span> <span class="o">=</span> <span class="n">fully_connected_network</span>

<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pack_padded_sequences_vectors</span><span class="p">:</span> <span class="n">PackedSequence</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">            Defines the computation performed at every call.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">lstm_out</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm_network</span><span class="p">(</span><span class="n">pack_padded_sequences_vectors</span><span class="p">)</span>
        <span class="n">lstm_out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">pad_packed_sequence</span><span class="p">(</span><span class="n">lstm_out</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">tag_space</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fully_connected_network</span><span class="p">(</span><span class="n">lstm_out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tag_space</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># We need to transpose since it&#39;s a sequence</span>

<span class="n">full_network</span> <span class="o">=</span> <span class="n">FullNetWork</span><span class="p">(</span><span class="n">lstm_network</span><span class="p">,</span> <span class="n">fully_connected_network</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="summary">
<h3>Summary<a class="headerlink" href="#summary" title="Permalink to this heading"></a></h3>
<p>So we have created an LSTM network (<code class="docutils literal notranslate"><span class="pre">lstm_network</span></code>), a fully connected network (<code class="docutils literal notranslate"><span class="pre">fully_connected_network</span></code>), those two components are used in the full network. This full network used padded, packed sequences (defined in the forward pass), so we created the <code class="docutils literal notranslate"><span class="pre">pad_collate_fn</span></code> function to process the needed work. The DataLoader will conduct that process. Finally, when we load the data, this will be done using the vectorizer, so the address will be represented using word embeddings. Also, the address components will be converted into categorical value (from 0 to 7).
Now that we have all the components for the network let’s define our SGD optimizer.</p>
</section>
</section>
<section id="the-training-loop">
<h2>The Training Loop<a class="headerlink" href="#the-training-loop" title="Permalink to this heading"></a></h2>
<p>Now that we have all the components for the network let’s train our model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">full_network</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="p">)</span>
<span class="n">loss_function</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">full_network</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">loss_function</span><span class="p">,</span>
              <span class="n">batch_metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">],</span>
              <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit_generator</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">valid_loader</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">test_loss</span><span class="p">,</span> <span class="n">test_acc</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate_generator</span><span class="p">(</span><span class="n">test_loader</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="predict-on-new-data">
<h2>Predict on New Data<a class="headerlink" href="#predict-on-new-data" title="Permalink to this heading"></a></h2>
<p>Now, let say we want to predict using our trained model. For these new addresses, we will only have the address and not the tags. Let us download this dataset.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">download_data</span><span class="p">(</span><span class="s1">&#39;./datasets/addresses/&#39;</span><span class="p">,</span> <span class="s2">&quot;predict&quot;</span><span class="p">)</span>
<span class="n">predict_data</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="s2">&quot;./datasets/addresses/predict.p&quot;</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">))</span>  <span class="c1"># 30,000 examples</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">predict_data</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span>
</pre></div>
</div>
<p>Here a snapshot of the output:</p>
<img alt="../_images/predict_data_snapshot.png" src="../_images/predict_data_snapshot.png" />
<p>We also need to reuse the vectorizer, but now with the <code class="docutils literal notranslate"><span class="pre">predict</span></code> argument set to <code class="docutils literal notranslate"><span class="pre">True</span></code> since we cannot parse the ground truth.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">predict_data_vectorized</span> <span class="o">=</span> <span class="n">Vectorizer</span><span class="p">(</span><span class="n">predict_data</span><span class="p">,</span> <span class="n">embedding_model</span><span class="p">,</span> <span class="n">predict</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>We need to change a little the <code class="docutils literal notranslate"><span class="pre">pad_collate_fn</span></code> since we also pad the labels.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">pad_collate_fn_predict</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The collate_fn add padding to the sequences so all can have</span>
<span class="sd">    the same length as the longest one.</span>

<span class="sd">    Args:</span>
<span class="sd">        batch (List[List]): The batch data of the word idx.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A tuple (x, y). The element x is a tensor of padded word vectors, and y</span>
<span class="sd">        their respective lengths of the sequences. The word vectors are padded with vectors of 0s.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># This gets us two lists of tensors and a list of integers.</span>
    <span class="c1"># Each tensor in the first list is a sequence of word vectors.</span>
    <span class="c1"># The list of integers consists of the lengths of the sequences in order.</span>

    <span class="n">sequences_vectors</span><span class="p">,</span> <span class="n">lengths</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="p">[</span>
        <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">seq_vectors</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">seq_vectors</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">seq_vectors</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="p">])</span>

    <span class="n">lengths</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">lengths</span><span class="p">)</span>

    <span class="n">padded_sequences_vectors</span> <span class="o">=</span> <span class="n">pad_sequence</span><span class="p">(</span><span class="n">sequences_vectors</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">pack_padded_sequences_vectors</span> <span class="o">=</span> <span class="n">pack_padded_sequence</span><span class="p">(</span><span class="n">padded_sequences_vectors</span><span class="p">,</span> <span class="n">lengths</span><span class="o">.</span><span class="n">cpu</span><span class="p">(),</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">pack_padded_sequences_vectors</span>


<span class="n">predict_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">predict_data_vectorized</span><span class="p">,</span>
                            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                            <span class="n">collate_fn</span><span class="o">=</span><span class="n">pad_collate_fn_predict</span><span class="p">,</span>
                            <span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>Now, let’s predict using the <a class="reference internal" href="../model.html#poutyne.Model.predict_generator" title="poutyne.Model.predict_generator"><code class="xref py py-meth docutils literal notranslate"><span class="pre">predict_generator</span></code></a> but without concatenating the returns since batches are not always the same size.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_generator</span><span class="p">(</span><span class="n">predict_loader</span><span class="p">,</span> <span class="n">concatenate_returns</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>Or using the <a class="reference internal" href="../model.html#poutyne.Model.predict_dataset" title="poutyne.Model.predict_dataset"><code class="xref py py-meth docutils literal notranslate"><span class="pre">predict_dataset</span></code></a> method:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_dataset</span><span class="p">(</span><span class="n">predict_data_vectorized</span><span class="p">,</span>
                                    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                    <span class="n">collate_fn</span><span class="o">=</span><span class="n">pad_collate_fn_predict</span><span class="p">,</span>
                                    <span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                                    <span class="n">concatenate_returns</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>But, as you can see, we have log probabilities of our model, so we need to take the highest probability per address element.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">idx_predictions</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">predictions</span><span class="p">:</span>
    <span class="n">idx_predictions</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>  <span class="c1"># we take the highest argument (so the tag idx).</span>
</pre></div>
</div>
<p>To convert those indexes into tags, we only need to convert them back using the inverse of the previous <code class="docutils literal notranslate"><span class="pre">tags_to_idx</span></code> dictionary.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">idx_to_tags</span> <span class="o">=</span> <span class="p">{</span>
    <span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;StreetNumber&quot;</span><span class="p">,</span>
    <span class="mi">1</span><span class="p">:</span> <span class="s2">&quot;StreetName&quot;</span><span class="p">,</span>
    <span class="mi">2</span><span class="p">:</span> <span class="s2">&quot;Unit&quot;</span><span class="p">,</span>
    <span class="mi">3</span><span class="p">:</span> <span class="s2">&quot;Municipality&quot;</span><span class="p">,</span>
    <span class="mi">4</span><span class="p">:</span> <span class="s2">&quot;Province&quot;</span><span class="p">,</span>
    <span class="mi">5</span><span class="p">:</span> <span class="s2">&quot;PostalCode&quot;</span><span class="p">,</span>
    <span class="mi">6</span><span class="p">:</span> <span class="s2">&quot;Orientation&quot;</span><span class="p">,</span>
    <span class="mi">7</span><span class="p">:</span> <span class="s2">&quot;GeneralDelivery&quot;</span>
<span class="p">}</span>

<span class="n">tags_predictions</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">address</span> <span class="ow">in</span> <span class="n">idx_predictions</span><span class="p">:</span>
    <span class="n">tags_predictions</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">idx_to_tags</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">tag</span><span class="p">)</span> <span class="k">for</span> <span class="n">tag</span> <span class="ow">in</span> <span class="n">address</span><span class="p">])</span>

<span class="n">tags_predictions</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span>
</pre></div>
</div>
<p>Here a snapshot of the output:</p>
<blockquote>
<div><img alt="../_images/predict_output_snapshot.png" src="../_images/predict_output_snapshot.png" />
</div></blockquote>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="tips_and_tricks.html" class="btn btn-neutral float-left" title="Tips and Tricks" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="policy_interface.html" class="btn btn-neutral float-right" title="Interface of policy" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2018-2022, Frédérik Paradis.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-177874682-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-177874682-1');
  gtag('config', 'G-VJM5JZMZ01');
</script>


</body>
</html>