<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Tips and Tricks &mdash; Poutyne 1.12 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Sequence Tagging With an RNN" href="sequence_tagging.html" />
    <link rel="prev" title="Introduction to PyTorch and Poutyne" href="introduction.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html">
            <img src="../_static/poutyne-light.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                1.12
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../model.html">Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../experiment.html">Experiment and ModelBundle</a></li>
<li class="toctree-l1"><a class="reference internal" href="../metrics.html">Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../callbacks.html">Callbacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../layers.html">Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils.html">Utils</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction to PyTorch and Poutyne</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Tips and Tricks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#hyperparameters-dataset-and-network">Hyperparameters, Dataset and Network</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#training-constants">Training Constants</a></li>
<li class="toctree-l3"><a class="reference internal" href="#loading-the-dataset">Loading the Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="#initializing-the-network">Initializing the Network</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#vanilla-usage">Vanilla Usage</a></li>
<li class="toctree-l2"><a class="reference internal" href="#plotting-training">Plotting Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="#initilalizing-your-optimizer-and-loss-function-yourself">Initilalizing Your Optimizer and Loss Function Yourself</a></li>
<li class="toctree-l2"><a class="reference internal" href="#bypassing-pytorch-dataloaders">Bypassing PyTorch DataLoaders</a></li>
<li class="toctree-l2"><a class="reference internal" href="#using-callbacks">Using Callbacks</a></li>
<li class="toctree-l2"><a class="reference internal" href="#making-your-own-callback">Making Your Own Callback</a></li>
<li class="toctree-l2"><a class="reference internal" href="#using-modelbundle">Using ModelBundle</a></li>
<li class="toctree-l2"><a class="reference internal" href="#coloring">Coloring</a></li>
<li class="toctree-l2"><a class="reference internal" href="#epoch-metrics">Epoch metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="#custom-metric-names">Custom Metric Names</a></li>
<li class="toctree-l2"><a class="reference internal" href="#multi-gpus">Multi-GPUs</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="sequence_tagging.html">Sequence Tagging With an RNN</a></li>
<li class="toctree-l1"><a class="reference internal" href="policy_interface.html">Interface of <code class="docutils literal notranslate"><span class="pre">policy</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="transfer_learning.html">Transfer learning example</a></li>
<li class="toctree-l1"><a class="reference internal" href="image_reconstruction.html">Image Reconstruction Using Poutyne</a></li>
<li class="toctree-l1"><a class="reference internal" href="classification_and_regression.html">Gender Classification and Eyes Location Detection: A Two Task Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="semantic_segmentation.html">Semantic segmentation using Poutyne</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Poutyne</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Tips and Tricks</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/examples/tips_and_tricks.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="tips-and-tricks">
<span id="id1"></span><h1>Tips and Tricks<a class="headerlink" href="#tips-and-tricks" title="Permalink to this heading"></a></h1>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>See the notebook <a class="reference external" href="https://github.com/GRAAL-Research/poutyne/blob/master/examples/tips_and_tricks.ipynb">here</a></p></li>
<li><p>Run in <a class="reference external" href="https://colab.research.google.com/github/GRAAL-Research/poutyne/blob/master/examples/tips_and_tricks.ipynb">Google Colab</a></p></li>
</ul>
</div>
<p>Poutyne over a variety of tools for fine-tuning the information generated during the training, such as colouring the training update message, a progress bar, multi-GPUs, user callbacks interface and a user naming interface for the metrics’ names.</p>
<p>Let’s import all the needed packages.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">shutil</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_auc_score</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">random_split</span><span class="p">,</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span><span class="p">,</span> <span class="n">utils</span>
<span class="kn">from</span> <span class="nn">torchvision.datasets.mnist</span> <span class="kn">import</span> <span class="n">MNIST</span>

<span class="kn">from</span> <span class="nn">poutyne</span> <span class="kn">import</span> <span class="n">set_seeds</span><span class="p">,</span> <span class="n">Model</span><span class="p">,</span> <span class="n">ModelCheckpoint</span><span class="p">,</span> <span class="n">CSVLogger</span><span class="p">,</span> <span class="n">Callback</span><span class="p">,</span> <span class="n">ModelBundle</span><span class="p">,</span> <span class="n">SKLearnMetrics</span><span class="p">,</span> <span class="n">plot_history</span>
</pre></div>
</div>
<section id="hyperparameters-dataset-and-network">
<h2>Hyperparameters, Dataset and Network<a class="headerlink" href="#hyperparameters-dataset-and-network" title="Permalink to this heading"></a></h2>
<p>In this section, we setup the hyperparameters, dataset and network we will use throughout these tips and tricks.</p>
<section id="training-constants">
<h3>Training Constants<a class="headerlink" href="#training-constants" title="Permalink to this heading"></a></h3>
<p>Now, let’s set our training constants. We first have the CUDA device used for training if one is present. Second, we set the <code class="docutils literal notranslate"><span class="pre">train_split</span></code> to 0.8 (80%) to use 80% of the dataset for training and 20% for testing the trained model. Third, we set the number of classes (i.e. one for each digit). Finally, we set the batch size (i.e. the number of elements to see before updating the model), the learning rate for the optimizer, and the number of epochs (i.e. the number of times we see the full dataset).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cuda_device</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">cuda_device</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

<span class="n">train_split_percent</span> <span class="o">=</span> <span class="mf">0.8</span>

<span class="n">num_classes</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">5</span>
</pre></div>
</div>
<p>In Poutyne, as we will see in the following sections, you can define your own loss functions and optimizers. However, we can also pass magic strings to use PyTorch’s standard optimizers and loss functions. Furthermore, for the optimizer, we can also use a dictionary to set other parameters as the learning rate, for instance, if we don’t want the default learning rate.</p>
<p>Here, we initialize the dictionary for our optimizer as well as the string for our loss function. We thus use SGD with the specified learning rate and the cross-entropy loss.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">optim</span><span class="o">=</span><span class="s1">&#39;sgd&#39;</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span> <span class="c1"># Could be &#39;sgd&#39; if we didn&#39;t need to change the learning rate.</span>
<span class="n">loss_function</span> <span class="o">=</span> <span class="s1">&#39;cross_entropy&#39;</span>
</pre></div>
</div>
</section>
<section id="loading-the-dataset">
<h3>Loading the Dataset<a class="headerlink" href="#loading-the-dataset" title="Permalink to this heading"></a></h3>
<p>The following code helps load the MNIST dataset and creates the PyTorch DataLoaders that split our datasets into batches. Then, the train DataLoader shuffles the examples of the training dataset to draw the examples without replacement.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">full_train_dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="s1">&#39;./datasets&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">())</span>
<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="s1">&#39;./datasets&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">())</span>

<span class="n">num_data</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">full_train_dataset</span><span class="p">)</span>
<span class="n">train_length</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">train_split_percent</span> <span class="o">*</span> <span class="n">num_data</span><span class="p">))</span>
<span class="n">valid_length</span> <span class="o">=</span> <span class="n">num_data</span> <span class="o">-</span> <span class="n">train_length</span>

<span class="n">train_dataset</span><span class="p">,</span> <span class="n">valid_dataset</span> <span class="o">=</span> <span class="n">random_split</span><span class="p">(</span><span class="n">full_train_dataset</span><span class="p">,</span>
                                            <span class="p">[</span><span class="n">train_length</span><span class="p">,</span> <span class="n">valid_length</span><span class="p">],</span>
                                            <span class="n">generator</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">))</span>

<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">valid_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">valid_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="initializing-the-network">
<h3>Initializing the Network<a class="headerlink" href="#initializing-the-network" title="Permalink to this heading"></a></h3>
<p>We create a fonction to initialize a simple convolutional neural network.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">create_network</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.25</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">32</span><span class="o">*</span><span class="mi">7</span><span class="o">*</span><span class="mi">7</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
    <span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="vanilla-usage">
<h2>Vanilla Usage<a class="headerlink" href="#vanilla-usage" title="Permalink to this heading"></a></h2>
<p>The following code trains our network in the simplest way possible with Poutyne. We use the accuracy metric so that we can see the performance during training.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Instantiating our network</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">create_network</span><span class="p">()</span>

<span class="c1"># Poutyne Model on GPU</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">loss_function</span><span class="p">,</span>
              <span class="n">batch_metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">],</span>
              <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># Train</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit_generator</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">valid_loader</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">)</span>

<span class="c1"># Test</span>
<span class="n">test_loss</span><span class="p">,</span> <span class="n">test_acc</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate_generator</span><span class="p">(</span><span class="n">test_loader</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="plotting-training">
<h2>Plotting Training<a class="headerlink" href="#plotting-training" title="Permalink to this heading"></a></h2>
<p>The training methods in Poutyne returns a list of dictionaries containing the metrics output during training. Using Poutyne, it is then possible to plot this training history.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">_</span> <span class="o">=</span> <span class="n">plot_history</span><span class="p">(</span><span class="n">history</span><span class="p">)</span>
</pre></div>
</div>
<img alt="../_images/plot_history_time.png" src="../_images/plot_history_time.png" />
<img alt="../_images/plot_history_loss.png" src="../_images/plot_history_loss.png" />
<img alt="../_images/plot_history_acc.png" src="../_images/plot_history_acc.png" />
<p>It is also possible to restrict the metrics shown and to customize the plots by using custom labels and titles.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">_</span> <span class="o">=</span> <span class="n">plot_history</span><span class="p">(</span>
    <span class="n">history</span><span class="p">,</span>
    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">,</span> <span class="s1">&#39;acc&#39;</span><span class="p">],</span>
    <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Loss&#39;</span><span class="p">,</span> <span class="s1">&#39;Accuracy&#39;</span><span class="p">],</span>
    <span class="n">titles</span><span class="o">=</span><span class="s1">&#39;Training of MNIST&#39;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<img alt="../_images/plot_history_loss_customized.png" src="../_images/plot_history_loss_customized.png" />
<img alt="../_images/plot_history_acc_customized.png" src="../_images/plot_history_acc_customized.png" />
</section>
<section id="initilalizing-your-optimizer-and-loss-function-yourself">
<h2>Initilalizing Your Optimizer and Loss Function Yourself<a class="headerlink" href="#initilalizing-your-optimizer-and-loss-function-yourself" title="Permalink to this heading"></a></h2>
<p>Instead of using magic strings for the optimizer and the loss function, it’s quite easy to initialize your own and pass them to Poutyne.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Instantiating our network</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">create_network</span><span class="p">()</span>

<span class="c1"># Instantiating our loss function and optimizer</span>
<span class="n">own_optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">network</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
<span class="n">own_loss_function</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>

<span class="c1"># Poutyne Model on GPU</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">own_optimizer</span><span class="p">,</span> <span class="n">own_loss_function</span><span class="p">,</span>
              <span class="n">batch_metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">],</span>
              <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># Train</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit_generator</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">valid_loader</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">)</span>

<span class="c1"># Test</span>
<span class="n">test_loss</span><span class="p">,</span> <span class="n">test_acc</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate_generator</span><span class="p">(</span><span class="n">test_loader</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="bypassing-pytorch-dataloaders">
<h2>Bypassing PyTorch DataLoaders<a class="headerlink" href="#bypassing-pytorch-dataloaders" title="Permalink to this heading"></a></h2>
<p>Above, we defined DataLoaders for our datasets. However, with Poutyne, it is not strictly necessary since it provides the <a class="reference internal" href="../model.html#poutyne.Model.fit_dataset" title="poutyne.Model.fit_dataset"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit_dataset</span></code></a> and <a class="reference internal" href="../model.html#poutyne.Model.evaluate_dataset" title="poutyne.Model.evaluate_dataset"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_dataset</span></code></a> methods to which you can pass the necessary parameters such as the batch size. Under the hood, Poutyne initializes the DataLoaders for you.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Instantiating our network</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">create_network</span><span class="p">()</span>

<span class="c1"># Poutyne Model on GPU</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">loss_function</span><span class="p">,</span>
              <span class="n">batch_metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">],</span>
              <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># Train</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit_dataset</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span>
                  <span class="n">valid_dataset</span><span class="p">,</span>
                  <span class="n">epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">,</span>
                  <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                  <span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Test</span>
<span class="n">test_loss</span><span class="p">,</span> <span class="n">test_acc</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate_dataset</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span>
                                             <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                             <span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="using-callbacks">
<h2>Using Callbacks<a class="headerlink" href="#using-callbacks" title="Permalink to this heading"></a></h2>
<p>One nice feature of Poutyne is <a class="reference internal" href="../callbacks.html#poutyne.Callback" title="poutyne.Callback"><code class="xref py py-class docutils literal notranslate"><span class="pre">callbacks</span></code></a>. Callbacks allow doing actions during the training of the neural network. In the following example, we use three callbacks. The first that saves the latest weights in a file to be able to continue the optimization at the end of training if more epochs are needed. The second that saves the best weights according to the performance on the validation dataset. The last that saves the displayed logs into a TSV file.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Saves everything into saves/convnet_mnist</span>
<span class="n">save_path</span> <span class="o">=</span> <span class="s2">&quot;saves/convnet_mnist&quot;</span>
<span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">save_path</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span>
    <span class="c1"># Save the latest weights to be able to continue the optimization at the end for more epochs.</span>
    <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_path</span><span class="p">,</span> <span class="s1">&#39;last_epoch.ckpt&#39;</span><span class="p">)),</span>

    <span class="c1"># Save the weights in a new file when the current model is better than all previous models.</span>
    <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_path</span><span class="p">,</span> <span class="s1">&#39;best_epoch_</span><span class="si">{epoch}</span><span class="s1">.ckpt&#39;</span><span class="p">),</span> <span class="n">monitor</span><span class="o">=</span><span class="s1">&#39;val_acc&#39;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;max&#39;</span><span class="p">,</span>
                    <span class="n">save_best_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">restore_best</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>

    <span class="c1"># Save the losses and accuracies for each epoch in a TSV.</span>
    <span class="n">CSVLogger</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_path</span><span class="p">,</span> <span class="s1">&#39;log.tsv&#39;</span><span class="p">),</span> <span class="n">separator</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">),</span>
<span class="p">]</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Instantiating our network</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">create_network</span><span class="p">()</span>

<span class="c1"># Poutyne Model on GPU</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">loss_function</span><span class="p">,</span>
              <span class="n">batch_metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">],</span>
              <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># Train</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit_generator</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span>
                    <span class="n">valid_loader</span><span class="p">,</span>
                    <span class="n">epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">,</span>
                    <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span>

<span class="c1"># Test</span>
<span class="n">test_loss</span><span class="p">,</span> <span class="n">test_acc</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate_generator</span><span class="p">(</span><span class="n">test_loader</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="making-your-own-callback">
<span id="id2"></span><h2>Making Your Own Callback<a class="headerlink" href="#making-your-own-callback" title="Permalink to this heading"></a></h2>
<p>While Poutyne provides a great number of <a class="reference internal" href="../callbacks.html#poutyne.Callback" title="poutyne.Callback"><code class="xref py py-class docutils literal notranslate"><span class="pre">predefined</span> <span class="pre">callbacks</span></code></a>, it is sometimes useful to make your own callback.</p>
<p>In the following example, we want to see the effect of temperature on the optimization of our neural network. To do so, we either increase or decrease the temperature during the optimization. As one can see in the result, temperature either as no effect or has a detrimental effect on the performance of the neural network. This is so because the temperature has for effect to artificially changing the learning rates. Since we have found the right learning rate, increasing or decreasing, it shows no improvement on the results.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">CrossEntropyLossWithTemperature</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This loss module is the cross-entropy loss function</span>
<span class="sd">    with temperature. It divides the logits by a temperature</span>
<span class="sd">    value before computing the cross-entropy loss.</span>

<span class="sd">    Args:</span>
<span class="sd">        initial_temperature (float): The initial value of the temperature.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">initial_temperature</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">temperature</span> <span class="o">=</span> <span class="n">initial_temperature</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">celoss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">y_true</span><span class="p">):</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">y_pred</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">temperature</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">celoss</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_true</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">TemperatureCallback</span><span class="p">(</span><span class="n">Callback</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This callback multiply the loss temperature with a decay before</span>
<span class="sd">    each batch.</span>

<span class="sd">    Args:</span>
<span class="sd">        celoss_with_temp (CrossEntropyLossWithTemperature): the loss module.</span>
<span class="sd">        decay (float): The value of the temperature decay.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">celoss_with_temp</span><span class="p">,</span> <span class="n">decay</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">celoss_with_temp</span> <span class="o">=</span> <span class="n">celoss_with_temp</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decay</span> <span class="o">=</span> <span class="n">decay</span>

    <span class="k">def</span> <span class="nf">on_train_batch_begin</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">logs</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">celoss_with_temp</span><span class="o">.</span><span class="n">temperature</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decay</span>
</pre></div>
</div>
<p>So our loss function will be the cross-entropy with temperature with an initial temperature of <code class="docutils literal notranslate"><span class="pre">0.1</span></code> and a temperature decay of <code class="docutils literal notranslate"><span class="pre">1.0008</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">loss_function</span> <span class="o">=</span> <span class="n">CrossEntropyLossWithTemperature</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">callbacks</span> <span class="o">=</span> <span class="n">callbacks</span> <span class="o">+</span> <span class="p">[</span><span class="n">TemperatureCallback</span><span class="p">(</span><span class="n">loss_function</span><span class="p">,</span> <span class="mf">1.0008</span><span class="p">)]</span>
</pre></div>
</div>
<p>Now let’s test our training loop for one epoch using the accuracy as the batch metric.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Instantiating our network</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">create_network</span><span class="p">()</span>

<span class="c1"># Poutyne Model on GPU</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">custom_loss_function</span><span class="p">,</span>
              <span class="n">batch_metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">],</span>
              <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># Train</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit_generator</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span>
                    <span class="n">valid_loader</span><span class="p">,</span>
                    <span class="n">epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">,</span>
                    <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span>

<span class="c1"># Test</span>
<span class="n">test_loss</span><span class="p">,</span> <span class="n">test_acc</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate_generator</span><span class="p">(</span><span class="n">test_loader</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="using-modelbundle">
<h2>Using ModelBundle<a class="headerlink" href="#using-modelbundle" title="Permalink to this heading"></a></h2>
<p>Most of the time, when using Poutyne (or even PyTorch in general), we will find ourselves in an iterative model hyperparameters finetuning loop. For efficient model search, we will usually wish to save our best performing models, their training and testing statistics and even sometimes wish to retrain an already trained model for further tuning. All of the above can be easily implemented with the flexibility of Poutyne Callbacks, but having to define and initialize each and every Callback object we wish for our model quickly feels cumbersome.</p>
<p>This is why Poutyne provides a <a class="reference internal" href="../experiment.html#poutyne.ModelBundle" title="poutyne.ModelBundle"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModelBundle</span></code></a> class, which aims specifically at enabling quick model iteration search, while not sacrificing the quality of a single experiment - statistics logging, best models saving, etc. As the name says, ModelBundle is actually a simple wrapper between a PyTorch network and Poutyne’s core Callback objects for logging and saving. Given a working directory where to output the various logging files and a PyTorch network, the ModelBundle class reduces the whole training loop to a single line.</p>
<p>The following code uses Poutyne’s <a class="reference internal" href="../experiment.html#poutyne.ModelBundle" title="poutyne.ModelBundle"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModelBundle</span></code></a> class to train a network for 5 epochs. The code is quite simpler than the code in the Poutyne Callbacks section while doing more (only a few lines). Once trained for 5 epochs, it is then possible to resume the optimization at the 5th epoch for 5 more epochs until the 10th epoch using the same function.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train_model_bundle</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function creates a Poutyne ModelBundle, trains the input module</span>
<span class="sd">    on the train loader and then tests its performance on the test loader.</span>
<span class="sd">    All training and testing statistics are saved, as well as best model</span>
<span class="sd">    checkpoints.</span>

<span class="sd">    Args:</span>
<span class="sd">        network (torch.nn.Module): The neural network to train.</span>
<span class="sd">        working_directory (str): The directory where to output files to save.</span>
<span class="sd">        epochs (int): The number of epochs. (Default: 5)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Everything is going to be saved in ./saves/{name}.</span>
    <span class="n">save_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;saves&#39;</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>

    <span class="c1"># Poutyne ModelBundle</span>
    <span class="n">model_bundle</span> <span class="o">=</span> <span class="n">ModelBundle</span><span class="o">.</span><span class="n">from_network</span><span class="p">(</span>
        <span class="n">save_path</span><span class="p">,</span>
        <span class="n">network</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
        <span class="n">task</span><span class="o">=</span><span class="s1">&#39;classif&#39;</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Train</span>
    <span class="n">model_bundle</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">valid_loader</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">)</span>

    <span class="c1"># Test</span>
    <span class="n">model_bundle</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">test_loader</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">network</span> <span class="o">=</span> <span class="n">create_network</span><span class="p">()</span>
<span class="n">train_model_bundle</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="s1">&#39;convnet_mnist_model_bundle&#39;</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
<p>Notice how setting <code class="docutils literal notranslate"><span class="pre">task='classif'</span></code> when instantiating <a class="reference internal" href="../experiment.html#poutyne.ModelBundle" title="poutyne.ModelBundle"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModelBundle</span></code></a> adds for use our loss function, the batch metric accuracy, the epoch metric F1 and set up callbacks that use them. If you wish, you still can use your own loss function and metrics instead of passing this argument.</p>
<p>We have trained for 5 epochs, let’s now resume training for another 5 epochs for a total of 10 epochs. Notice that we reinstantiate the network. ModelBundle will load back the weights for us and resume training.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">network</span> <span class="o">=</span> <span class="n">create_network</span><span class="p">()</span>
<span class="n">train_model_bundle</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="s1">&#39;convnet_mnist_model_bundle&#39;</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="coloring">
<h2>Coloring<a class="headerlink" href="#coloring" title="Permalink to this heading"></a></h2>
<p>Also, Poutyne use by default a coloring template of the training step when the package <cite>colorama</cite> is installed.
One could either remove the coloring (<code class="docutils literal notranslate"><span class="pre">progress_options=dict(coloring=False)</span></code>) or set a different coloring template using the fields:
<code class="docutils literal notranslate"><span class="pre">text_color</span></code>, <code class="docutils literal notranslate"><span class="pre">ratio_color</span></code>, <code class="docutils literal notranslate"><span class="pre">metric_value_color</span></code>, <code class="docutils literal notranslate"><span class="pre">time_color</span></code> and <code class="docutils literal notranslate"><span class="pre">progress_bar_color</span></code>.
If a field is not specified, the default color will be used.
<a class="reference external" href="https://github.com/tartley/colorama/blob/9946cfb/colorama/ansi.py#L49">See available colors in colorama’s source code</a>.</p>
<p>Here an example where we set the <code class="docutils literal notranslate"><span class="pre">text_color</span></code> to RED and the <code class="docutils literal notranslate"><span class="pre">progress_bar_color</span></code> to LIGHTGREEN_EX.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">progress_options</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
    <span class="n">coloring</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">text_color</span><span class="o">=</span><span class="s2">&quot;RED&quot;</span><span class="p">,</span> <span class="n">progress_bar_color</span><span class="o">=</span><span class="s2">&quot;LIGHTGREEN_EX&quot;</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Instantiating our network</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">create_network</span><span class="p">()</span>

<span class="c1"># Poutyne Model on GPU</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">loss_function</span><span class="p">,</span>
              <span class="n">batch_metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">],</span>
              <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># Train</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit_generator</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span>
                    <span class="n">valid_loader</span><span class="p">,</span>
                    <span class="n">epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">,</span>
                    <span class="n">progress_options</span><span class="o">=</span><span class="n">progress_options</span><span class="p">)</span>

<span class="c1"># Test</span>
<span class="n">test_loss</span><span class="p">,</span> <span class="n">test_acc</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate_generator</span><span class="p">(</span><span class="n">test_loader</span><span class="p">,</span>
                                               <span class="n">progress_options</span><span class="o">=</span><span class="n">progress_options</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="epoch-metrics">
<h2>Epoch metrics<a class="headerlink" href="#epoch-metrics" title="Permalink to this heading"></a></h2>
<p>It’s also possible to used epoch metrics such as <a class="reference internal" href="../metrics.html#poutyne.F1" title="poutyne.F1"><code class="xref py py-class docutils literal notranslate"><span class="pre">F1</span></code></a>. You could also define your own epoch metric using the <code class="xref py py-class docutils literal notranslate"><span class="pre">EpochMetric</span></code> interface.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Instantiating our network</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">create_network</span><span class="p">()</span>

<span class="c1"># Poutyne Model on GPU</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">loss_function</span><span class="p">,</span>
              <span class="n">batch_metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">],</span>
              <span class="n">epoch_metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;f1&#39;</span><span class="p">],</span>
              <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># Train</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit_generator</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">valid_loader</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">)</span>

<span class="c1"># Test</span>
<span class="n">test_loss</span><span class="p">,</span> <span class="p">(</span><span class="n">test_acc</span><span class="p">,</span> <span class="n">test_f1</span><span class="p">)</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate_generator</span><span class="p">(</span><span class="n">test_loader</span><span class="p">)</span>
</pre></div>
</div>
<p>Furthermore, you could also use the <a class="reference internal" href="../metrics.html#poutyne.SKLearnMetrics" title="poutyne.SKLearnMetrics"><code class="xref py py-class docutils literal notranslate"><span class="pre">SKLearnMetrics</span></code></a> wrapper to wrap a Scikit-learn metric as an epoch metric. Below, we show how to compute the AUC ROC using the <a class="reference internal" href="../metrics.html#poutyne.SKLearnMetrics" title="poutyne.SKLearnMetrics"><code class="xref py py-class docutils literal notranslate"><span class="pre">SKLearnMetrics</span></code></a> class.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute softmax function.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">e_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">e_x</span> <span class="o">/</span> <span class="n">e_x</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">roc_auc</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Since the `roc_auc_score` from Scikit-learn requires normalized probabilities,</span>
<span class="sd">    we use the softmax function on the predictions.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="c1"># kwargs are keyword arguments we wish to pass to roc_auc.</span>
<span class="n">roc_epoch_metric</span> <span class="o">=</span> <span class="n">SKLearnMetrics</span><span class="p">(</span><span class="n">roc_auc</span><span class="p">,</span>
                                  <span class="n">kwargs</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">multi_class</span><span class="o">=</span><span class="s1">&#39;ovr&#39;</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Instantiating our network</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">create_network</span><span class="p">()</span>

<span class="c1"># Poutyne Model on GPU</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">loss_function</span><span class="p">,</span>
              <span class="n">batch_metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">],</span>
              <span class="n">epoch_metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;f1&#39;</span><span class="p">,</span> <span class="n">roc_epoch_metric</span><span class="p">],</span>
              <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># Train</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit_generator</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">valid_loader</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">)</span>

<span class="c1"># Test</span>
<span class="n">test_loss</span><span class="p">,</span> <span class="p">(</span><span class="n">test_acc</span><span class="p">,</span> <span class="n">test_f1</span><span class="p">,</span> <span class="n">test_roc</span><span class="p">)</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate_generator</span><span class="p">(</span><span class="n">test_loader</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="custom-metric-names">
<h2>Custom Metric Names<a class="headerlink" href="#custom-metric-names" title="Permalink to this heading"></a></h2>
<p>It’s also possible to name the metric using a tuple format <code class="docutils literal notranslate"><span class="pre">(&lt;metric</span> <span class="pre">name&gt;,</span> <span class="pre">metric)</span></code>. That way, it’s possible to use multiple times the same metric type (i.e. having micro and macro F1-score).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Instantiating our network</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">create_network</span><span class="p">()</span>

<span class="c1"># Poutyne Model on GPU</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">loss_function</span><span class="p">,</span>
              <span class="n">batch_metrics</span><span class="o">=</span><span class="p">[(</span><span class="s2">&quot;My accuracy name&quot;</span><span class="p">,</span> <span class="s1">&#39;accuracy&#39;</span><span class="p">)],</span>
              <span class="n">epoch_metrics</span><span class="o">=</span><span class="p">[(</span><span class="s2">&quot;My f1 name&quot;</span><span class="p">,</span> <span class="s1">&#39;f1&#39;</span><span class="p">)],</span>
              <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># Train</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit_generator</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">valid_loader</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">)</span>

<span class="c1"># Test</span>
<span class="n">test_loss</span><span class="p">,</span> <span class="p">(</span><span class="n">test_acc</span><span class="p">,</span> <span class="n">test_f1</span><span class="p">)</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate_generator</span><span class="p">(</span><span class="n">test_loader</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="multi-gpus">
<h2>Multi-GPUs<a class="headerlink" href="#multi-gpus" title="Permalink to this heading"></a></h2>
<p>Finally, it’s also possible to use multi-GPUs for your training either by specifying a list of devices or using the arg <code class="docutils literal notranslate"><span class="pre">&quot;all&quot;</span></code> to take them all.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Obviously, you need more than one GPUs for that option.</p>
</div>
<p>In our case here, multi-gpus takes more time because the task is not big enough to profit from multi-gpus.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Instantiating our network</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">create_network</span><span class="p">()</span>

<span class="c1"># Poutyne Model on GPU</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">loss_function</span><span class="p">,</span>
              <span class="n">batch_metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">],</span>
              <span class="n">device</span><span class="o">=</span><span class="s2">&quot;all&quot;</span><span class="p">)</span>

<span class="c1"># Train</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit_generator</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">valid_loader</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="introduction.html" class="btn btn-neutral float-left" title="Introduction to PyTorch and Poutyne" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="sequence_tagging.html" class="btn btn-neutral float-right" title="Sequence Tagging With an RNN" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2018-2022, Frédérik Paradis.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-177874682-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-177874682-1');
  gtag('config', 'G-VJM5JZMZ01');
</script>


</body>
</html>