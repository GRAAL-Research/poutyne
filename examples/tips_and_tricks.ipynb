{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poutyne's Tips and Tricks\n",
    "\n",
    "Poutyne also offers a variety of tools for fine-tuning the information generated during the training, such as colouring the training update message, a progress bar, multi-GPUs, user callbacks interface and a user naming interface for the metrics' names. \n",
    "\n",
    "Let's install the latest version of Poutyne and colorama (if they are not already), and import all the needed packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade poutyne\n",
    "%pip install --upgrade colorama\n",
    "%matplotlib inline\n",
    "import os\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "\n",
    "from poutyne import set_seeds, Model, ModelCheckpoint, CSVLogger, Callback, Experiment, SKLearnMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters, Dataset and Network\n",
    "\n",
    "In this section, we setup the hyperparameters, dataset and network we will use throughout these tips and tricks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Constants\n",
    "\n",
    "Now, let's set our training constants. We first have the CUDA device used for training if one is present. Second, we set the `train_split` to 0.8 (80%) to use 80% of the dataset for training and 20% for testing the trained model. Third, we set the number of classes (i.e. one for each digit). Finally, we set the batch size (i.e. the number of elements to see before updating the model), the learning rate for the optimizer, and the number of epochs (i.e. the number of times we see the full dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_device = 0\n",
    "device = torch.device(\"cuda:%d\" % cuda_device if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_split_percent = 0.8\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "batch_size = 32\n",
    "learning_rate = 0.1\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Poutyne, as we will see in the following sections, you can define your own loss functions and optimizers. However, we can also pass magic strings to use PyTorch's standard optimizers and loss functions. Furthermore, for the optimizer, we can also use a dictionary to set other parameters as the learning rate, for instance, if we don't want the default learning rate.\n",
    "\n",
    "Here, we initialize the dictionary for our optimizer as well as the string for our loss function. We thus use SGD with the specified learning rate and the cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = dict(optim='sgd', lr=learning_rate) # Could be 'sgd' if we didn't need to change the learning rate.\n",
    "loss_function = 'cross_entropy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset\n",
    "\n",
    "The following code helps load the MNIST dataset and creates the PyTorch DataLoaders that split our datasets into batches. Then, the train DataLoader shuffles the examples of the training dataset to draw the examples without replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_dataset = MNIST('./datasets', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = MNIST('./datasets', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "num_data = len(full_train_dataset)\n",
    "train_length = int(math.floor(train_split_percent * num_data))\n",
    "valid_length = num_data - train_length\n",
    "\n",
    "train_dataset, valid_dataset = random_split(full_train_dataset, \n",
    "                                            [train_length, valid_length],\n",
    "                                            generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=2, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48000, 12000)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the Network\n",
    "\n",
    "We initialize a simple convolutional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network():\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.Dropout(0.25),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(32*7*7, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(128, num_classes)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla Usage\n",
    "\n",
    "The following code trains our network in the simplest way possible with Poutyne. We use the accuracy metric so that we can see the performance during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[35mEpoch: \u001B[36m1/5 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m5.92s \u001B[35mloss:\u001B[94m 0.386890\u001B[35m acc:\u001B[94m 87.452083\u001B[35m val_loss:\u001B[94m 0.084405\u001B[35m val_acc:\u001B[94m 97.491667\u001B[0m\n",
      "\u001B[35mEpoch: \u001B[36m2/5 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m6.33s \u001B[35mloss:\u001B[94m 0.126130\u001B[35m acc:\u001B[94m 96.168750\u001B[35m val_loss:\u001B[94m 0.063451\u001B[35m val_acc:\u001B[94m 97.991667\u001B[0m\n",
      "\u001B[35mEpoch: \u001B[36m3/5 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m6.29s \u001B[35mloss:\u001B[94m 0.095663\u001B[35m acc:\u001B[94m 97.112500\u001B[35m val_loss:\u001B[94m 0.054529\u001B[35m val_acc:\u001B[94m 98.291667\u001B[0m\n",
      "\u001B[35mEpoch: \u001B[36m4/5 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m6.23s \u001B[35mloss:\u001B[94m 0.081406\u001B[35m acc:\u001B[94m 97.514583\u001B[35m val_loss:\u001B[94m 0.050878\u001B[35m val_acc:\u001B[94m 98.441667\u001B[0m\n",
      "\u001B[35mEpoch: \u001B[36m5/5 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m5.92s \u001B[35mloss:\u001B[94m 0.072686\u001B[35m acc:\u001B[94m 97.827083\u001B[35m val_loss:\u001B[94m 0.045625\u001B[35m val_acc:\u001B[94m 98.600000\u001B[0m\n",
      "\u001B[35mTest steps: \u001B[36m313 \u001B[32m0.76s \u001B[35mtest_loss:\u001B[94m 0.036329\u001B[35m test_acc:\u001B[94m 98.760000\u001B[0m                                                   \n"
     ]
    }
   ],
   "source": [
    "# Instantiating our network\n",
    "network = create_network()\n",
    "\n",
    "# Poutyne Model on GPU\n",
    "model = Model(network, optimizer, loss_function,\n",
    "              batch_metrics=['accuracy'], \n",
    "              device=device)\n",
    "\n",
    "# Train\n",
    "model.fit_generator(train_loader, valid_loader, epochs=num_epochs)\n",
    "\n",
    "# Test\n",
    "test_loss, test_acc = model.evaluate_generator(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initilalizing Your Optimizer and Loss Function Yourself\n",
    "\n",
    "Instead of using magic strings for the optimizer and the loss function, it's quite easy to initialize your own and pass them to Poutyne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[35mEpoch: \u001B[36m1/5 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m6.35s \u001B[35mloss:\u001B[94m 0.377052\u001B[35m acc:\u001B[94m 87.650000\u001B[35m val_loss:\u001B[94m 0.103967\u001B[35m val_acc:\u001B[94m 96.783333\u001B[0m\n",
      "\u001B[35mEpoch: \u001B[36m2/5 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m6.40s \u001B[35mloss:\u001B[94m 0.123300\u001B[35m acc:\u001B[94m 96.245833\u001B[35m val_loss:\u001B[94m 0.066045\u001B[35m val_acc:\u001B[94m 98.075000\u001B[0m\n",
      "\u001B[35mEpoch: \u001B[36m3/5 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m6.29s \u001B[35mloss:\u001B[94m 0.097580\u001B[35m acc:\u001B[94m 97.058333\u001B[35m val_loss:\u001B[94m 0.052810\u001B[35m val_acc:\u001B[94m 98.508333\u001B[0m\n",
      "\u001B[35mEpoch: \u001B[36m4/5 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m6.26s \u001B[35mloss:\u001B[94m 0.080322\u001B[35m acc:\u001B[94m 97.583333\u001B[35m val_loss:\u001B[94m 0.045271\u001B[35m val_acc:\u001B[94m 98.708333\u001B[0m\n",
      "\u001B[35mEpoch: \u001B[36m5/5 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m6.20s \u001B[35mloss:\u001B[94m 0.070436\u001B[35m acc:\u001B[94m 97.804167\u001B[35m val_loss:\u001B[94m 0.045102\u001B[35m val_acc:\u001B[94m 98.641667\u001B[0m\n",
      "\u001B[35mTest steps: \u001B[36m313 \u001B[32m0.72s \u001B[35mtest_loss:\u001B[94m 0.031262\u001B[35m test_acc:\u001B[94m 98.930000\u001B[0m                                                   \n"
     ]
    }
   ],
   "source": [
    "# Instantiating our network\n",
    "network = create_network()\n",
    "\n",
    "# Instantiating our loss function and optimizer\n",
    "own_optimizer = optim.SGD(network.parameters(), lr=learning_rate)\n",
    "own_loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Poutyne Model on GPU\n",
    "model = Model(network, own_optimizer, own_loss_function, \n",
    "              batch_metrics=['accuracy'], \n",
    "              device=device)\n",
    "\n",
    "# Train\n",
    "model.fit_generator(train_loader, valid_loader, epochs=num_epochs)\n",
    "\n",
    "# Test\n",
    "test_loss, test_acc = model.evaluate_generator(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bypassing PyTorch DataLoaders\n",
    "\n",
    "Above, we defined DataLoaders for our datasets. However, with Poutyne, it is not strictly necessary since it provides the [`fit_dataset`](https://poutyne.org/model.html#poutyne.Model.fit_dataset) and [`evaluate_dataset`](https://poutyne.org/model.html#poutyne.Model.evaluate_dataset) methods to which you can pass the necessary parameters such as the batch size. Under the hood, Poutyne initializes the DataLoaders for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[35mEpoch: \u001B[36m1/5 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m6.35s \u001B[35mloss:\u001B[94m 0.367043\u001B[35m acc:\u001B[94m 88.095833\u001B[35m val_loss:\u001B[94m 0.084185\u001B[35m val_acc:\u001B[94m 97.541667\u001B[0m\n",
      "\u001B[35mEpoch: \u001B[36m2/5 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m6.18s \u001B[35mloss:\u001B[94m 0.124308\u001B[35m acc:\u001B[94m 96.256250\u001B[35m val_loss:\u001B[94m 0.068236\u001B[35m val_acc:\u001B[94m 98.033333\u001B[0m\n",
      "\u001B[35mEpoch: \u001B[36m3/5 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m6.47s \u001B[35mloss:\u001B[94m 0.098265\u001B[35m acc:\u001B[94m 97.043750\u001B[35m val_loss:\u001B[94m 0.060035\u001B[35m val_acc:\u001B[94m 98.291667\u001B[0m\n",
      "\u001B[35mEpoch: \u001B[36m4/5 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m6.32s \u001B[35mloss:\u001B[94m 0.081726\u001B[35m acc:\u001B[94m 97.487500\u001B[35m val_loss:\u001B[94m 0.055153\u001B[35m val_acc:\u001B[94m 98.416667\u001B[0m\n",
      "\u001B[35mEpoch: \u001B[36m5/5 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m6.03s \u001B[35mloss:\u001B[94m 0.073448\u001B[35m acc:\u001B[94m 97.689583\u001B[35m val_loss:\u001B[94m 0.049198\u001B[35m val_acc:\u001B[94m 98.558333\u001B[0m\n",
      "\u001B[35mTest steps: \u001B[36m313 \u001B[32m0.72s \u001B[35mtest_loss:\u001B[94m 0.037922\u001B[35m test_acc:\u001B[94m 98.700000\u001B[0m                                                  \n"
     ]
    }
   ],
   "source": [
    "# Instantiating our network\n",
    "network = create_network()\n",
    "\n",
    "# Poutyne Model on GPU\n",
    "model = Model(network, optimizer, loss_function,\n",
    "              batch_metrics=['accuracy'], \n",
    "              device=device)\n",
    "\n",
    "# Train\n",
    "model.fit_dataset(train_dataset, \n",
    "                  valid_dataset, \n",
    "                  epochs=num_epochs, \n",
    "                  batch_size=batch_size, \n",
    "                  num_workers=2)\n",
    "\n",
    "# Test\n",
    "test_loss, test_acc = model.evaluate_dataset(test_dataset, \n",
    "                                             batch_size=batch_size, \n",
    "                                             num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Using Callbacks\n",
    "\n",
    "One nice feature of Poutyne is [callbacks](https://poutyne.org/callbacks.html). Callbacks allow doing actions during the training of the neural network. In the following example, we use three callbacks. The first that saves the latest weights in a file to be able to continue the optimization at the end of training if more epochs are needed. The second that saves the best weights according to the performance on the validation dataset. The last that saves the displayed logs into a TSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Saves everything into saves/lstm_unidirectional\n",
    "save_path = \"saves/convnet_mnist\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "callbacks = [\n",
    "    # Save the latest weights to be able to continue the optimization at the end for more epochs.\n",
    "    ModelCheckpoint(os.path.join(save_path, 'last_epoch.ckpt')),\n",
    "\n",
    "    # Save the weights in a new file when the current model is better than all previous models.\n",
    "    ModelCheckpoint(os.path.join(save_path, 'best_epoch_{epoch}.ckpt'), monitor='val_acc', mode='max', \n",
    "                    save_best_only=True, restore_best=True, verbose=True),\n",
    "\n",
    "    # Save the losses and accuracies for each epoch in a TSV.\n",
    "    CSVLogger(os.path.join(save_path, 'log.tsv'), separator='\\t'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[35mEpoch: \u001B[36m1/5 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m6.73s \u001B[35mloss:\u001B[94m 0.362374\u001B[35m acc:\u001B[94m 88.189583\u001B[35m val_loss:\u001B[94m 0.085186\u001B[35m val_acc:\u001B[94m 97.383333\u001B[0m\n",
      "Epoch 1: val_acc improved from -inf to 97.38333, saving file to saves/convnet_mnist/best_epoch_1.ckpt\n",
      "\u001B[35mEpoch: \u001B[36m2/5 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m6.44s \u001B[35mloss:\u001B[94m 0.127077\u001B[35m acc:\u001B[94m 96.181250\u001B[35m val_loss:\u001B[94m 0.060619\u001B[35m val_acc:\u001B[94m 98.291667\u001B[0m\n",
      "Epoch 2: val_acc improved from 97.38333 to 98.29167, saving file to saves/convnet_mnist/best_epoch_2.ckpt\n",
      "\u001B[35mEpoch: \u001B[36m3/5 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m6.33s \u001B[35mloss:\u001B[94m 0.096829\u001B[35m acc:\u001B[94m 97.004167\u001B[35m val_loss:\u001B[94m 0.055416\u001B[35m val_acc:\u001B[94m 98.350000\u001B[0m\n",
      "Epoch 3: val_acc improved from 98.29167 to 98.35000, saving file to saves/convnet_mnist/best_epoch_3.ckpt\n",
      "\u001B[35mEpoch: \u001B[36m4/5 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m6.24s \u001B[35mloss:\u001B[94m 0.082727\u001B[35m acc:\u001B[94m 97.564583\u001B[35m val_loss:\u001B[94m 0.047692\u001B[35m val_acc:\u001B[94m 98.591667\u001B[0m\n",
      "Epoch 4: val_acc improved from 98.35000 to 98.59167, saving file to saves/convnet_mnist/best_epoch_4.ckpt\n",
      "\u001B[35mEpoch: \u001B[36m5/5 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m6.37s \u001B[35mloss:\u001B[94m 0.075551\u001B[35m acc:\u001B[94m 97.735417\u001B[35m val_loss:\u001B[94m 0.042950\u001B[35m val_acc:\u001B[94m 98.683333\u001B[0m\n",
      "Epoch 5: val_acc improved from 98.59167 to 98.68333, saving file to saves/convnet_mnist/best_epoch_5.ckpt\n",
      "Restoring model from saves/convnet_mnist/best_epoch_5.ckpt\n",
      "\u001B[35mTest steps: \u001B[36m313 \u001B[32m0.73s \u001B[35mtest_loss:\u001B[94m 0.032706\u001B[35m test_acc:\u001B[94m 98.940000\u001B[0m                                                   \n"
     ]
    }
   ],
   "source": [
    "# Instantiating our network\n",
    "network = create_network()\n",
    "\n",
    "# Poutyne Model on GPU\n",
    "model = Model(network, optimizer, loss_function, \n",
    "              batch_metrics=['accuracy'],\n",
    "              device=device)\n",
    "\n",
    "# Train\n",
    "model.fit_generator(train_loader,\n",
    "                    valid_loader,\n",
    "                    epochs=num_epochs,\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "# Test\n",
    "test_loss, test_acc = model.evaluate_generator(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Making Your Own Callback\n",
    "\n",
    "While Poutyne provides a great number of [predefined callbacks](https://poutyne.org/callbacks.html), it is sometimes useful to make your own callback.\n",
    "\n",
    "In the following example, we want to see the effect of temperature on the optimization of our neural network. To do so, we either increase or decrease the temperature during the optimization. As one can see in the result, temperature either as no effect or has a detrimental effect on the performance of the neural network. This is so because the temperature has for effect to artificially changing the learning rates. Since we have found the right learning rate, increasing or decreasing, it shows no improvement on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLossWithTemperature(nn.Module):\n",
    "    \"\"\"\n",
    "    This loss module is the cross-entropy loss function\n",
    "    with temperature. It divides the logits by a temperature\n",
    "    value before computing the cross-entropy loss.\n",
    "\n",
    "    Args:\n",
    "        initial_temperature (float): The initial value of the temperature.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, initial_temperature):\n",
    "        super().__init__()\n",
    "        self.temperature = initial_temperature\n",
    "        self.celoss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_pred = y_pred / self.temperature\n",
    "        return self.celoss(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TemperatureCallback(Callback):\n",
    "    \"\"\"\n",
    "    This callback multiply the loss temperature with a decay before\n",
    "    each batch.\n",
    "\n",
    "    Args:\n",
    "        celoss_with_temp (CrossEntropyLossWithTemperature): the loss module.\n",
    "        decay (float): The value of the temperature decay.\n",
    "    \"\"\"\n",
    "    def __init__(self, celoss_with_temp, decay):\n",
    "        super().__init__()\n",
    "        self.celoss_with_temp = celoss_with_temp\n",
    "        self.decay = decay\n",
    "\n",
    "    def on_train_batch_begin(self, batch, logs):\n",
    "        self.celoss_with_temp.temperature *= self.decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "So our loss function will be the cross-entropy with temperature with an initial temperature of `0.1` and a temperature decay of `1.0008`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "custom_loss_function = CrossEntropyLossWithTemperature(0.1)\n",
    "callbacks = [TemperatureCallback(custom_loss_function, 1.0008)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now let's test our training loop for one epoch using the accuracy as the batch metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[35mEpoch: \u001B[36m1/5 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m6.01s \u001B[35mloss:\u001B[94m 0.465483\u001B[35m acc:\u001B[94m 85.233333\u001B[35m val_loss:\u001B[94m 0.081043\u001B[35m val_acc:\u001B[94m 97.533333\u001B[0m\n",
      "\u001B[35mEpoch: \u001B[36m2/5 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m6.03s \u001B[35mloss:\u001B[94m 0.133980\u001B[35m acc:\u001B[94m 96.070833\u001B[35m val_loss:\u001B[94m 0.062399\u001B[35m val_acc:\u001B[94m 98.191667\u001B[0m\n",
      "\u001B[35mEpoch: \u001B[36m3/5 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m6.21s \u001B[35mloss:\u001B[94m 0.107722\u001B[35m acc:\u001B[94m 96.822917\u001B[35m val_loss:\u001B[94m 0.058225\u001B[35m val_acc:\u001B[94m 98.316667\u001B[0m\n",
      "\u001B[35mEpoch: \u001B[36m4/5 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m6.19s \u001B[35mloss:\u001B[94m 0.109482\u001B[35m acc:\u001B[94m 96.829167\u001B[35m val_loss:\u001B[94m 0.063321\u001B[35m val_acc:\u001B[94m 98.200000\u001B[0m\n",
      "\u001B[35mEpoch: \u001B[36m5/5 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m6.27s \u001B[35mloss:\u001B[94m 0.125524\u001B[35m acc:\u001B[94m 96.508333\u001B[35m val_loss:\u001B[94m 0.075388\u001B[35m val_acc:\u001B[94m 98.008333\u001B[0m\n",
      "\u001B[35mTest steps: \u001B[36m313 \u001B[32m0.73s \u001B[35mtest_loss:\u001B[94m 0.066517\u001B[35m test_acc:\u001B[94m 98.250000\u001B[0m                                                   \n"
     ]
    }
   ],
   "source": [
    "# Instantiating our network\n",
    "network = create_network()\n",
    "\n",
    "# Poutyne Model on GPU\n",
    "model = Model(network, optimizer, custom_loss_function, \n",
    "              batch_metrics=['accuracy'],\n",
    "              device=device)\n",
    "\n",
    "# Train\n",
    "model.fit_generator(train_loader,\n",
    "                    valid_loader,\n",
    "                    epochs=num_epochs,\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "# Test\n",
    "test_loss, test_acc = model.evaluate_generator(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Experiment\n",
    "\n",
    "Most of the time, when using Poutyne (or even Pytorch in general), we will find ourselves in an iterative model hyperparameters finetuning loop. For efficient model search, we will usually wish to save our best performing models, their training and testing statistics and even sometimes wish to retrain an already trained model for further tuning. All of the above can be easily implemented with the flexibility of Poutyne Callbacks, but having to define and initialize each and every Callback object we wish for our model quickly feels cumbersome.\n",
    "\n",
    "This is why Poutyne provides an [Experiment class](https://poutyne.org/experiment.html), which aims specifically at enabling quick model iteration search, while not sacrificing the quality of a single experiment - statistics logging, best models saving, etc. Experiment is actually a simple wrapper between a PyTorch network and Poutyne's core Callback objects for logging and saving. Given a working directory where to output the various logging files and a PyTorch network, the Experiment class reduces the whole training loop to a single line.\n",
    "\n",
    "The following code uses [Poutyne's Experiment class](https://poutyne.org/experiment.html) to train a network for 5 epochs. The code is quite simpler than the code in the Poutyne Callbacks section while doing more (only a few lines). Once trained for 5 epochs, it is then possible to resume the optimization at the 5th epoch for 5 more epochs until the 10th epoch using the same function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_train(network, name, epochs=5):\n",
    "    \"\"\"\n",
    "    This function creates a Poutyne Experiment, trains the input module\n",
    "    on the train loader and then tests its performance on the test loader.\n",
    "    All training and testing statistics are saved, as well as best model\n",
    "    checkpoints.\n",
    "    \n",
    "    Args:\n",
    "        network (torch.nn.Module): The neural network to train.\n",
    "        working_directory (str): The directory where to output files to save.\n",
    "        epochs (int): The number of epochs. (Default: 5)\n",
    "    \"\"\"\n",
    "    # Everything is going to be saved in ./saves/{name}.\n",
    "    save_path = os.path.join('saves', name)\n",
    "\n",
    "    # Poutyne Experiment\n",
    "    expt = Experiment(save_path, network, optimizer=optimizer, task='classif', device=device)\n",
    "\n",
    "    # Train\n",
    "    expt.train(train_loader, valid_loader, epochs=epochs)\n",
    "\n",
    "    # Test\n",
    "    expt.test(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[35mEpoch: \u001B[36m1/5 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m8.24s \u001B[35mloss:\u001B[94m 0.391692\u001B[35m acc:\u001B[94m 87.477083\u001B[35m fscore_micro:\u001B[94m 0.874771\u001B[35m val_loss:\u001B[94m 0.094987\u001B[35m val_acc:\u001B[94m 97.133333\u001B[35m val_fscore_micro:\u001B[94m 0.971333\u001B[0m\n",
      "Epoch 1: val_acc improved from -inf to 97.13333, saving file to saves/convnet_mnist_experiment/checkpoint_epoch_1.ckpt\n",
      "\u001B[35mEpoch: \u001B[36m2/5 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m8.12s \u001B[35mloss:\u001B[94m 0.135298\u001B[35m acc:\u001B[94m 95.875000\u001B[35m fscore_micro:\u001B[94m 0.958750\u001B[35m val_loss:\u001B[94m 0.068016\u001B[35m val_acc:\u001B[94m 98.016667\u001B[35m val_fscore_micro:\u001B[94m 0.980167\u001B[0m\n",
      "Epoch 2: val_acc improved from 97.13333 to 98.01667, saving file to saves/convnet_mnist_experiment/checkpoint_epoch_2.ckpt\n",
      "\u001B[35mEpoch: \u001B[36m3/5 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m8.17s \u001B[35mloss:\u001B[94m 0.103265\u001B[35m acc:\u001B[94m 96.818750\u001B[35m fscore_micro:\u001B[94m 0.968188\u001B[35m val_loss:\u001B[94m 0.063111\u001B[35m val_acc:\u001B[94m 98.116667\u001B[35m val_fscore_micro:\u001B[94m 0.981167\u001B[0m\n",
      "Epoch 3: val_acc improved from 98.01667 to 98.11667, saving file to saves/convnet_mnist_experiment/checkpoint_epoch_3.ckpt\n",
      "\u001B[35mEpoch: \u001B[36m4/5 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m7.99s \u001B[35mloss:\u001B[94m 0.090514\u001B[35m acc:\u001B[94m 97.268750\u001B[35m fscore_micro:\u001B[94m 0.972687\u001B[35m val_loss:\u001B[94m 0.049040\u001B[35m val_acc:\u001B[94m 98.475000\u001B[35m val_fscore_micro:\u001B[94m 0.984750\u001B[0m\n",
      "Epoch 4: val_acc improved from 98.11667 to 98.47500, saving file to saves/convnet_mnist_experiment/checkpoint_epoch_4.ckpt\n",
      "\u001B[35mEpoch: \u001B[36m5/5 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m8.39s \u001B[35mloss:\u001B[94m 0.074840\u001B[35m acc:\u001B[94m 97.666667\u001B[35m fscore_micro:\u001B[94m 0.976667\u001B[35m val_loss:\u001B[94m 0.052821\u001B[35m val_acc:\u001B[94m 98.341667\u001B[35m val_fscore_micro:\u001B[94m 0.983417\u001B[0m\n",
      "Restoring model from saves/convnet_mnist_experiment/checkpoint_epoch_4.ckpt\n",
      "Found best checkpoint at epoch: 4\n",
      "lr: 0.1, loss: 0.0905136, acc: 97.2687, fscore_micro: 0.972687, val_loss: 0.0490398, val_acc: 98.475, val_fscore_micro: 0.98475\n",
      "Loading checkpoint saves/convnet_mnist_experiment/checkpoint_epoch_4.ckpt\n",
      "Running test\n",
      "\u001B[35mTest steps: \u001B[36m313 \u001B[32m0.90s \u001B[35mtest_loss:\u001B[94m 0.039983\u001B[35m test_acc:\u001B[94m 98.780000\u001B[35m test_fscore_micro:\u001B[94m 0.987800\u001B[0m             \n"
     ]
    }
   ],
   "source": [
    "network = create_network()\n",
    "experiment_train(network, 'convnet_mnist_experiment', epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how setting `task='classif'` when instantiating `Experiment` adds for use our loss function, the batch metric accuracy, the epoch metric F1 and set up callbacks that use them. If you wish, you still can use your own loss function and metrics instead of passing this argument.\n",
    "\n",
    "We have trained for 5 epochs, let's now resume training for another 5 epochs for a total of 10 epochs. Notice that we reinstantiate the network. Experiment will load back the weights for us and resume training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from saves/convnet_mnist_experiment/checkpoint.ckpt and starting at epoch 6.\n",
      "Loading optimizer state from saves/convnet_mnist_experiment/checkpoint.optim and starting at epoch 6.\n",
      "\u001B[35mEpoch: \u001B[36m 6/10 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m8.35s \u001B[35mloss:\u001B[94m 0.060369\u001B[35m acc:\u001B[94m 98.133333\u001B[35m fscore_micro:\u001B[94m 0.981333\u001B[35m val_loss:\u001B[94m 0.044301\u001B[35m val_acc:\u001B[94m 98.750000\u001B[35m val_fscore_micro:\u001B[94m 0.987500\u001B[0m\n",
      "Epoch 6: val_acc improved from 98.47500 to 98.75000, saving file to saves/convnet_mnist_experiment/checkpoint_epoch_6.ckpt\n",
      "\u001B[35mEpoch: \u001B[36m 7/10 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m8.25s \u001B[35mloss:\u001B[94m 0.049694\u001B[35m acc:\u001B[94m 98.452083\u001B[35m fscore_micro:\u001B[94m 0.984521\u001B[35m val_loss:\u001B[94m 0.046923\u001B[35m val_acc:\u001B[94m 98.691667\u001B[35m val_fscore_micro:\u001B[94m 0.986917\u001B[0m\n",
      "\u001B[35mEpoch: \u001B[36m 8/10 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m7.98s \u001B[35mloss:\u001B[94m 0.045632\u001B[35m acc:\u001B[94m 98.622917\u001B[35m fscore_micro:\u001B[94m 0.986229\u001B[35m val_loss:\u001B[94m 0.052571\u001B[35m val_acc:\u001B[94m 98.558333\u001B[35m val_fscore_micro:\u001B[94m 0.985583\u001B[0m\n",
      "\u001B[35mEpoch: \u001B[36m 9/10 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m7.97s \u001B[35mloss:\u001B[94m 0.042206\u001B[35m acc:\u001B[94m 98.710417\u001B[35m fscore_micro:\u001B[94m 0.987104\u001B[35m val_loss:\u001B[94m 0.042196\u001B[35m val_acc:\u001B[94m 98.808333\u001B[35m val_fscore_micro:\u001B[94m 0.988083\u001B[0m\n",
      "Epoch 9: val_acc improved from 98.75000 to 98.80833, saving file to saves/convnet_mnist_experiment/checkpoint_epoch_9.ckpt\n",
      "\u001B[35mEpoch: \u001B[36m10/10 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m8.19s \u001B[35mloss:\u001B[94m 0.038014\u001B[35m acc:\u001B[94m 98.843750\u001B[35m fscore_micro:\u001B[94m 0.988437\u001B[35m val_loss:\u001B[94m 0.041589\u001B[35m val_acc:\u001B[94m 98.916667\u001B[35m val_fscore_micro:\u001B[94m 0.989167\u001B[0m\n",
      "Epoch 10: val_acc improved from 98.80833 to 98.91667, saving file to saves/convnet_mnist_experiment/checkpoint_epoch_10.ckpt\n",
      "Restoring model from saves/convnet_mnist_experiment/checkpoint_epoch_10.ckpt\n",
      "Found best checkpoint at epoch: 10\n",
      "lr: 0.1, loss: 0.0380142, acc: 98.8438, fscore_micro: 0.988437, val_loss: 0.0415888, val_acc: 98.9167, val_fscore_micro: 0.989167\n",
      "Loading checkpoint saves/convnet_mnist_experiment/checkpoint_epoch_10.ckpt\n",
      "Running test\n",
      "\u001B[35mTest steps: \u001B[36m313 \u001B[32m0.91s \u001B[35mtest_loss:\u001B[94m 0.032788\u001B[35m test_acc:\u001B[94m 98.980000\u001B[35m test_fscore_micro:\u001B[94m 0.989800\u001B[0m             \n"
     ]
    }
   ],
   "source": [
    "network = create_network()\n",
    "experiment_train(network, 'convnet_mnist_experiment', epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Coloring\n",
    "\n",
    "Also, Poutyne use by default a coloring template of the training step when the package `colorama` is installed.\n",
    "One could either remove the coloring (`progress_options=dict(coloring=False)`) or set a different coloring template using the fields:\n",
    "`text_color`, `ratio_color`, `metric_value_color`, `time_color` and `progress_bar_color`.\n",
    "If a field is not specified, the default color will be used.\n",
    "[See available colors in colorama's source code](https://github.com/tartley/colorama/blob/9946cfb/colorama/ansi.py#L49).\n",
    "\n",
    "Here an example where we set the `text_color` to RED and the `progress_bar_color` to LIGHTGREEN_EX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_options = dict(\n",
    "    coloring=dict(text_color=\"RED\", progress_bar_color=\"LIGHTGREEN_EX\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[31mEpoch: \u001B[36m1/5 \u001B[31mTrain steps: \u001B[36m1500 \u001B[31mVal steps: \u001B[36m375 \u001B[32m6.27s \u001B[31mloss:\u001B[94m 0.353306\u001B[31m acc:\u001B[94m 88.485417\u001B[31m val_loss:\u001B[94m 0.092008\u001B[31m val_acc:\u001B[94m 97.275000\u001B[0m\n",
      "\u001B[31mEpoch: \u001B[36m2/5 \u001B[31mTrain steps: \u001B[36m1500 \u001B[31mVal steps: \u001B[36m375 \u001B[32m6.17s \u001B[31mloss:\u001B[94m 0.131179\u001B[31m acc:\u001B[94m 95.954167\u001B[31m val_loss:\u001B[94m 0.074757\u001B[31m val_acc:\u001B[94m 97.750000\u001B[0m\n",
      "\u001B[31mEpoch: \u001B[36m3/5 \u001B[31mTrain steps: \u001B[36m1500 \u001B[31mVal steps: \u001B[36m375 \u001B[32m6.31s \u001B[31mloss:\u001B[94m 0.103332\u001B[31m acc:\u001B[94m 96.918750\u001B[31m val_loss:\u001B[94m 0.054675\u001B[31m val_acc:\u001B[94m 98.300000\u001B[0m\n",
      "\u001B[31mEpoch: \u001B[36m4/5 \u001B[31mTrain steps: \u001B[36m1500 \u001B[31mVal steps: \u001B[36m375 \u001B[32m6.08s \u001B[31mloss:\u001B[94m 0.086977\u001B[31m acc:\u001B[94m 97.312500\u001B[31m val_loss:\u001B[94m 0.049309\u001B[31m val_acc:\u001B[94m 98.516667\u001B[0m\n",
      "\u001B[31mEpoch: \u001B[36m5/5 \u001B[31mTrain steps: \u001B[36m1500 \u001B[31mVal steps: \u001B[36m375 \u001B[32m6.18s \u001B[31mloss:\u001B[94m 0.077654\u001B[31m acc:\u001B[94m 97.633333\u001B[31m val_loss:\u001B[94m 0.041916\u001B[31m val_acc:\u001B[94m 98.725000\u001B[0m\n",
      "\u001B[31mTest steps: \u001B[36m313 \u001B[32m0.77s \u001B[31mtest_loss:\u001B[94m 0.030876\u001B[31m test_acc:\u001B[94m 99.000000\u001B[0m                                                   \n"
     ]
    }
   ],
   "source": [
    "# Instantiating our network\n",
    "network = create_network()\n",
    "\n",
    "# Poutyne Model on GPU\n",
    "model = Model(network, optimizer, loss_function, \n",
    "              batch_metrics=['accuracy'],\n",
    "              device=device)\n",
    "\n",
    "# Train\n",
    "model.fit_generator(train_loader, \n",
    "                    valid_loader, \n",
    "                    epochs=num_epochs, \n",
    "                    progress_options=progress_options)\n",
    "\n",
    "# Test\n",
    "test_loss, test_acc = model.evaluate_generator(test_loader, \n",
    "                                               progress_options=progress_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epoch Metrics\n",
    "It's also possible to used epoch metrics such as [`F1-score`](https://poutyne.org/metrics.html#poutyne.FBeta). You could also define your own epoch metric using the [`EpochMetric`](https://poutyne.org/metrics.html#epoch-metric-interface) interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[35mEpoch: \u001B[36m1/5 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m8.16s \u001B[35mloss:\u001B[94m 0.403972\u001B[35m acc:\u001B[94m 86.772917\u001B[35m fscore_micro:\u001B[94m 0.867729\u001B[35m val_loss:\u001B[94m 0.081378\u001B[35m val_acc:\u001B[94m 97.675000\u001B[35m val_fscore_micro:\u001B[94m 0.976750\u001B[0m\n",
      "\u001B[35mEpoch: \u001B[36m2/5 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m8.18s \u001B[35mloss:\u001B[94m 0.124362\u001B[35m acc:\u001B[94m 96.193750\u001B[35m fscore_micro:\u001B[94m 0.961937\u001B[35m val_loss:\u001B[94m 0.057505\u001B[35m val_acc:\u001B[94m 98.375000\u001B[35m val_fscore_micro:\u001B[94m 0.983750\u001B[0m\n",
      "\u001B[35mEpoch: \u001B[36m3/5 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m8.21s \u001B[35mloss:\u001B[94m 0.095135\u001B[35m acc:\u001B[94m 97.104167\u001B[35m fscore_micro:\u001B[94m 0.971042\u001B[35m val_loss:\u001B[94m 0.049130\u001B[35m val_acc:\u001B[94m 98.625000\u001B[35m val_fscore_micro:\u001B[94m 0.986250\u001B[0m\n",
      "\u001B[35mEpoch: \u001B[36m4/5 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m7.97s \u001B[35mloss:\u001B[94m 0.082603\u001B[35m acc:\u001B[94m 97.504167\u001B[35m fscore_micro:\u001B[94m 0.975042\u001B[35m val_loss:\u001B[94m 0.044362\u001B[35m val_acc:\u001B[94m 98.766667\u001B[35m val_fscore_micro:\u001B[94m 0.987667\u001B[0m\n",
      "\u001B[35mEpoch: \u001B[36m5/5 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m7.82s \u001B[35mloss:\u001B[94m 0.075309\u001B[35m acc:\u001B[94m 97.743750\u001B[35m fscore_micro:\u001B[94m 0.977437\u001B[35m val_loss:\u001B[94m 0.042211\u001B[35m val_acc:\u001B[94m 98.733333\u001B[35m val_fscore_micro:\u001B[94m 0.987333\u001B[0m\n",
      "\u001B[35mTest steps: \u001B[36m313 \u001B[32m0.94s \u001B[35mtest_loss:\u001B[94m 0.030675\u001B[35m test_acc:\u001B[94m 99.020000\u001B[35m test_fscore_micro:\u001B[94m 0.990200\u001B[0m             \n"
     ]
    }
   ],
   "source": [
    "# Instantiating our network\n",
    "network = create_network()\n",
    "\n",
    "# Poutyne Model on GPU\n",
    "model = Model(network, optimizer, loss_function,\n",
    "              batch_metrics=['accuracy'], \n",
    "              epoch_metrics=['f1'], \n",
    "              device=device)\n",
    "\n",
    "# Train\n",
    "model.fit_generator(train_loader, valid_loader, epochs=num_epochs)\n",
    "\n",
    "# Test\n",
    "test_loss, (test_acc, test_f1) = model.evaluate_generator(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, you could also use the [`SKLearnMetrics`](https://poutyne.org/metrics.html#poutyne.SKLearnMetrics) wrapper to wrap a Scikit-learn metric as an epoch metric. Below, we show how to compute the AUC ROC using the [`SKLearnMetrics`](https://poutyne.org/metrics.html#poutyne.SKLearnMetrics) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=1):\n",
    "    \"\"\"\n",
    "    Compute softmax function.\n",
    "    \"\"\"\n",
    "    e_x = np.exp(x - x.max(axis=axis, keepdims=True))\n",
    "    return e_x / e_x.sum(axis=axis, keepdims=True)\n",
    "\n",
    "def roc_auc(y_true, y_pred, **kwargs):\n",
    "    \"\"\"\n",
    "    Since the `roc_auc_score` from Scikit-learn requires normalized probabilities,\n",
    "    we use the softmax function on the predictions.\n",
    "    \"\"\"\n",
    "    y_pred = softmax(y_pred)\n",
    "    return roc_auc_score(y_true, y_pred, **kwargs)\n",
    "\n",
    "# kwargs are keyword arguments we wish to pass to roc_auc.\n",
    "roc_epoch_metric = SKLearnMetrics(roc_auc, \n",
    "                                  kwargs=dict(multi_class='ovr', average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[35mEpoch: \u001B[36m1/5 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m8.29s \u001B[35mloss:\u001B[94m 0.378403\u001B[35m acc:\u001B[94m 87.687500\u001B[35m fscore_micro:\u001B[94m 0.876875\u001B[35m roc_auc:\u001B[94m 0.991043\u001B[35m val_loss:\u001B[94m 0.080758\u001B[35m val_acc:\u001B[94m 97.466667\u001B[35m val_fscore_micro:\u001B[94m 0.974667\u001B[35m val_roc_auc:\u001B[94m 0.999433\u001B[0m\n",
      "\u001B[35mEpoch: \u001B[36m2/5 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m8.51s \u001B[35mloss:\u001B[94m 0.124227\u001B[35m acc:\u001B[94m 96.262500\u001B[35m fscore_micro:\u001B[94m 0.962625\u001B[35m roc_auc:\u001B[94m 0.998729\u001B[35m val_loss:\u001B[94m 0.057270\u001B[35m val_acc:\u001B[94m 98.275000\u001B[35m val_fscore_micro:\u001B[94m 0.982750\u001B[35m val_roc_auc:\u001B[94m 0.999701\u001B[0m\n",
      "\u001B[35mEpoch: \u001B[36m3/5 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m8.40s \u001B[35mloss:\u001B[94m 0.096673\u001B[35m acc:\u001B[94m 97.066667\u001B[35m fscore_micro:\u001B[94m 0.970667\u001B[35m roc_auc:\u001B[94m 0.999156\u001B[35m val_loss:\u001B[94m 0.062194\u001B[35m val_acc:\u001B[94m 98.108333\u001B[35m val_fscore_micro:\u001B[94m 0.981083\u001B[35m val_roc_auc:\u001B[94m 0.999711\u001B[0m\n",
      "\u001B[35mEpoch: \u001B[36m4/5 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m8.45s \u001B[35mloss:\u001B[94m 0.082043\u001B[35m acc:\u001B[94m 97.500000\u001B[35m fscore_micro:\u001B[94m 0.975000\u001B[35m roc_auc:\u001B[94m 0.999416\u001B[35m val_loss:\u001B[94m 0.051070\u001B[35m val_acc:\u001B[94m 98.541667\u001B[35m val_fscore_micro:\u001B[94m 0.985417\u001B[35m val_roc_auc:\u001B[94m 0.999809\u001B[0m\n",
      "\u001B[35mEpoch: \u001B[36m5/5 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m8.48s \u001B[35mloss:\u001B[94m 0.070684\u001B[35m acc:\u001B[94m 97.733333\u001B[35m fscore_micro:\u001B[94m 0.977333\u001B[35m roc_auc:\u001B[94m 0.999577\u001B[35m val_loss:\u001B[94m 0.046408\u001B[35m val_acc:\u001B[94m 98.625000\u001B[35m val_fscore_micro:\u001B[94m 0.986250\u001B[35m val_roc_auc:\u001B[94m 0.999857\u001B[0m\n",
      "\u001B[35mTest steps: \u001B[36m313 \u001B[32m1.03s \u001B[35mtest_loss:\u001B[94m 0.035296\u001B[35m test_acc:\u001B[94m 98.920000\u001B[35m test_fscore_micro:\u001B[94m 0.989200\u001B[35m test_roc_auc:\u001B[94m 0.999931\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# Instantiating our network\n",
    "network = create_network()\n",
    "\n",
    "# Poutyne Model on GPU\n",
    "model = Model(network, optimizer, loss_function,\n",
    "              batch_metrics=['accuracy'], \n",
    "              epoch_metrics=['f1', roc_epoch_metric], \n",
    "              device=device)\n",
    "\n",
    "# Train\n",
    "model.fit_generator(train_loader, valid_loader, epochs=num_epochs)\n",
    "\n",
    "# Test\n",
    "test_loss, (test_acc, test_f1, test_roc) = model.evaluate_generator(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Metric Names\n",
    "\n",
    "It's also possible to name the metric using a tuple format `(<metric name>, metric)`. That way, it's possible to use multiple times the same metric type (i.e. having micro and macro F1-score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[35mEpoch: \u001B[36m1/5 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m8.21s \u001B[35mloss:\u001B[94m 0.361928\u001B[35m My accuracy name:\u001B[94m 88.272917\u001B[35m My f1 name:\u001B[94m 0.882729\u001B[35m val_loss:\u001B[94m 0.090292\u001B[35m val_My accuracy name:\u001B[94m 97.208333\u001B[35m val_My f1 name:\u001B[94m 0.972083\u001B[0m\n",
      "\u001B[35mEpoch: \u001B[36m2/5 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m8.35s \u001B[35mloss:\u001B[94m 0.130258\u001B[35m My accuracy name:\u001B[94m 96.054167\u001B[35m My f1 name:\u001B[94m 0.960542\u001B[35m val_loss:\u001B[94m 0.065651\u001B[35m val_My accuracy name:\u001B[94m 98.116667\u001B[35m val_My f1 name:\u001B[94m 0.981167\u001B[0m\n",
      "\u001B[35mEpoch: \u001B[36m3/5 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m8.28s \u001B[35mloss:\u001B[94m 0.096897\u001B[35m My accuracy name:\u001B[94m 97.022917\u001B[35m My f1 name:\u001B[94m 0.970229\u001B[35m val_loss:\u001B[94m 0.050718\u001B[35m val_My accuracy name:\u001B[94m 98.491667\u001B[35m val_My f1 name:\u001B[94m 0.984917\u001B[0m\n",
      "\u001B[35mEpoch: \u001B[36m4/5 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m8.04s \u001B[35mloss:\u001B[94m 0.079877\u001B[35m My accuracy name:\u001B[94m 97.612500\u001B[35m My f1 name:\u001B[94m 0.976125\u001B[35m val_loss:\u001B[94m 0.047217\u001B[35m val_My accuracy name:\u001B[94m 98.558333\u001B[35m val_My f1 name:\u001B[94m 0.985583\u001B[0m\n",
      "\u001B[35mEpoch: \u001B[36m5/5 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m8.25s \u001B[35mloss:\u001B[94m 0.069439\u001B[35m My accuracy name:\u001B[94m 97.843750\u001B[35m My f1 name:\u001B[94m 0.978437\u001B[35m val_loss:\u001B[94m 0.046242\u001B[35m val_My accuracy name:\u001B[94m 98.600000\u001B[35m val_My f1 name:\u001B[94m 0.986000\u001B[0m\n",
      "\u001B[35mTest steps: \u001B[36m313 \u001B[32m0.91s \u001B[35mtest_loss:\u001B[94m 0.035947\u001B[35m test_My accuracy name:\u001B[94m 98.730000\u001B[35m test_My f1 name:\u001B[94m 0.987300\u001B[0m               \n"
     ]
    }
   ],
   "source": [
    "# Instantiating our network\n",
    "network = create_network()\n",
    "\n",
    "# Poutyne Model on GPU\n",
    "model = Model(network, optimizer, loss_function,\n",
    "              batch_metrics=[(\"My accuracy name\", 'accuracy')],\n",
    "              epoch_metrics=[(\"My f1 name\", 'f1')],\n",
    "              device=device)\n",
    "\n",
    "# Train\n",
    "model.fit_generator(train_loader, valid_loader, epochs=num_epochs)\n",
    "\n",
    "# Test\n",
    "test_loss, (test_acc, test_f1) = model.evaluate_generator(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-GPUs\n",
    "\n",
    "Finally, it's also possible to use multi-GPUs for your training either by specifying a list of devices or using the arg `\"all\"` to take them all.\n",
    "\n",
    "> Obviously, you need more than one GPUs for that option.\n",
    "\n",
    "In our case here, multi-gpus takes more time because the task is not big enough to profit from multi-gpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With a Single GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[35mEpoch: \u001B[36m1/5 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m13.00s \u001B[35mloss:\u001B[94m 0.366883\u001B[35m acc:\u001B[94m 88.293750\u001B[35m val_loss:\u001B[94m 0.101252\u001B[35m val_acc:\u001B[94m 96.675000\u001B[0m\n",
      "\u001B[35mEpoch: \u001B[36m2/5 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m11.26s \u001B[35mloss:\u001B[94m 0.135737\u001B[35m acc:\u001B[94m 95.912500\u001B[35m val_loss:\u001B[94m 0.074831\u001B[35m val_acc:\u001B[94m 97.833333\u001B[0m\n",
      "\u001B[35mEpoch: \u001B[36m3/5 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m11.22s \u001B[35mloss:\u001B[94m 0.103911\u001B[35m acc:\u001B[94m 96.941667\u001B[35m val_loss:\u001B[94m 0.058809\u001B[35m val_acc:\u001B[94m 98.225000\u001B[0m\n",
      "\u001B[35mEpoch: \u001B[36m4/5 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m11.44s \u001B[35mloss:\u001B[94m 0.090600\u001B[35m acc:\u001B[94m 97.306250\u001B[35m val_loss:\u001B[94m 0.058117\u001B[35m val_acc:\u001B[94m 98.316667\u001B[0m\n",
      "\u001B[35mEpoch: \u001B[36m5/5 \u001B[35mTrain steps: \u001B[36m1500 \u001B[35mVal steps: \u001B[36m375 \u001B[32m11.22s \u001B[35mloss:\u001B[94m 0.080267\u001B[35m acc:\u001B[94m 97.552083\u001B[35m val_loss:\u001B[94m 0.048293\u001B[35m val_acc:\u001B[94m 98.608333\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'epoch': 1,\n",
       "  'loss': 0.3668832160917421,\n",
       "  'time': 13.000929268077016,\n",
       "  'acc': 88.29375,\n",
       "  'val_loss': 0.10125183857356508,\n",
       "  'val_acc': 96.675},\n",
       " {'epoch': 2,\n",
       "  'loss': 0.13573715605431547,\n",
       "  'time': 11.255656457971781,\n",
       "  'acc': 95.9125,\n",
       "  'val_loss': 0.0748305463741223,\n",
       "  'val_acc': 97.83333333333333},\n",
       " {'epoch': 3,\n",
       "  'loss': 0.10391089688939974,\n",
       "  'time': 11.217931404709816,\n",
       "  'acc': 96.94166666666666,\n",
       "  'val_loss': 0.05880866790888831,\n",
       "  'val_acc': 98.225},\n",
       " {'epoch': 4,\n",
       "  'loss': 0.09060041940018224,\n",
       "  'time': 11.444195121061057,\n",
       "  'acc': 97.30625,\n",
       "  'val_loss': 0.058117124153844395,\n",
       "  'val_acc': 98.31666666666666},\n",
       " {'epoch': 5,\n",
       "  'loss': 0.08026705734905167,\n",
       "  'time': 11.219108303077519,\n",
       "  'acc': 97.55208333333333,\n",
       "  'val_loss': 0.04829291540489066,\n",
       "  'val_acc': 98.60833333333333}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiating our network\n",
    "network = create_network()\n",
    "\n",
    "# Poutyne Model on GPU\n",
    "model = Model(network, optimizer, loss_function, \n",
    "              batch_metrics=['accuracy'],\n",
    "              device=\"all\")\n",
    "\n",
    "# Train\n",
    "model.fit_generator(train_loader, valid_loader, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}