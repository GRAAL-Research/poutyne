{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poutyne's Tips and Tricks\n",
    "\n",
    "Poutyne also over a variety of tools for fine-tuning the information generated during the training, such as colouring the training update message, a progress bar, multi-GPUs, user callbacks interface and a user naming interface for the metrics' names. \n",
    "\n",
    "We will explore those tools using a different problem presented in [Introduction to Pytorch and Poutyne](https://github.com/GRAAL-Research/poutyne/blob/master/examples/introduction_pytorch_poutyne.ipynb).\n",
    "\n",
    "Let's import all the needed packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence, pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from poutyne import set_seeds\n",
    "from poutyne.framework import Model, ModelCheckpoint, CSVLogger, Callback\n",
    "from poutyne.framework.metrics import F1\n",
    "from poutyne.framework.metrics import acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Also, we need to set Pythons's, NumPy's and PyTorch's seeds by using Poutyne function so that our training is (almost) reproducible."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set Pythons's, NumPy's and PyTorch's seeds so that our training are (almost) reproducible.\n",
    "set_seeds(42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Train a Recurrent Neural Network (RNN)\n",
    "\n",
    "In this notebook, we train an RNN, or more precisely, an LSTM, to predict the sequence of tags associated with a given address, known as parsing address.\n",
    "\n",
    "This task consists of detecting, by tagging, the different parts of an address such as the civic number, the street name or the postal code (or zip code). The following figure shows an example of such a tagging.\n",
    "\n",
    "![address parsing canada](https://poutyne.org/img/address_parsing.png)\n",
    "\n",
    "Since addresses are written in a predetermined sequence, RNN is the best way to crack this problem. For our architecture, we will use two components, an RNN and a fully-connected layer.\n",
    "\n",
    "## Training Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let's set our training constants. We first have the Cuda device used for training if one is present. Secondly, we set the batch size (i.e. the number of elements to see before updating the model) and the learning rate for the optimizer."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cuda_device = 0\n",
    "device = torch.device(\"cuda:%d\" % cuda_device if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batch_size = 32\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## RNN\n",
    "\n",
    "For the first components, instead of using a vanilla RNN, we will use a variant of it, know as a long short-term memory (LSTM) (to learn more about [LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/). For now, we will use a single layer unidirectional LSTM. \n",
    "\n",
    "Also, since our data is textual, we will use the well-known word embeddings to encode the textual information. So the LSTM input and hidden state dimensions will be of the same size. This size corresponds to the word embeddings dimension, which in our case will be the [French pre trained](https://fasttext.cc/docs/en/crawl-vectors.html) fastText embeddings of dimension 300. \n",
    "\n",
    "> See [here](https://discuss.pytorch.org/t/could-someone-explain-batch-first-true-in-lstm/15402) the explanation why we use the `batch_first` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dimension = 300\n",
    "num_layer = 1\n",
    "bidirectional = False\n",
    "\n",
    "lstm_network = nn.LSTM(input_size=dimension,\n",
    "                       hidden_size=dimension,\n",
    "                       num_layers=num_layer,\n",
    "                       bidirectional=bidirectional,\n",
    "                       batch_first=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Fully-connected Layer\n",
    "\n",
    "We use this layer to map the representation of the LSTM (300) to the tag space (8, the number of tags) and predict the most likely tag using a softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_dim = dimension #the output of the LSTM\n",
    "tag_dimension = 8\n",
    "\n",
    "fully_connected_network = nn.Linear(input_dim, tag_dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## The Dataset\n",
    "\n",
    "Now let's download our dataset; it already split into a train, valid and test set using the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def download_data(saving_dir, data_type):\n",
    "    \"\"\"\n",
    "    Function to download the dataset using data_type to specify if we want the train, valid or test.\n",
    "    \"\"\"\n",
    "\n",
    "    root_url = \"https://graal-research.github.io/poutyne-external-assets/tips_and_tricks_assets/{}.p\"\n",
    "\n",
    "    url = root_url.format(data_type)\n",
    "    r = requests.get(url)\n",
    "    os.makedirs(saving_dir, exist_ok=True)\n",
    "\n",
    "    open(os.path.join(saving_dir, f\"{data_type}.p\"), 'wb').write(r.content)\n",
    "    \n",
    "download_data('./data/', \"train\")\n",
    "download_data('./data/', \"valid\")\n",
    "download_data('./data/', \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's load in memory the data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "\n",
    "train_data = pickle.load(open(\"./data/train.p\", \"rb\"))  # 80,000 examples\n",
    "valid_data = pickle.load(open(\"./data/valid.p\", \"rb\"))  # 20,000 examples\n",
    "test_data = pickle.load(open(\"./data/test.p\", \"rb\"))  # 30,000 examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If we take a look at the training dataset, it's a list of 80,000 tuples where the first element is the full address, and the second element is a list of the tag (the ground truth)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('33 harnesworth crescent city of hamilton ontario l8b0j3',\n",
       "  ['StreetNumber',\n",
       "   'StreetName',\n",
       "   'StreetName',\n",
       "   'Municipality',\n",
       "   'Municipality',\n",
       "   'Municipality',\n",
       "   'Province',\n",
       "   'PostalCode']),\n",
       " ('1449 mouettes longueuil quebec j4j5k4',\n",
       "  ['StreetNumber', 'StreetName', 'Municipality', 'Province', 'PostalCode'])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Since the address is a text, we need to *convert* it into categorical value, such as word embeddings, for that we will use a vectorizer. This embedding vectorizer will be able to extract for every word embedding value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "class EmbeddingVectorizer:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Embedding vectorizer\n",
    "        \"\"\"\n",
    "\n",
    "        fasttext.util.download_model('fr', if_exists='ignore') \n",
    "        self.embedding_model = fasttext.load_model(\"./cc.fr.300.bin\")\n",
    "\n",
    "    def __call__(self, address):\n",
    "        \"\"\"\n",
    "        Convert address to embedding vectors\n",
    "        :param address: The address to convert\n",
    "        :return: The embeddings vectors\n",
    "        \"\"\"\n",
    "        embeddings = []\n",
    "        for word in address.split():\n",
    "            embeddings.append(self.embedding_model[word])\n",
    "        return embeddings\n",
    "     \n",
    "embedding_model = EmbeddingVectorizer() \n",
    "# Sorry for the bad download templating the problem is on the side of fastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We also need a vectorizer to convert the address tag (e.g. StreeNumber, StreetName) into categorical values. So we will use a Vectorizer class that can use the embedding vectorizer and convert the address tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Vectorizer:\n",
    "    def __init__(self, dataset, embedding_model):\n",
    "        self.data = dataset\n",
    "        self.embedding_model = embedding_model\n",
    "        self.tags_set = {\n",
    "            \"StreetNumber\": 0,\n",
    "            \"StreetName\": 1,\n",
    "            \"Unit\": 2,\n",
    "            \"Municipality\": 3,\n",
    "            \"Province\": 4,\n",
    "            \"PostalCode\": 5,\n",
    "            \"Orientation\": 6,\n",
    "            \"GeneralDelivery\": 7\n",
    "        }\n",
    "        \n",
    "    def __len__(self):\n",
    "        # for the dataloader\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        data = self.data[item]\n",
    "        address = data[0]\n",
    "        address_vector = self.embedding_model(address)\n",
    "\n",
    "        tags = data[1]\n",
    "        idx_tags = self._convert_tags_to_idx(tags)\n",
    "\n",
    "        return address_vector, idx_tags\n",
    "\n",
    "    def _convert_tags_to_idx(self, tags):\n",
    "        idx_tags = []\n",
    "        for tag in tags:\n",
    "            idx_tags.append(self.tags_set[tag])\n",
    "        return idx_tags\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_vectorize = Vectorizer(train_data, embedding_model)\n",
    "valid_data_vectorize = Vectorizer(valid_data, embedding_model)\n",
    "test_data_vectorize = Vectorizer(test_data, embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### DataLoader\n",
    "\n",
    "Now, since all the addresses are not of the same size, it is impossible to batch size them since all elements of a tensor must have the same lengths. But there a trick, padding!\n",
    "\n",
    "The idea is simple. We will add *empty* tokens at the ends of a sequence up to the longest one in a batch. At the moment of evaluating the loss, that tokens will be skip using a mask value. That way, we can pad and pack the sequence to minimize the training time ([here](https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch) a good explanation of why we pad and pack sequence).\n",
    "\n",
    "For that, we will use the `collate_fn` of the PyTorch DataLoader, and on running time, that process will be done. We will create a class, that will save the padding value (`0`) and the mask value (`-100`) and use the `__call__` method to process the batch elements.\n",
    "\n",
    "One time to take into account, since we have packed the sequence, we need the lengths of each sequence for the forward pass to unpack them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class PadCollate:\n",
    "    \"\"\"\n",
    "        A variant of collate_fn that pads the sequence to the longest sequence in the minibatch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.pad_idx = 0\n",
    "        self.mask_value = -100\n",
    "\n",
    "    def _pad_collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        **Args:**\n",
    "\n",
    "            :batch - list of (List, List) where the first element of the tuple are the word idx and the second element\n",
    "            are the target label.\n",
    "\n",
    "        Returns:\n",
    "            A list of the padded tensor sequence idx and the padded label tensor of size of the longest sequence length.\n",
    "\n",
    "        \"\"\"\n",
    "        sequences_vectors, sequences_labels, lengths = zip(\n",
    "            *[(torch.FloatTensor(seq_vectors), torch.LongTensor(labels), len(seq_vectors)) for (seq_vectors, labels)\n",
    "              in sorted(batch, key=lambda x: len(x[0]), reverse=True)])\n",
    "\n",
    "        lengths = torch.LongTensor(lengths)\n",
    "\n",
    "        padded_sequences_vectors = pad_sequence(sequences_vectors, batch_first=True, padding_value=self.pad_idx)\n",
    "\n",
    "        padded_sequences_labels = pad_sequence(sequences_labels, batch_first=True, padding_value=self.pad_idx)\n",
    "\n",
    "        mask = self._mask_padding_sequences(lengths)\n",
    "        masked_target = torch.where(mask, padded_sequences_labels,\n",
    "                                    torch.ones_like(mask) * self.mask_value)\n",
    "\n",
    "        # We also pass the mask for the F1 score since it need a mask tensor to be compute\n",
    "        return (padded_sequences_vectors, lengths), (masked_target, mask)\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        return self._pad_collate_fn(batch)\n",
    "\n",
    "    @staticmethod\n",
    "    def _mask_padding_sequences(lengths):\n",
    "        \"\"\"\n",
    "        Create a mask from the padding sequences lengths.\n",
    "    \n",
    "        Args:\n",
    "            lengths: The lengths use to create the padded sequence.\n",
    "        \"\"\"\n",
    "\n",
    "        max_len = lengths[0]\n",
    "        mask = torch.arange(max_len).expand(len(lengths), max_len) < lengths.unsqueeze(1)\n",
    "        return mask.bool()\n",
    "\n",
    "collate_fn = PadCollate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data_vectorize, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_data_vectorize, batch_size=batch_size, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_data_vectorize, batch_size=batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Full Network\n",
    "\n",
    "Now, since we have packed the sequence, we cannot use the PyTorch `nn.Sequential` constructor to define our model, so we will define the forward pass for it to unpack the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class FullNetWork(nn.Module):\n",
    "    def __init__(self, lstm_network, fully_connected_network):\n",
    "        super().__init__()\n",
    "        self.hidden_state = None\n",
    "        \n",
    "        self.lstm_network = lstm_network\n",
    "        self.fully_connected_network = fully_connected_network\n",
    "        \n",
    "    def forward(self, padded_sequences_vectors, lengths):\n",
    "        \"\"\"\n",
    "            Defines the computation performed at every call.\n",
    "        \"\"\"\n",
    "        pack_padded_sequences_vectors = pack_padded_sequence(padded_sequences_vectors, lengths, batch_first=True)\n",
    "\n",
    "        lstm_out, self.hidden_state = self.lstm_network(pack_padded_sequences_vectors)\n",
    "        lstm_out, _ = pad_packed_sequence(lstm_out, batch_first=True)\n",
    "\n",
    "        tag_space = self.fully_connected_network(lstm_out)\n",
    "        return tag_space.transpose(-1, 1) # we need to transpose since it's a sequence\n",
    "\n",
    "full_network = FullNetWork(lstm_network, fully_connected_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "So we have created an LSTM network (`lstm_network`), a fully connected network (`fully_connected_network`), those two components are used in the full network. This full network used padded, packed sequences (defined in the forward pass), so we created the `PadCollate` class to process the need work. The DataLoader will conduct that process. Finally, when we load the data, this will be done using the vectorizer, so the address will be represented using word embeddings. Also, the address components will be converted into categorical value (from 0 to 7)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Training Loop\n",
    "\n",
    "Now that we have all the components for the network let's define our SGD optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(full_network.parameters(), lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Poutyne Callbacks\n",
    "\n",
    "One nice feature of Poutyne is [callbacks](https://poutyne.org/callbacks.html). Callbacks allow doing actions during the training of the neural network. In the following example, we use three callbacks. One that saves the latest weights in a file to be able to continue the optimization at the end of training if more epochs are needed. Another one that saves the best weights according to the performance on the validation dataset. Finally, another one that saves the displayed logs into a TSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "name_of_network = \"lstm_unidirectional\"\n",
    "\n",
    "callbacks = [\n",
    "        # Save the latest weights to be able to continue the optimization at the end for more epochs.\n",
    "        ModelCheckpoint(name_of_network + '_last_epoch.ckpt', temporary_filename='last_epoch.ckpt.tmp'),\n",
    "\n",
    "        # Save the weights in a new file when the current model is better than all previous models.\n",
    "        ModelCheckpoint(name_of_network + '_best_epoch_{epoch}.ckpt', monitor='val_accuracy', mode='max', save_best_only=True, restore_best=True, verbose=True, temporary_filename='best_epoch.ckpt.tmp'),\n",
    "\n",
    "        # Save the losses and accuracies for each epoch in a TSV.\n",
    "        CSVLogger(name_of_network + '_log.tsv', separator='\\t'),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Making Your own Callback\n",
    "\n",
    "While Poutyne provides a great number of [predefined callbacks](https://poutyne.org/callbacks.html), it is sometimes useful to make your own callback.\n",
    "\n",
    "In the following example, we want to see the effect of temperature on the optimization of our neural network. To do so, we either increase or decrease the temperature during the optimization. As one can see in the result, temperature either as no effect or has a detrimental effect on the performance of the neural network. This is so because the temperature has for effect to artificially changing the learning rates. Since we have found the right learning rate, increasing or decreasing, it shows no improvement on the results.\n",
    "\n",
    "> Since we use a mask, y_true is a tuple where the first element is the ground truth and the second one is the mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CrossEntropyLossWithTemperature(nn.Module):\n",
    "    \"\"\"\n",
    "    This loss module is the cross-entropy loss function\n",
    "    with temperature. It divides the logits by a temperature\n",
    "    value before computing the cross-entropy loss.\n",
    "\n",
    "    Args:\n",
    "        initial_temperature (float): The initial value of the temperature.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, initial_temperature):\n",
    "        super().__init__()\n",
    "        self.temperature = initial_temperature\n",
    "        self.celoss = nn.CrossEntropyLoss(ignore_index=-100)  # we use the same -100 ignore index\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_pred = y_pred / self.temperature\n",
    "        # Since y_true is a tuple where y_true[1] is the mask\n",
    "        return self.celoss(y_pred, y_true[0])\n",
    "\n",
    "\n",
    "class TemperatureCallback(Callback):\n",
    "    \"\"\"\n",
    "    This callback multiply the loss temperature with a decay before\n",
    "    each batch.\n",
    "\n",
    "    Args:\n",
    "        celoss_with_temp (CrossEntropyLossWithTemperature): the loss module.\n",
    "        decay (float): The value of the temperature decay.\n",
    "    \"\"\"\n",
    "    def __init__(self, celoss_with_temp, decay):\n",
    "        super().__init__()\n",
    "        self.celoss_with_temp = celoss_with_temp\n",
    "        self.decay = decay\n",
    "\n",
    "    def on_train_batch_begin(self, batch, logs):\n",
    "        self.celoss_with_temp.temperature *= self.decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "So our loss function will be the cross-entropy with temperature with an initial temperature of `0.1` and a temperature decay of `1.0008`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss_function = CrossEntropyLossWithTemperature(0.1)\n",
    "callbacks = callbacks + [TemperatureCallback(loss_function, 1.0008)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Finally, as we saw early, `y_true` is a tuple, so we need to modify a little bit the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_true, ignore_index=-100):\n",
    "    \"\"\"\n",
    "    Wrapper function around the accuracy where the y is a tuple of (tag, mask).\n",
    "    \"\"\"\n",
    "\n",
    "    # Since y_true[1] is the mask\n",
    "    return acc(y_pred, y_true=y_true[0], ignore_index=ignore_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now let's test our training loop for one epoch using the accuracy as the batch metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-23-4059d1c53606>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mModel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfull_network\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mloss_function\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_metrics\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0maccuracy\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_loader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalid_loader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcallbacks\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcallbacks\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/anaconda3/envs/deeptagger/lib/python3.8/site-packages/poutyne/framework/model.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, x, y, validation_data, batch_size, epochs, steps_per_epoch, validation_steps, batches_per_step, initial_epoch, verbose, coloring, progress_bar, callbacks)\u001B[0m\n\u001B[1;32m    279\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    280\u001B[0m         \"\"\"\n\u001B[0;32m--> 281\u001B[0;31m         \u001B[0mtrain_generator\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_dataloader_from_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    282\u001B[0m         \u001B[0mvalid_generator\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    283\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mvalidation_data\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/deeptagger/lib/python3.8/site-packages/poutyne/framework/model.py\u001B[0m in \u001B[0;36m_dataloader_from_data\u001B[0;34m(self, args, batch_size)\u001B[0m\n\u001B[1;32m    298\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_dataloader_from_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    299\u001B[0m         \u001B[0margs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnumpy_to_torch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 300\u001B[0;31m         \u001B[0mdataset\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTensorDataset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m1\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    301\u001B[0m         \u001B[0mgenerator\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mDataLoader\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    302\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mgenerator\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/deeptagger/lib/python3.8/site-packages/poutyne/utils.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, *tensors)\u001B[0m\n\u001B[1;32m    139\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    140\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 141\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_len\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_rabbit_hole\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtensors\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    142\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    143\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__getitem__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mindex\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/deeptagger/lib/python3.8/site-packages/poutyne/utils.py\u001B[0m in \u001B[0;36m_rabbit_hole\u001B[0;34m(obj)\u001B[0m\n\u001B[1;32m    135\u001B[0m                 \u001B[0mlengths\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0m_rabbit_hole\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mo\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mo\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mobj\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    136\u001B[0m                 \u001B[0;32mfor\u001B[0m \u001B[0mlength\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mlengths\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 137\u001B[0;31m                     \u001B[0;32massert\u001B[0m \u001B[0mlength\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mlengths\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    138\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mlengths\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    139\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAssertionError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model = Model(full_network, optimizer, loss_function, batch_metrics=[accuracy])\n",
    "model.to(device)\n",
    "model.fit_generator(train_loader, \n",
    "                    valid_loader, \n",
    "                    epochs=1, \n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Coloring\n",
    "Also, Poutyne use by default a coloring template of the training step when the package `colorama` is installed.\n",
    "One could either remove the coloring (`color_log=False`) or set a different coloring template using the fields:\n",
    "`text_color`, `ratio_color`, `metric_value_color`, `time_color` and `progress_bar_color`.\n",
    "If a field is not specified, the default colour will be used.\n",
    "\n",
    "Here an example where we set the `text_color` to MAGENTA and the `ratio_color` to BLUE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.fit_generator(train_loader, \n",
    "                    valid_loader, \n",
    "                    epochs=1, \n",
    "                    callbacks=callbacks,\n",
    "                    coloring={\"text_color\": \"MAGENTA\", \"ratio_color\":\"BLUE\"})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epoch metrics\n",
    "It's also possible to used epoch metrics such as F1-score. You could also define your own epoch metric using the `EpochMetric` interface.\n",
    "\n",
    "Furthermore, you could also use the `SKLearnMetrics` wrapper to wrap a Scikit-learn metric as an epoch metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[93mEpoch: \u001B[94m1/1 \u001B[93mStep: \u001B[94m2500/2500 \u001B[93m100.00% |\u001B[92m█████████████████████████\u001B[93m|\u001B[32m50s \u001B[93mloss:\u001B[96m 0.014880\u001B[93m accuracy:\u001B[96m 99.679686\u001B[93m fscore_micro:\u001B[96m 0.996783\u001B[93m val_loss:\u001B[96m 0.012145\u001B[93m val_accuracy:\u001B[96m 99.752318\u001B[93m val_fscore_micro:\u001B[96m 0.997510\u001B[0m\n",
      "Epoch 1: val_accuracy improved from 99.60738 to 99.75232, saving file to lstm_unidirectional_best_epoch_1.ckpt\n",
      "Restoring model from lstm_unidirectional_best_epoch_1.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'epoch': 1,\n",
       "  'loss': 0.014880063536483794,\n",
       "  'time': 50.43786235805601,\n",
       "  'accuracy': 99.6796859161377,\n",
       "  'fscore_micro': 0.9967827796936035,\n",
       "  'val_loss': 0.012145331314951182,\n",
       "  'val_accuracy': 99.75231840820312,\n",
       "  'val_fscore_micro': 0.9975102543830872}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(full_network, \n",
    "              optimizer, \n",
    "              loss_function, \n",
    "              batch_metrics=[accuracy], \n",
    "              epoch_metrics=[F1()])\n",
    "model.to(device)\n",
    "model.fit_generator(train_loader, \n",
    "                    valid_loader, \n",
    "                    epochs=1, \n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric naming\n",
    "\n",
    "It's also possible to name the metric using a tuple format `(<metric name>, metric)`. That way, it's possible to use multiple times the same metric type (i.e. having micro and macro F1-score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[93mEpoch: \u001B[94m1/1 \u001B[93mStep: \u001B[94m2500/2500 \u001B[93m100.00% |\u001B[92m█████████████████████████\u001B[93m|\u001B[32m50s \u001B[93mloss:\u001B[96m 0.008395\u001B[93m My accuracy name:\u001B[96m 99.811045\u001B[93m My metric name:\u001B[96m 0.998107\u001B[93m val_loss:\u001B[96m 0.007556\u001B[93m val_My accuracy name:\u001B[96m 99.823281\u001B[93m val_My metric name:\u001B[96m 0.998227\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'epoch': 1,\n",
       "  'loss': 0.008395192819228396,\n",
       "  'time': 50.160522730089724,\n",
       "  'My accuracy name': 99.811045022583,\n",
       "  'My metric name': 0.9981074929237366,\n",
       "  'val_loss': 0.0075557078192010525,\n",
       "  'val_My accuracy name': 99.82328148193359,\n",
       "  'val_My metric name': 0.9982274174690247}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(full_network, \n",
    "              optimizer, \n",
    "              loss_function, \n",
    "              batch_metrics=[(\"My accuracy name\", accuracy)], \n",
    "              epoch_metrics=[(\"My metric name\", F1())])\n",
    "model.to(device)\n",
    "model.fit_generator(train_loader, \n",
    "                    valid_loader, \n",
    "                    epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-GPUs\n",
    "\n",
    "Finally, it's also possible to use multi-GPUs for your training either by specifying a list of devices or using the arg `\"all\"` to take them all.\n",
    "\n",
    "> Obviously, you need more than one GPUs for that option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[93mEpoch: \u001B[94m1/1 \u001B[93mStep: \u001B[94m2500/2500 \u001B[93m100.00% |\u001B[92m█████████████████████████\u001B[93m|\u001B[32m50s \u001B[93mloss:\u001B[96m 0.007212\u001B[93m My accuracy name:\u001B[96m 99.833352\u001B[93m My metric name:\u001B[96m 0.998331\u001B[93m val_loss:\u001B[96m 0.006575\u001B[93m val_My accuracy name:\u001B[96m 99.850907\u001B[93m val_My metric name:\u001B[96m 0.998505\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'epoch': 1,\n",
       "  'loss': 0.007211980563937686,\n",
       "  'time': 50.16214045207016,\n",
       "  'My accuracy name': 99.83335216369629,\n",
       "  'My metric name': 0.9983305335044861,\n",
       "  'val_loss': 0.006574607047159225,\n",
       "  'val_My accuracy name': 99.85090672607421,\n",
       "  'val_My metric name': 0.9985048174858093}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(full_network, \n",
    "              optimizer, \n",
    "              loss_function, \n",
    "              batch_metrics=[(\"My accuracy name\", accuracy)], \n",
    "              epoch_metrics=[(\"My metric name\", F1())])\n",
    "model.to(\"all\")\n",
    "model.fit_generator(train_loader, \n",
    "                    valid_loader, \n",
    "                    epochs=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}